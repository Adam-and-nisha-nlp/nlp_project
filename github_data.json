[
    {
        "repo": "/nyukat/breast_cancer_classifier",
        "language": "Jupyter Notebook",
        "readme_contents": "# Deep Neural Networks Improve Radiologists' Performance in Breast Cancer Screening\n\n## Introduction\nThis is an implementation of the model used for breast cancer classification as described in our paper [Deep Neural Networks Improve Radiologists' Performance in Breast Cancer Screening](https://ieeexplore.ieee.org/document/8861376). The implementation allows users to get breast cancer predictions by applying one of our pretrained models: a model which takes images as input (*image-only*) and a model which takes images and heatmaps as input (*image-and-heatmaps*). \n\n* Input images: 2 CC view mammography images of size 2677x1942 and 2 MLO view mammography images of size 2974x1748. Each image is saved as 16-bit png file and gets standardized separately before being fed to the models.\n* Input heatmaps: output of the patch classifier constructed to be the same size as its corresponding mammogram. Two heatmaps are generated for each mammogram, one for benign and one for malignant category. The value of each pixel in both of them is between 0 and 1.\n* Output: 2 predictions for each breast, probability of benign and malignant findings: `left_benign`, `right_benign`, `left_malignant`, and `right_malignant`.\n\nBoth models act on screening mammography exams with four standard views (L-CC, R-CC, L-MLO, R-MLO). As a part of this repository, we provide 4 sample exams (in `sample_data/images` directory and exam list stored in `sample_data/exam_list_before_cropping.pkl`). Heatmap generation model and cancer classification models are implemented in PyTorch. \n\n**Update (2019/10/26)**: [Our paper](https://ieeexplore.ieee.org/document/8861376) will be published in the IEEE Transactions on Medical Imaging!\n\n**Update (2019/08/26)**: We have added a [TensorFlow implementation](using_tensorflow.md) of our *image-wise* model.\n\n**Update (2019/06/21)**: We have included the *image-wise* model as described in the paper that generates predictions based on a single mammogram image. This model slightly under-performs the *view-wise* model used above, but can be used on single mammogram images as opposed to full exams.\n\n**Update (2019/05/15)**: Fixed a minor bug that caused the output DataFrame columns (`left_malignant`, `right_benign`) to be swapped. Note that this does not affect the operation of the model.\n\n## Prerequisites\n\n* Python (3.6)\n* PyTorch (0.4.1)\n* torchvision (0.2.0)\n* NumPy (1.14.3)\n* SciPy (1.0.0)\n* H5py (2.7.1)\n* imageio (2.4.1)\n* pandas (0.22.0)\n* tqdm (4.19.8)\n* opencv-python (3.4.2)\n\n## License\n\nThis repository is licensed under the terms of the GNU AGPLv3 license.\n\n## How to run the code\n\n### Exam-level\n\nHere we describe how to get predictions from *view-wise* model, which is our best-performing model. This model takes 4 images from each view as input and outputs predictions for each exam.\n\n```bash\nbash run.sh\n``` \nwill automatically run the entire pipeline and save the prediction results in csv. \n\nWe recommend running the code with a gpu (set by default). To run the code with cpu only, please change `DEVICE_TYPE` in `run.sh` to 'cpu'.  \n\nIf running the individual Python scripts, please include the path to this repository in your `PYTHONPATH` . \n\nYou should obtain the following outputs for the sample exams provided in the repository. \n\nPredictions using *image-only* model (found in `sample_output/image_predictions.csv` by default):\n\n| index | left_benign | right_benign | left_malignant | right_malignant |\n| ----- | ----------- | ------------ | -------------- | --------------- |\n| 0     | 0.0580      | 0.0754       | 0.0091         | 0.0179          |\n| 1     | 0.0646      | 0.9536       | 0.0012         | 0.7258          |\n| 2     | 0.4388      | 0.3526       | 0.2325         | 0.1061          |\n| 3     | 0.3765      | 0.6483       | 0.0909         | 0.2579          |\n\n\nPredictions using *image-and-heatmaps* model (found in `sample_output/imageheatmap_predictions.csv` by default):\n\n| index | left_benign  | right_benign | left_malignant | right_malignant |\n| ----- | ------------ | ------------ | -------------- | --------------- |\n| 0     | 0.0612       | 0.0555       | 0.0099         | 0.0063          |\n| 1     | 0.0507       | 0.8025       | 0.0009         | 0.9000          |\n| 2     | 0.2877       | 0.2286       | 0.2524         | 0.0461          |\n| 3     | 0.4181       | 0.3172       | 0.3174         | 0.0485          |\n\n### Single Image\n\nHere we also upload *image-wise* model, which is different from and performs worse than the *view-wise* model described above. The csv output from *view-wise* model will be different from that of *image-wise* model in this section. Because this model has the benefit of creating predictions for each image separately, we make this model public to facilitate transfer learning.\n\nTo use the *image-wise* model, run a command such as the following:\n\n```bash\nbash run_single.sh \"sample_data/images/0_L_CC.png\" \"L-CC\"\n``` \n\nwhere the first argument is path to a mammogram image, and the second argument is the view corresponding to that image.\n\nYou should obtain the following output based on the above example command:\n\n```\nStage 1: Crop Mammograms\nStage 2: Extract Centers\nStage 3: Generate Heatmaps\nStage 4a: Run Classifier (Image)\n{\"benign\": 0.040191903710365295, \"malignant\": 0.008045293390750885}\nStage 4b: Run Classifier (Image+Heatmaps)\n{\"benign\": 0.052365876734256744, \"malignant\": 0.005510155577212572}\n```\n\n#### Image-level Notebook\n\nWe have included a [sample notebook](sample_notebook.ipynb) that contains code for running the classifiers with and without heatmaps (excludes preprocessing).\n\n## Data\n\nTo use one of the pretrained models, the input is required to consist of at least four images, at least one for each view (L-CC, L-MLO, R-CC, R-MLO). \n\nThe original 12-bit mammograms are saved as rescaled 16-bit images to preserve the granularity of the pixel intensities, while still being correctly displayed in image viewers.\n\n`sample_data/exam_list_before_cropping.pkl` contains a list of exam information before preprocessing. Each exam is represented as a dictionary with the following format:\n\n```python\n{\n  'horizontal_flip': 'NO',\n  'L-CC': ['0_L_CC'],\n  'R-CC': ['0_R_CC'],\n  'L-MLO': ['0_L_MLO'],\n  'R-MLO': ['0_R_MLO'],\n}\n```\n\nWe expect images from `L-CC` and `L-MLO` views to be facing right direction, and images from `R-CC` and `R-MLO` views are facing left direction. `horizontal_flip` indicates whether all images in the exam are flipped horizontally from expected. Values for `L-CC`, `R-CC`, `L-MLO`, and `R-MLO` are list of image filenames without extension and directory name. \n\nAdditional information for each image gets included as a dictionary. Such dictionary has all 4 views as keys, and the values are the additional information for the corresponding key. For example, `window_location`, which indicates the top, bottom, left and right edges of cropping window, is a dictionary that has 4 keys and has 4 lists as values which contain the corresponding information for the images. Additionally, `rightmost_pixels`, `bottommost_pixels`, `distance_from_starting_side` and `best_center` are added after preprocessing. \nDescription for these attributes can be found in the preprocessing section. \nThe following is an example of exam information after cropping and extracting optimal centers:\n\n```python\n{\n  'horizontal_flip': 'NO',\n  'L-CC': ['0_L_CC'],\n  'R-CC': ['0_R_CC'],\n  'L-MLO': ['0_L_MLO'],\n  'R-MLO': ['0_R_MLO'],\n  'window_location': {\n    'L-CC': [(353, 4009, 0, 2440)],\n    'R-CC': [(71, 3771, 952, 3328)],\n    'L-MLO': [(0, 3818, 0, 2607)],\n    'R-MLO': [(0, 3724, 848, 3328)]\n   },\n  'rightmost_points': {\n    'L-CC': [((1879, 1958), 2389)],\n    'R-CC': [((2207, 2287), 2326)],\n    'L-MLO': [((2493, 2548), 2556)],\n    'R-MLO': [((2492, 2523), 2430)]\n   },\n  'bottommost_points': {\n    'L-CC': [(3605, (100, 100))],\n    'R-CC': [(3649, (101, 106))],\n    'L-MLO': [(3767, (1456, 1524))],\n    'R-MLO': [(3673, (1164, 1184))]\n   },\n  'distance_from_starting_side': {\n    'L-CC': [0],\n    'R-CC': [0],\n    'L-MLO': [0],\n    'R-MLO': [0]\n   },\n  'best_center': {\n    'L-CC': [(1850, 1417)],\n    'R-CC': [(2173, 1354)],\n    'L-MLO': [(2279, 1681)],\n    'R-MLO': [(2185, 1555)]\n   }\n}\n```\n\nThe labels for the included exams are as follows:\n\n| index | left_benign | right_benign | left_malignant | right_malignant |\n| ----- | ----------- | ------------ | -------------- | --------------- |\n| 0     | 0           | 0            | 0              | 0               |\n| 1     | 0           | 0            | 0              | 1               |\n| 2     | 1           | 0            | 0              | 0               |\n| 3     | 1           | 1            | 1              | 1               |\n\n\n## Pipeline\n\nThe pipeline consists of four stages.\n\n1. Crop mammograms\n2. Calculate optimal centers\n3. Generate Heatmaps\n4. Run classifiers\n\nThe following variables defined in `run.sh` can be modified as needed:\n* `NUM_PROCESSES`: The number of processes to be used in preprocessing (`src/cropping/crop_mammogram.py` and `src/optimal_centers/get_optimal_centers.py`). Default: 10.\n* `DEVICE_TYPE`: Device type to use in heatmap generation and classifiers, either 'cpu' or 'gpu'. Default: 'gpu'\n* `NUM_EPOCHS`: The number of epochs to be averaged in the output of the classifiers. Default: 10.\n* `HEATMAP_BATCH_SIZE`: The batch size to use in heatmap generation. Default: 100.\n* `GPU_NUMBER`: Specify which one of the GPUs to use when multiple GPUs are available. Default: 0. \n\n* `DATA_FOLDER`: The directory where the mammogram is stored.\n* `INITIAL_EXAM_LIST_PATH`: The path where the initial exam list without any metadata is stored.\n* `PATCH_MODEL_PATH`: The path where the saved weights for the patch classifier is saved.\n* `IMAGE_MODEL_PATH`: The path where the saved weights for the *image-only* model is saved.\n* `IMAGEHEATMAPS_MODEL_PATH`: The path where the saved weights for the *image-and-heatmaps* model is saved.\n\n* `CROPPED_IMAGE_PATH`: The directory to save cropped mammograms.\n* `CROPPED_EXAM_LIST_PATH`: The path to save the new exam list with cropping metadata.\n* `EXAM_LIST_PATH`: The path to save the new exam list with best center metadata.\n* `HEATMAPS_PATH`: The directory to save heatmaps.\n* `IMAGE_PREDICTIONS_PATH`: The path to save predictions of *image-only* model.\n* `IMAGEHEATMAPS_PREDICTIONS_PATH`: The path to save predictions of *image-and-heatmaps* model.\n\n\n### Preprocessing\n\nRun the following commands to crop mammograms and calculate information about augmentation windows.\n\n#### Crop mammograms\n```bash\npython3 src/cropping/crop_mammogram.py \\\n    --input-data-folder $DATA_FOLDER \\\n    --output-data-folder $CROPPED_IMAGE_PATH \\\n    --exam-list-path $INITIAL_EXAM_LIST_PATH  \\\n    --cropped-exam-list-path $CROPPED_EXAM_LIST_PATH  \\\n    --num-processes $NUM_PROCESSES\n```\n`src/import_data/crop_mammogram.py` crops the mammogram around the breast and discards the background in order to improve image loading time and time to run segmentation algorithm and saves each cropped image to `$PATH_TO_SAVE_CROPPED_IMAGES/short_file_path.png` using h5py. In addition, it adds additional information for each image and creates a new image list to `$CROPPED_IMAGE_LIST_PATH` while discarding images which it fails to crop. Optional --verbose argument prints out information about each image. The additional information includes the following:\n- `window_location`: location of cropping window w.r.t. original dicom image so that segmentation map can be cropped in the same way for training.\n- `rightmost_points`: rightmost nonzero pixels after correctly being flipped.\n- `bottommost_points`: bottommost nonzero pixels after correctly being flipped.\n- `distance_from_starting_side`: records if zero-value gap between the edge of the image and the breast is found in the side where the breast starts to appear and thus should have been no gap. Depending on the dataset, this value can be used to determine wrong value of `horizontal_flip`.\n\n\n#### Calculate optimal centers\n```bash\npython3 src/optimal_centers/get_optimal_centers.py \\\n    --cropped-exam-list-path $CROPPED_EXAM_LIST_PATH \\\n    --data-prefix $CROPPED_IMAGE_PATH \\\n    --output-exam-list-path $EXAM_LIST_PATH \\\n    --num-processes $NUM_PROCESSES\n```\n`src/optimal_centers/get_optimal_centers.py` outputs new exam list with additional metadata to `$EXAM_LIST_PATH`. The additional information includes the following:\n- `best_center`: optimal center point of the window for each image. The augmentation windows drawn with `best_center` as exact center point could go outside the boundary of the image. This usually happens when the cropped image is smaller than the window size. In this case, we pad the image and shift the window to be inside the padded image in augmentation. Refer to [the data report](https://cs.nyu.edu/~kgeras/reports/datav1.0.pdf) for more details.\n\n\n### Heatmap Generation\n```bash\npython3 src/heatmaps/run_producer.py \\\n    --model-path $PATCH_MODEL_PATH \\\n    --data-path $EXAM_LIST_PATH \\\n    --image-path $CROPPED_IMAGE_PATH \\\n    --batch-size $HEATMAP_BATCH_SIZE \\\n    --output-heatmap-path $HEATMAPS_PATH \\\n    --device-type $DEVICE_TYPE \\\n    --gpu-number $GPU_NUMBER\n```\n\n`src/heatmaps/run_producer.py` generates heatmaps by combining predictions for patches of images and saves them as hdf5 format in `$HEATMAPS_PATH` using `$DEVICE_TYPE` device. `$DEVICE_TYPE` can either be 'gpu' or 'cpu'. `$HEATMAP_BATCH_SIZE` should be adjusted depending on available memory size.  An optional argument `--gpu-number`  can be used to specify which GPU to use.\n\n### Running the models\n\n`src/modeling/run_model.py` can provide predictions using cropped images either with or without heatmaps. When using heatmaps, please use the`--use-heatmaps` flag and provide appropriate the `--model-path` and `--heatmaps-path` arguments. Depending on the available memory, the optional argument `--batch-size` can be provided. Another optional argument `--gpu-number` can be used to specify which GPU to use.\n\n#### Run image only model\n```bash\npython3 src/modeling/run_model.py \\\n    --model-path $IMAGE_MODEL_PATH \\\n    --data-path $EXAM_LIST_PATH \\\n    --image-path $CROPPED_IMAGE_PATH \\\n    --output-path $IMAGE_PREDICTIONS_PATH \\\n    --use-augmentation \\\n    --num-epochs $NUM_EPOCHS \\\n    --device-type $DEVICE_TYPE \\\n    --gpu-number $GPU_NUMBER\n```\n\nThis command makes predictions only using images for `$NUM_EPOCHS` epochs with random augmentation and outputs averaged predictions per exam to `$IMAGE_PREDICTIONS_PATH`. \n\n#### Run image+heatmaps model \n```bash\npython3 src/modeling/run_model.py \\\n    --model-path $IMAGEHEATMAPS_MODEL_PATH \\\n    --data-path $EXAM_LIST_PATH \\\n    --image-path $CROPPED_IMAGE_PATH \\\n    --output-path $IMAGEHEATMAPS_PREDICTIONS_PATH \\\n    --use-heatmaps \\\n    --heatmaps-path $HEATMAPS_PATH \\\n    --use-augmentation \\\n    --num-epochs $NUM_EPOCHS \\\n    --device-type $DEVICE_TYPE \\\n    --gpu-number $GPU_NUMBER\n```\n\nThis command makes predictions using images and heatmaps for `$NUM_EPOCHS` epochs with random augmentation and outputs averaged predictions per exam to `$IMAGEHEATMAPS_PREDICTIONS_PATH`. \n\n## Getting image from dicom files and saving as 16-bit png files\n\nDicom files can be converted into png files with the following function, which then can be used by the code in our repository (pypng 0.0.19 and pydicom 1.2.2 libraries are required). \n\n```python3\nimport png\nimport pydicom\n\ndef save_dicom_image_as_png(dicom_filename, png_filename, bitdepth=12):\n    \"\"\"\n    Save 12-bit mammogram from dicom as rescaled 16-bit png file.\n    :param dicom_filename: path to input dicom file.\n    :param png_filename: path to output png file.\n    :param bitdepth: bit depth of the input image. Set it to 12 for 12-bit mammograms.\n    \"\"\"\n    image = pydicom.read_file(dicom_filename).pixel_array\n    with open(png_filename, 'wb') as f:\n        writer = png.Writer(height=image.shape[0], width=image.shape[1], bitdepth=bitdepth, greyscale=True)\n        writer.write(f, image.tolist())\n```\n\n## Reference\n\nIf you found this code useful, please cite our paper:\n\n**Deep Neural Networks Improve Radiologists' Performance in Breast Cancer Screening**\\\nNan Wu, Jason Phang, Jungkyu Park, Yiqiu Shen, Zhe Huang, Masha Zorin, Stanis\u0142aw Jastrz\u0119bski, Thibault F\u00e9vry, Joe Katsnelson, Eric Kim, Stacey Wolfson, Ujas Parikh, Sushma Gaddam, Leng Leng Young Lin, Kara Ho, Joshua D. Weinstein, Beatriu Reig, Yiming Gao, Hildegard Toth, Kristine Pysarenko, Alana Lewin, Jiyon Lee, Krystal Airola, Eralda Mema, Stephanie Chung, Esther Hwang, Naziya Samreen, S. Gene Kim, Laura Heacock, Linda Moy, Kyunghyun Cho, Krzysztof J. Geras\\\nIEEE Transactions on Medical Imaging\\\n2019\n\n    @article{wu2019breastcancer, \n        title = {Deep Neural Networks Improve Radiologists' Performance in Breast Cancer Screening},\n        author = {Nan Wu and Jason Phang and Jungkyu Park and Yiqiu Shen and Zhe Huang and Masha Zorin and Stanis\\l{}aw Jastrz\\k{e}bski and Thibault F\\'{e}vry and Joe Katsnelson and Eric Kim and Stacey Wolfson and Ujas Parikh and Sushma Gaddam and Leng Leng Young Lin and Kara Ho and Joshua D. Weinstein and Beatriu Reig and Yiming Gao and Hildegard Toth and Kristine Pysarenko and Alana Lewin and Jiyon Lee and Krystal Airola and Eralda Mema and Stephanie Chung and Esther Hwang and Naziya Samreen and S. Gene Kim and Laura Heacock and Linda Moy and Kyunghyun Cho and Krzysztof J. Geras}, \n        journal = {IEEE Transactions on Medical Imaging},\n        year = {2019}\n    }\n"
    },
    {
        "repo": "/lishen/end2end-all-conv",
        "language": "Jupyter Notebook",
        "readme_contents": "Shield: [![CC BY-NC-SA 4.0][cc-by-nc-sa-shield]][cc-by-nc-sa]\n\nThis work is licensed under a\n[Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License][cc-by-nc-sa].\n\n[![CC BY-NC-SA 4.0][cc-by-nc-sa-image]][cc-by-nc-sa]\n\n[cc-by-nc-sa]: http://creativecommons.org/licenses/by-nc-sa/4.0/\n[cc-by-nc-sa-image]: https://licensebuttons.net/l/by-nc-sa/4.0/88x31.png\n[cc-by-nc-sa-shield]: https://img.shields.io/badge/License-CC%20BY--NC--SA%204.0-lightgrey.svg\n\n# Deep Learning to Improve Breast Cancer Detection on Screening Mammography (End-to-end Training for Whole Image Breast Cancer Screening using An All Convolutional Design)\nLi Shen, Ph.D. CS\n\nIcahn School of Medicine at Mount Sinai\n\nNew York, New York, USA\n\n![Fig1](https://raw.githubusercontent.com/lishen/end2end-all-conv/master/ddsm_train/Fig-1%20patch%20to%20whole%20image%20conv.jpg \"Convert conv net from patch to whole image\")\n\n## Introduction\nThis is the companion site for our paper that was originally titled \"End-to-end Training for Whole Image Breast Cancer Diagnosis using An All Convolutional Design\" and was retitled as \"Deep Learning to Improve Breast Cancer Detection on Screening Mammography\". The paper has been published [here](https://rdcu.be/bPOYf). You may also find the arXiv version [here](https://arxiv.org/abs/1708.09427). This work was initially presented at the NIPS17 workshop on machine learning for health. Access the 4-page short paper [here](https://arxiv.org/abs/1711.05775). Download the [poster](https://raw.githubusercontent.com/lishen/end2end-all-conv/master/ddsm_train/NIPS17%20ML4H%20Poster.pdf).\n\nFor our entry in the DREAM2016 Digital Mammography challenge, see this [write-up](https://www.synapse.org/LiShenDMChallenge). This work is much improved from our method used in the challenge.\n\n## Whole image model downloads\nA few best whole image models are available for downloading at this Google Drive [folder](https://drive.google.com/drive/folders/0B1PVLadG_dCKV2pZem5MTjc1cHc?resourcekey=0-t4vtopuv27D9NnMC97w6hg&usp=sharing). YaroslavNet is the DM challenge top-performing team's [method](https://www.synapse.org/#!Synapse:syn9773040/wiki/426908). Here is a table for model AUCs:\n\n| Database  | Patch Classifier  | Top Layers (two blocks)  | Single AUC  | Augmented AUC  |\n|---|---|---|---|---|\n| DDSM  | Resnet50  | \\[512-512-1024\\]x2  | 0.86  | 0.88  |\n| DDSM  | VGG16  | 512x1  | 0.83  | 0.86  |\n| DDSM  | VGG16  | \\[512-512-1024\\]x2  | 0.85  | 0.88  |\n| DDSM | YaroslavNet | heatmap + max pooling + FC16-8 + shortcut | 0.83 | 0.86 |\n| INbreast  | VGG16  | 512x1  | 0.92  | 0.94  |\n| INbreast  | VGG16  | \\[512-512-1024\\]x2  | 0.95  | 0.96  |\n\n- Inference level augmentation is obtained by horizontal and vertical flips to generate 4 predictions.\n- The listed scores are single model AUC and prediction averaged AUC.\n- 3 Model averaging on DDSM gives AUC of 0.91\n- 2 Model averaging on INbreast gives AUC of 0.96.\n\n## Patch classifier model downloads\nSeveral patch classifier models (i.e. patch state) are also available for downloading at this Google Drive [folder](https://drive.google.com/drive/folders/0B1PVLadG_dCKZDVNYWZ1bll0cFU?resourcekey=0-EU80p95OCgKqOZZbvJIN-w&usp=sharing). Here is a table for model acc:\n\n| Model  | Train Set | Accuracy |\n|---|---|---|\n| Resnet50  | S10  | 0.89  |\n| VGG16  | S10  | 0.84  |\n| VGG19  | S10  | 0.79  |\n| YaroslavNet (Final) | S10 | 0.89 |\n| Resnet50  | S30  | 0.91  |\n| VGG16  | S30  | 0.86  |\n| VGG19  | S30  | 0.89  |\n\nWith patch classifier models, you can convert them into any whole image classifier by adding convolutional, FC and heatmap layers on top and see for yourself.\n\n## A bit explanation of this repository's file structure\n- The **.py** files under the root directory are Python modules to be imported.\n- You shall set the `PYTHONPATH` variable like this: `export PYTHONPATH=$PYTHONPATH:your_path_to_repos/end2end-all-conv` so that the Python modules can be imported.\n- The code for patch sampling, patch classifier and whole image training are under the [ddsm_train](./ddsm_train) folder.\n- [sample_patches_combined.py](./ddsm_train/sample_patches_combined.py) is used to sample patches from images and masks.\n- [patch_clf_train.py](./ddsm_train/patch_clf_train.py) is used to train a patch classifier.\n- [image_clf_train.py](./ddsm_train/image_clf_train.py) is used to train a whole image classifier, either on top of a patch classifier or from another already trained whole image classifier (i.e. finetuning).\n- There are multiple shell scripts under the [ddsm_train](./ddsm_train) folder to serve as examples.\n\n## Some input files' format\nI've got a lot of requests asking about the format of some input files. Here I provide the first few lines and hope they can be helpful:\n\n**roi_mask_path.csv**\n```\npatient_id,side,view,abn_num,pathology,type\nP_00005,RIGHT,CC,1,MALIGNANT,calc\nP_00005,RIGHT,MLO,1,MALIGNANT,calc\nP_00007,LEFT,CC,1,BENIGN,calc\nP_00007,LEFT,MLO,1,BENIGN,calc\nP_00008,LEFT,CC,1,BENIGN_WITHOUT_CALLBACK,calc\n```\n\n**pat_train.txt**\n```\nP_00601\nP_00413\nP_01163\nP_00101\nP_01122\n```\n\n## Transfer learning is as easy as 1-2-3\nIn order to transfer a model to your own data, follow these easy steps.\n### Determine the rescale factor\nThe rescale factor is used to rescale the pixel intensities so that the max value is 255. For PNG format, the max value is 65535, so the rescale factor is 255/65535 = 0.003891. If your images are already in the 255 scale, set rescale factor to 1.\n### Calculate the pixel-wise mean\nThis is simply the mean pixel intensity of your train set images.\n### Image size\nThis is currently fixed at 1152x896 for the models in this study. However, you can change the image size when converting from a patch classifier to a whole image classifier.\n### Finetune\nNow you can finetune a model on your own data for cancer predictions! You may check out this shell [script](ddsm_train/train_image_clf_inbreast.sh). Alternatively, copy & paste from here:\n```shell\nTRAIN_DIR=\"INbreast/train\"\nVAL_DIR=\"INbreast/val\"\nTEST_DIR=\"INbreast/test\"\nRESUME_FROM=\"ddsm_vgg16_s10_[512-512-1024]x2_hybrid.h5\"\nBEST_MODEL=\"INbreast/transferred_inbreast_best_model.h5\"\nFINAL_MODEL=\"NOSAVE\"\nexport NUM_CPU_CORES=4\n\npython image_clf_train.py \\\n    --no-patch-model-state \\\n    --resume-from $RESUME_FROM \\\n    --img-size 1152 896 \\\n    --no-img-scale \\\n    --rescale-factor 0.003891 \\\n    --featurewise-center \\\n    --featurewise-mean 44.33 \\\n    --no-equalize-hist \\\n    --batch-size 4 \\\n    --train-bs-multiplier 0.5 \\\n    --augmentation \\\n    --class-list neg pos \\\n    --nb-epoch 0 \\\n    --all-layer-epochs 50 \\\n    --load-val-ram \\\n    --load-train-ram \\\n    --optimizer adam \\\n    --weight-decay 0.001 \\\n    --hidden-dropout 0.0 \\\n    --weight-decay2 0.01 \\\n    --hidden-dropout2 0.0 \\\n    --init-learningrate 0.0001 \\\n    --all-layer-multiplier 0.01 \\\n    --es-patience 10 \\\n    --auto-batch-balance \\\n    --best-model $BEST_MODEL \\\n    --final-model $FINAL_MODEL \\\n    $TRAIN_DIR $VAL_DIR $TEST_DIR\n```\nSome explanations of the arguments:\n- The batch size for training is the product of `--batch-size` and `--train-bs-multiplier`. Because training uses roughtly twice (both forward and back props) the GPU memory of testing, `--train-bs-multiplier` is set to 0.5 here.\n- For model finetuning, only the second stage of the two-stage training is used here. So `--nb-epoch` is set to 0.\n- `--load-val-ram` and `--load-train-ram` will load the image data from the validation and train sets into memory. You may want to turn off these options if you don't have sufficient memory. When turned off, out-of-core training will be used.\n- `--weight-decay` and `--hidden-dropout` are for stage 1. `--weight-decay2` and `--hidden-dropout2` are for stage 2.\n- The learning rate for stage 1 is `--init-learningrate`. The learning rate for stage 2 is the product of `--init-learningrate` and `--all-layer-multiplier`.\n\n## Computational environment\nThe research in this study is carried out on a Linux workstation with 8 CPU cores and a single NVIDIA Quadro M4000 GPU with 8GB memory. The deep learning framework is Keras 2 with Tensorflow as the backend. \n### About Keras version\nIt is known that Keras >= 2.1.0 can give errors due an API change. See issue [#7](https://github.com/lishen/end2end-all-conv/issues/7). Use Keras with version < 2.1.0. For example, Keras=2.0.8 is known to work.\n\n# TERMS OF USE\nAll data is free to use for non-commercial purposes. For commercial use please contact [MSIP](https://www.ip.mountsinai.org/).\n\n\n\n"
    },
    {
        "repo": "/ImagingLab/ICIAR2018",
        "language": "Python",
        "readme_contents": "# ICIAR2018\n### Two-Stage Convolutional Neural Network for Breast Cancer Histology Image Classification\nThis repository is the part A of the ICIAR 2018 Grand Challenge on BreAst Cancer Histology (BACH) images for automatically classifying H&E stained breast histology microscopy images in four classes: normal, benign, in situ carcinoma and invasive carcinoma. \n\nWe are presenting a CNN approach using two convolutional networks to classify histology images in a patchwise fashion. The first network, receives overlapping patches (35 patches) of the whole-slide image and learns to generate spatially smaller outputs. The second network is trained on the downsampled patches of the whole image using the output of the first network. The number of channels in the input to the second network is equal to the total number of patches extracted from the microscopy image in a non-overlapping fashion (12 patches) times the depth of the feature maps generted by the first network (C): \n<p align='center'>  \n  <img src='img/network.png' width='600' height='280' />\n</p>\n\n## Prerequisites\n- Linux\n- Python 3\n- NVIDIA GPU (12G or 24G memory) + CUDA cuDNN\n\n## Getting Started\n### Installation\n- Clone this repo:\n```bash\ngit clone https://github.com/ImagingLab/ICIAR2018\ncd ICIAR2018\n```\n- Install PyTorch and dependencies from http://pytorch.org\n- Install python requirements:\n```bash\npip install -r requirements.txt\n```\n\n### Dataset\n- We use the ICIAR2018 dataset. To train a model on the full dataset, please download it from the [official website](https://iciar2018-challenge.grand-challenge.org/dataset/) (registration required). The dataset is composed of 400 high resolution Hematoxylin and Eosin (H&E) stained breast histology microscopy images labelled as normal, benign, in situ carcinoma, and invasive carcinoma (100 images for each category):\n<p align='center'>  \n  <img src='img/dataset.jpg' width='600' height='134' />\n</p>\nAfter downloading, please put it under the `datasets` folder in the same way the sub-directories are provided.\n\n\n### Testing\n- The pre-trained ICIAR2018 dataset model resides under `./checkpoints`.\n- To test the model, run `test.py` script\n- Use `--testset-path` command-line argument to provide the path to the `test` folder.\n```bash\npython test.py --testset-path ./dataset/test \n```\n- If you don't provide the test-set path, an open-file dialogbox will appear to select an image for test.\nThe test results will be printed on the screen.\n\n\n\n### Training\n- To train the model, run `train.py` script\n```bash\npython train.py\n```\n- To change the number of feature-maps generated by the patch-wise network use `--channels` argument:\n```bash\npython train.py --channels 1\n```\n\n\n### Validation & ROC Curves\n- To validate the model on the validation set and plot the ROC curves, run `validate.py` script\n```bash\npython validate.py\n```\n- To change the number of feature-maps generated by the patch-wise network use `--channels` argument:\n```bash\npython train.py --channels 1\n```\n\n## Citation\nIf you use this code for your research, please cite our paper <a href=\"https://arxiv.org/abs/1803.04054\">Two-Stage Convolutional Neural Network for Breast Cancer Histology Image Classification</a>:\n\n```\n@inproceedings{nazeri2018two,\n  title={Two-Stage Convolutional Neural Network for Breast Cancer Histology Image Classification},\n  author={Nazeri, Kamyar and Aminpour, Azad and Ebrahimi, Mehran},\n  booktitle={International Conference Image Analysis and Recognition},\n  pages={717--726},\n  year={2018},\n  organization={Springer}\n}\n```\n"
    },
    {
        "repo": "/Jean-njoroge/Breast-cancer-risk-prediction",
        "language": "Jupyter Notebook",
        "readme_contents": "# Breast-cancer-risk-prediction\n\n> Necessity, who is the mother of invention. \u2013 Plato*\n\n## Welcome to my GitHub repository on Using Predictive Analytics model to diagnose breast cancer.\n---\n\n### Objective:\nThe repository is a learning exercise to:\n* Apply the fundamental concepts of machine learning from an available dataset\n* Evaluate and interpret my results and justify my interpretation based on observed data set\n* Create notebooks that serve as computational records and document my thought process. \n\nThe analysis is divided into four sections, saved in juypter notebooks in this repository\n1. Identifying the problem  and Data Sources\n2. Exploratory Data Analysis\n3. Pre-Processing the Data\n4. Build model to predict whether breast cell tissue is  malignant or Benign\n\n### [Notebook 1](https://github.com/ShiroJean/Breast-cancer-risk-prediction/blob/master/NB1_IdentifyProblem%2BDataClean.ipynb): Identifying the problem and Getting data.\n**Notebook goal:Identify the types of information contained in our data set**\nIn this notebook I used Python modules to import external data sets for the purpose of getting to know/familiarize myself with the data to get a good grasp of the data and think about how to handle the data in different ways.\u00a0\n### [Notebook 2](https://github.com/ShiroJean/Breast-cancer-risk-prediction/blob/master/NB2_ExploratoryDataAnalysis.ipynb) Exploratory Data Analysis\n**Notebook goal: \u00a0Explore the variables to assess how they relate to the response variable** \nIn this notebook, I am getting familiar with the data using data exploration and visualization techniques using python libraries (Pandas, matplotlib, seaborn. Familiarity with the data is important which will provide useful knowledge for data pre-processing)\n### [Notebook 3](https://github.com/ShiroJean/Breast-cancer-risk-prediction/blob/master/NB3_DataPreprocesing.ipynb) Pre-Processing the data\n**Notebook goal:Find the most predictive features of the data and filter it so it will enhance the predictive power of the analytics model.**\nIn this notebook I use feature selection to reduce high-dimension data, feature extraction and transformation for dimensionality reduction. This is essential in preparing the data before predictive models are developed.\n### [Notebook 4](https://github.com/ShiroJean/Breast-cancer-risk-prediction/blob/master/NB4_PredictiveModelUsingSVM.ipynb) Predictive model using Support Vector Machine (svm)\n**Notebook goal: Construct predictive models to predict the diagnosis of a breast tumor.** \nIn this notebook, I construct a predictive model using SVM machine learning algorithm to predict the diagnosis of a breast tumor. The diagnosis of a breast tumor is a binary variable (benign or malignant). I also evaluate the model using confusion matrix the receiver operating curves (ROC), which are essential in assessing and interpreting the fitted model.\n\n### [Notebook 5](https://github.com/ShiroJean/Breast-cancer-risk-prediction/blob/master/NB_5%20OptimizingSVMClassifier.ipynb): Optimizing the  Support Vector Classifier\n**Notebook goal: Construct predictive models to predict the diagnosis of a breast tumor.** \nIn this notebook, I aim to tune parameters of the SVM Classification model using scikit-learn.\n\n"
    },
    {
        "repo": "/abhinavsagar/breast-cancer-classification",
        "language": "Jupyter Notebook",
        "readme_contents": "# Breast-cancer-classification\n\nBreast Cancer Classification using CNN and transfer learning\n\n## Citing\n\nIf you find this code useful in your research, please consider citing the blog:\n\n```\n@misc{sagarconvolutional,\n  Author = {Abhinav Sagar},\n  Title = {Convolutional Neural Network for Breast Cancer Classification},\n  Year = {2019},\n  Journal = {Towards Data Science},\n}\n```\n\n## IMPORTANT\n\nAbsolutely, under NO circumstance, should one ever screen patients using computer vision software trained with this code (or any home made software for that matter). \n\nCheck out the corresponding medium blog post [https://towardsdatascience.com/convolutional-neural-network-for-breast-cancer-classification-52f1213dcc9](https://towardsdatascience.com/convolutional-neural-network-for-breast-cancer-classification-52f1213dcc9).\n\n## Data\n\nThe dataset can be downloaded from [here](https://web.inf.ufpr.br/vri/databases/breast-cancer-histopathological-database-breakhis/). This is a binary classification problem. I split the data as shown-\n\n```\ndataset train\n  benign\n   b1.jpg\n   b2.jpg\n   //\n  malignant\n   m1.jpg\n   m2.jpg\n   //  validation\n   benign\n    b1.jpg\n    b2.jpg\n    //\n   malignant\n    m1.jpg\n    m2.jpg\n    //...\n```    \n\n## Environment and tools\n\n1. Jupyter Notebook\n2. Numpy\n3. Pandas\n4. Scikit-image\n5. Matplotlib\n6. Scikit-learn\n7. Keras\n\n## Installation\n\n`pip install numpy pandas scikit-image matplotlib scikit-learn keras`\n\n`jupyter notebook`\n\n## Model\n\n![model](images/image6.png)\n\n## Results\n\n### Loss/Accuracy vs Epoch\n\n![loss/accuracy](images/image1.png)\n\n![loss/accuracy](images/image2.png)\n\n### Confusion Matrix\n\n![roc-auc](images/image3.png)\n\n### ROC-AUC curve\n\n![roc-auc](images/image4.png)\n\n### Correct/Incorrect classification samples\n\n![results](images/image5.png)\n\n\n![results](images/image7.png)\n\nThe model is able to reach a validation accuracy of 98.3%, precision 0.65, recall 0.95, f1 score of 0.77 and ROC-AUC as 0.692.\n\n## References\n\n1. https://peerj.com/articles/6201.pdf\n\n2. https://arxiv.org/pdf/1811.04241\n\n3. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6440620/\n\n## License\n\n```\nMIT License\n\nCopyright (c) 2019 Abhinav Sagar\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n```\n"
    },
    {
        "repo": "/bupt-ai-cz/BCI",
        "language": "Python",
        "readme_contents": "# BCI: Breast Cancer Immunohistochemical Image Generation through Pyramid Pix2pix ![visitors](https://visitor-badge.glitch.me/badge?page_id=bupt-ai-cz.BCI)\n[Project](https://bupt-ai-cz.github.io/BCI/) | [Arxiv](https://arxiv.org/pdf/2204.11425v1.pdf) | [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bci-breast-cancer-immunohistochemical-image/image-to-image-translation-on-bci)](https://paperswithcode.com/sota/image-to-image-translation-on-bci?p=bci-breast-cancer-immunohistochemical-image)  [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bci-breast-cancer-immunohistochemical-image/image-to-image-translation-on-llvip)](https://paperswithcode.com/sota/image-to-image-translation-on-llvip?p=bci-breast-cancer-immunohistochemical-image) | [![Tweet](https://img.shields.io/twitter/url/http/shields.io.svg?style=social)](https://twitter.com/intent/tweet?text=Codes%20and%20Data%20for%20Paper:%20\"BCI:%20Breast%20Cancer%20Immunohistochemical%20Image%20Generation%20through%20Pyramid%20Pix2pix\"%20&url=https://github.com/bupt-ai-cz/BCI)  \n\n## News\n- \u26a1(2022-10-20): We have released the trained [model](https://github.com/bupt-ai-cz/BCI#trained-models) on BCI and LLVIP datasets.\n- \u26a1(2022-6-29): We host a competition for breast cancer immunohistochemistry image generation on [Grand Challenge](https://bci.grand-challenge.org/)\uff01\n- \u26a1(2022-4-26): We have released BCI dataset and the code of PyramidPix2pix. You can download BCI dataset from the [homepage](https://bupt-ai-cz.github.io/BCI/).\n\n<!--\n- \u26a1(2022-4-25): The data is available [HERE!](https://bupt-ai-cz.github.io/BCI/)\n-->\n---\n\n![datasetview_github](imgs/datasetpreview6.png)\n\n---\n## Framework\n![framework](https://github.com/bupt-ai-cz/BCI/blob/main/imgs/framework.png)\n## Setup\n### 1)Envs\n- Linux\n- Python>=3.6\n- CPU or NVIDIA GPU + CUDA CuDNN\n\nInstall python packages\n```\ngit clone https://github.com/bupt-ai-cz/BCI\ncd PyramidPix2pix\npip install -r requirements.txt\n```\n### 2)Prepare dataset\n- Download BCI dataset from our homepage.\n- Combine HE and IHC images.\n\n  Project [pix2pix](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix) provides a python script to generate pix2pix training data in the form of pairs of images {A,B}, where A and B are two different depictions of the same underlying scene, these can be pairs {HE, IHC}. Then we can learn to translate A(HE images) to B(IHC images).\n\n  Create folder `/path/to/data` with subfolders `A` and `B`. `A` and `B` should each have their own subfolders `train`, `val`, `test`, etc. In `/path/to/data/A/train`, put training images in style A. In `/path/to/data/B/train`, put the corresponding images in style B. Repeat same for other data splits (`val`, `test`, etc).\n\n  Corresponding images in a pair {A,B} must be the same size and have the same filename, e.g., `/path/to/data/A/train/1.jpg` is considered to correspond to `/path/to/data/B/train/1.jpg`.\n\n  Once the data is formatted this way, call:\n  ```\n  python datasets/combine_A_and_B.py --fold_A /path/to/data/A --fold_B /path/to/data/B --fold_AB /path/to/data\n  ```\n\n  This will combine each pair of images (A,B) into a single image file, ready for training.\n\n- File structure\n  ```\n  PyramidPix2pix\n    \u251c\u2500\u2500datasets\n         \u251c\u2500\u2500 BCI\n               \u251c\u2500\u2500train\n               |    \u251c\u2500\u2500 00000_train_1+.png\n               |    \u251c\u2500\u2500 00001_train_3+.png\n               |    \u2514\u2500\u2500 ...\n               \u2514\u2500\u2500test\n                    \u251c\u2500\u2500 00000_test_1+.png\n                    \u251c\u2500\u2500 00001_test_2+.png\n                    \u2514\u2500\u2500 ...\n\n  ```\n## Train\nTrain at full resolution(1024*1024): \n```\npython train.py --dataroot ./datasets/BCI --gpu_ids 0 --pattern L1_L2_L3_L4\n```\nBy default, four scales of the pyramid are used for supervision. You can change the option `--pattern` to use less scales (e.g. `--pattern L1_L2_L3`).\n\nTrain at resolution 512*512 (less GPU memory required):\n```\npython train.py --dataroot ./datasets/BCI --preprocess crop --crop_size 512 --gpu_ids 0 --pattern L1_L2_L3_L4\n```\nImages are randomly cropped if trained at low resolution.\n## Test\nTest at full resolution(1024*1024): \n```\npython test.py --dataroot ./datasets/BCI --gpu_ids 0\n```\nTest at resolution 512*512:\n```\npython test.py --dataroot ./datasets/BCI --preprocess crop --crop_size 512 --gpu_ids 0\n```\nSee `PyramidPix2pix/options` for more train and test options.\n<!-- The testing process requires less memory, we recommend testing at full resolution, regardless of the resolution used in the training process. -->\n## Evaluate\nCalculate average PSNR and SSIM.\n```\npython evaluate.py --result_path ./results/pyramidpix2pix\n```\n## Trained models\n| dataset |     device      | backbone | PSNR | SSIM | model |\n|---------|-----------------|-----------------------|------------|--------|-------|\n|  BCI    | Tesla V100-32GB | resnet_9  | 21.16    | 0.477  |   [download](https://github.com/bupt-ai-cz/BCI/releases/download/v1.0/trained_on_BCI.zip)    |\n| LLVIP   | Tesla V100-32GB | resnet_9  | 12.189   | 0.279  |   [download](https://github.com/bupt-ai-cz/BCI/releases/download/v1.0/trained_on_LLVIP.zip)    |\n\n## Results\n![visualization](imgs/results1.png)\n![results2](https://github.com/bupt-ai-cz/BCI/blob/main/imgs/results2.png) | ![results3](https://github.com/bupt-ai-cz/BCI/blob/main/imgs/results3.png)\n---|---\n![results4](https://github.com/bupt-ai-cz/BCI/blob/main/imgs/results4.png) | ![results5](https://github.com/bupt-ai-cz/BCI/blob/main/imgs/results5.png)\n\n\n## Citation\n```\n@InProceedings{Liu_2022_CVPR,\n    author    = {Liu, Shengjie and Zhu, Chuang and Xu, Feng and Jia, Xinyu and Shi, Zhongyue and Jin, Mulan},\n    title     = {BCI: Breast Cancer Immunohistochemical Image Generation Through Pyramid Pix2pix},\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},\n    month     = {June},\n    year      = {2022},\n    pages     = {1815-1824}\n}\n```\n## Contact\nShengjie Liu (shengjie.Liu@bupt.edu.cn)\n\nChuang Zhu (czhu@bupt.edu.cn)\n\nIf you have any questions, you can contact us directly.\n"
    },
    {
        "repo": "/nyukat/BIRADS_classifier",
        "language": "Python",
        "readme_contents": "# High-resolution breast cancer screening with multi-view deep convolutional neural networks\n## Introduction\nThis is an implementation of the model used for [BI-RADS](https://breast-cancer.ca/bi-rads/) classification as described in our paper [\"High-resolution breast cancer screening with multi-view deep convolutional neural networks\"](https://arxiv.org/abs/1703.07047). The implementation allows users to get the BI-RADS prediction by applying our pretrained CNN model on standard screening mammogram exam with four views. As a part of this repository, we provide a sample exam (in `images` directory). The model is implemented in both TensorFlow and PyTorch.\n\n## Prerequisites\n\n* Python (3.6)\n* TensorFlow (1.5.0) or PyTorch (0.4.0)\n* NumPy (1.14.3)\n* SciPy (1.0.0)\n* Pillow (5.1.0)\n\n## Data\n\nTo use the pretrained model, the input is required to consist of four images, one for each view (L-CC, L-MLO, R-CC, R-MLO). Each image has to have the size of 2600x2000 pixels. The images in the provided sample exam were already cropped to the correct size.\n\n## How to run the code\nAvailable options can be found at the bottom of the files `birads_prediction_tf.py` or `birads_prediction_torch.py`.\n\nRun the following command to use the model.\n\n```bash\n# Using TensorFlow\npython birads_prediction_tf.py\n\n# Using PyTorch\npython birads_prediction_torch.py\n```\n\nThis loads an included sample of four scan views, feeds them into a pretrained copy of our model, and outputs the predicted probabilities of each BI-RADS classification.\n\nYou should get the following output:\n\n```\nBI-RADS prediction:\n        BI-RADS 0:      0.21831559\n        BI-RADS 1:      0.38092783\n        BI-RADS 2:      0.4007566\n```\n\n## Additional options\n\nAdditional flags can be provided to the above script:\n\n* `--model-path`: path to a TensorFlow checkpoint or PyTorch pickle of a saved model. By default, this points to the saved model in this repository.\n* `--device-type`: whether to use a CPU or GPU. By default, the CPU is used.\n* `--gpu-number`: which GPU is used. By default, GPU 0 is used. (Not used if running with CPU)\n* `--image-path`: path to saved images. By default, this points to the saved images in this repository. \n\nFor example, to run this script using TensorFlow on GPU 2, run:\n\n```bash\npython birads_prediction_tf.py --device-type gpu --gpu-number 2\n```\n\n## Converting TensorFlow Models\n\nThis repository contains pretrained models in both TensorFlow and PyTorch. The model was originally trained in TensorFlow and translated to PyTorch using the following script:\n\n```bash\npython convert_model.py saved_models/model.ckpt saved_models/model.p\n```\n\n## Tests\n\nTests can be configured to your environment.\n\n```bash\n# Using TensorFlow, without GPU support\npython test_inference.py --using tf\n\n# Using PyTorch, without GPU support\npython test_inference.py --using torch\n\n# Using TensorFlow, with GPU support\npython test_inference.py --using tf --with-gpu\n\n# Using PyTorch, with GPU support\npython test_inference.py --using torch --with-gpu\n```\n\n## Reference\n\nIf you found this code useful, please cite our paper:\n\n**\"High-resolution breast cancer screening with multi-view deep convolutional neural networks\"**\\\nKrzysztof J. Geras, Stacey Wolfson, Yiqiu Shen, Nan Wu, S. Gene Kim, Eric Kim, Laura Heacock, Ujas Parikh, Linda Moy, Kyunghyun Cho\\\n2017\n\n    @article{geras2017high, \n        title = {High-resolution breast cancer screening with multi-view deep convolutional neural networks},\n        author = {Krzysztof J. Geras and Stacey Wolfson and Yiqiu Shen and Nan Wu and S. Gene Kim and Eric Kim and Laura Heacock and Ujas Parikh and Linda Moy and Kyunghyun Cho}, \n        journal = {arXiv:1703.07047},\n        year = {2017}\n    }\n"
    },
    {
        "repo": "/nyukat/GMIC",
        "language": "Jupyter Notebook",
        "readme_contents": "# An interpretable classifier for high-resolution breast cancer screening images utilizing weakly supervised localization\n\n## Introduction\nThis is an implementation of the Globally-Aware Multiple Instance Classifier (GMIC) model as described in [our paper](https://arxiv.org/abs/2002.07613). The architecture of the proposed model is shown below.\n\n<p align=\"center\">\n  <img width=\"793\" height=\"729\" src=\"https://github.com/nyukat/GMIC/blob/master/mia_structure.png\">\n</p>\n\nHighlights of GMIC:\n- **High Accuracy**: GMIC outperformed ResNet-34 and Faster R-CNN.\n- **High Efficiency**: Compared to ResNet-34, GMIC has **28.8%** fewer parameters, uses **78.43%** less GPU memory and is **4.1x** faster during inference and **5.6x** faster during training.\n- **Weakly Supervised Lesion Localization**: Despite being trained with only image-level labels indicating the presence of any benign or malignant lesion, GMIC is able to generate pixel-level saliency maps (shown below) that provide additional interpretability.\n\nThe implementation allows users to obtain breast cancer predictions and visualization of saliency maps by applying one of our pretrained models. We provide weights for 5 GMIC-ResNet-18 models. The model is implemented in PyTorch. \n\n* Input: A mammography image that is cropped to 2944 x 1920 and are saved as 16-bit png files. As a part of this repository, we provide 4 sample exams (in `sample_data/images` directory and exam list stored in `sample_data/exam_list_before_cropping.pkl`), each of which includes 2 CC view images and 2 MLO view images. Those exams contain original mammogrphy images and therefore need to be preprocessed (see the Preprocessing section). \n\n* Output: The GMIC model generates one prediction for each image: probability of benign and malignant findings. All predictions are saved into a csv file `$OUTPUT_PATH/predictions.csv` that contains the following columns: image_index, benign_pred, malignant_pred, benign_label, malignant_label. In addition, each input image is associated with a visualization file saved under `$OUTPUT_PATH/visualization`. An exemplar visualization file is illustrated below. The images (from left to right) represent:\n  * input mammography with ground truth annotation (green=benign, red=malignant),\n  * patch map that illustrates the locations of ROI proposal patches (blue squares),\n  * saliency map for benign class,\n  * saliency map for malignant class,\n  * 6 ROI proposal patches with the associated attention score on top.\n  \n![alt text](https://github.com/nyukat/GMIC/blob/master/sample_data/sample_visualization.png)\n\n**Update (2021/03/08)**: Updated the documentation\n\n**Update (2020/12/15)**: Added the preprocessing pipeline.\n\n**Update (2020/12/16)**: Added the [example notebook](https://github.com/nyukat/GMIC/blob/master/example_notebook.ipynb).\n\n\n## Prerequisites\n\n* Python (3.6)\n* PyTorch (1.1.0)\n* torchvision (0.2.2)\n* NumPy (1.14.3)\n* SciPy (1.0.0)\n* H5py (2.7.1)\n* imageio (2.4.1)\n* pandas (0.22.0)\n* opencv-python (3.4.2)\n* tqdm (4.19.8)\n* matplotlib (3.0.2)\n\n\n## License\n\nThis repository is licensed under the terms of the GNU AGPLv3 license.\n\n## How to run the code\n\nYou need to first install conda in your environment. **Before running the code, please run `pip install -r requirements.txt` first.** Once you have installed all the dependencies, `run.sh` will automatically run the entire pipeline and save the prediction results in csv. Note that you need to first cd to the project directory and then execute `. ./run.sh`. When running the individual Python scripts, please include the path to this repository in your `PYTHONPATH`. \n\nWe recommend running the code with a GPU. To run the code with CPU only, please change `DEVICE_TYPE` in run.sh to 'cpu'. \n\nThe following variables defined in `run.sh` can be modified as needed:\n* `MODEL_PATH`: The path where the model weights are saved.\n* `CROPPED_IMAGE_PATH`: The directory where cropped mammograms are saved.\n* `SEG_PATH`: The directory where ground truth segmenations are saved.\n* `EXAM_LIST_PATH`: The path where the exam list is stored.\n* `OUTPUT_PATH`: The path where visualization files and predictions will be saved.\n* `DEVICE_TYPE`: Device type to use in heatmap generation and classifiers, either 'cpu' or 'gpu'.\n* `GPU_NUMBER`: GPUs number multiple GPUs are available.\n* `MODEL_INDEX`: Which one of the five models to use. Valid values include {'1', '2', '3', '4', '5','ensemble'}.\n* `visualization-flag`: Whether to generate visualization.\n\n\nYou should obtain the following outputs for the sample exams provided in the repository (found in `sample_output/predictions.csv` by default). \n\nimage_index  |  benign_pred  |  malignant_pred  |  benign_label  |  malignant_label\n-------------|---------------|------------------|----------------|-----------------\n0_L-CC       |  0.1356       |  0.0081          |  0             |  0\n0_R-CC       |  0.8929       |  0.3259          |  1             |  0\n0_L-MLO      |  0.2368       |  0.0335          |  0             |  0\n0_R-MLO      |  0.9509       |  0.1812          |  1             |  0\n1_L-CC       |  0.0546       |  0.0168          |  0             |  0\n1_R-CC       |  0.5986       |  0.9910          |  0             |  1\n1_L-MLO      |  0.0414       |  0.0139          |  0             |  0\n1_R-MLO      |  0.5383       |  0.9308          |  0             |  1\n2_L-CC       |  0.0678       |  0.0227          |  0             |  0\n2_R-CC       |  0.1917       |  0.0603          |  1             |  0\n2_L-MLO      |  0.1210       |  0.0093          |  0             |  0\n2_R-MLO      |  0.2440       |  0.0231          |  1             |  0\n3_L-CC       |  0.6295       |  0.9326          |  0             |  1\n3_R-CC       |  0.2291       |  0.1603          |  0             |  0\n3_L-MLO      |  0.6304       |  0.7496          |  0             |  1\n3_R-MLO      |  0.0622       |  0.0507          |  0             |  0\n\n\n## Data\n\n`sample_data/images` contains 4 exams each of which includes 4 the original mammography images (L-CC, L-MLO, R-CC, R-MLO). All mammography images are saved in png format. The original 12-bit mammograms are saved as rescaled 16-bit images to preserve the granularity of the pixel intensities, while still being correctly displayed in image viewers.\n\n`sample_data/segmentation` contains the binary pixel-level segmentation labels for some exams. All segmentations are saved as png images.\n\n`sample_data/exam_list_before_cropping.pkl` contains a list of exam information. Each exam is represented as a dictionary with the following format:\n\n```python\n{'horizontal_flip': 'NO',\n  'L-CC': ['0_L-CC'],\n  'L-MLO': ['0_L-MLO'],\n  'R-MLO': ['0_R-MLO'],\n  'R-CC': ['0_R-CC'],\n  'best_center': {'R-CC': [(1136.0, 158.0)],\n   'R-MLO': [(1539.0, 252.0)],\n   'L-MLO': [(1530.0, 307.0)],\n   'L-CC': [(1156.0, 262.0)]},\n  'cancer_label': {'benign': 1,\n   'right_benign': 0,\n   'malignant': 0,\n   'left_benign': 1,\n   'unknown': 0,\n   'right_malignant': 0,\n   'left_malignant': 0},\n  'L-CC_benign_seg': ['0_L-CC_benign'],\n  'L-CC_malignant_seg': ['0_L-CC_malignant'],\n  'L-MLO_benign_seg': ['0_L-MLO_benign'],\n  'L-MLO_malignant_seg': ['0_L-MLO_malignant'],\n  'R-MLO_benign_seg': ['0_R-MLO_benign'],\n  'R-MLO_malignant_seg': ['0_R-MLO_malignant'],\n  'R-CC_benign_seg': ['0_R-CC_benign'],\n  'R-CC_malignant_seg': ['0_R-CC_malignant']}\n```\nIn their original formats, images from `L-CC` and `L-MLO` views face right, and images from `R-CC` and `R-MLO` views face left. We horizontally flipped `R-CC` and `R-MLO` images so that all four views face right. Values for `L-CC`, `R-CC`, `L-MLO`, and `R-MLO` are list of image filenames without extensions and directory name. \n\n### Preprocessing\n\nRun the following commands to crop mammograms and calculate information about augmentation windows.\n\n#### Crop mammograms\n```bash\npython3 src/cropping/crop_mammogram.py \\\n    --input-data-folder $DATA_FOLDER \\\n    --output-data-folder $CROPPED_IMAGE_PATH \\\n    --exam-list-path $INITIAL_EXAM_LIST_PATH  \\\n    --cropped-exam-list-path $CROPPED_EXAM_LIST_PATH  \\\n    --num-processes $NUM_PROCESSES\n```\n`src/import_data/crop_mammogram.py` crops the mammogram around the breast and discards the background in order to improve image loading time and time to run segmentation algorithm and saves each cropped image to `$PATH_TO_SAVE_CROPPED_IMAGES/short_file_path.png` using h5py. In addition, it adds additional information for each image and creates a new image list to `$CROPPED_IMAGE_LIST_PATH` while discarding images which it fails to crop. Optional --verbose argument prints out information about each image. The additional information includes the following:\n- `window_location`: location of cropping window w.r.t. original dicom image so that segmentation map can be cropped in the same way for training.\n- `rightmost_points`: rightmost nonzero pixels after correctly being flipped.\n- `bottommost_points`: bottommost nonzero pixels after correctly being flipped.\n- `distance_from_starting_side`: records if zero-value gap between the edge of the image and the breast is found in the side where the breast starts to appear and thus should have been no gap. Depending on the dataset, this value can be used to determine wrong value of `horizontal_flip`.\n\n\n#### Calculate optimal centers\n```bash\npython3 src/optimal_centers/get_optimal_centers.py \\\n    --cropped-exam-list-path $CROPPED_EXAM_LIST_PATH \\\n    --data-prefix $CROPPED_IMAGE_PATH \\\n    --output-exam-list-path $EXAM_LIST_PATH \\\n    --num-processes $NUM_PROCESSES\n```\n`src/optimal_centers/get_optimal_centers.py` outputs new exam list with additional metadata to `$EXAM_LIST_PATH`. The additional information includes the following:\n- `best_center`: optimal center point of the window for each image. The augmentation windows drawn with `best_center` as exact center point could go outside the boundary of the image. This usually happens when the cropped image is smaller than the window size. In this case, we pad the image and shift the window to be inside the padded image in augmentation. Refer to [the data report](https://cs.nyu.edu/~kgeras/reports/datav1.0.pdf) for more details.\n\n#### Outcomes of preprocessing\nAfter the preprocessing step, you should have the following files in the `$OUTPUT_PATH` directory (default is sample_output):\n- cropped_images: a folder that contains the cropped images corresponding to all images in the sample_data/images.\n- data.pkl: the pickle file of a data list that includes the preprocessing metadata for each image and exam.\n\n\n## Reference\n\nIf you found this code useful, please cite our paper:\n\n**An interpretable classifier for high-resolution breast cancer screening images utilizing weakly supervised localization**\\\nYiqiu Shen, Nan Wu, Jason Phang, Jungkyu Park, Kangning Liu, Sudarshini Tyagi, Laura Heacock, S. Gene Kim, Linda Moy, Kyunghyun Cho and Krzysztof J. Geras\\\nMedical Image Analysis\n2020\n    \n    @article{shen2020interpretable, \n    title={An interpretable classifier for high-resolution breast cancer screening images utilizing weakly supervised localization},\n    author={Shen, Yiqiu and Wu, Nan and Phang, Jason and Park, Jungkyu and Liu, Kangning and Tyagi, Sudarshini and Heacock, Laura and Kim, S Gene and Moy, Linda and Cho, Kyunghyun and others},\n    journal={Medical Image Analysis},\n    pages={101908},\n    year={2020},\n    publisher={Elsevier}\n}\n\n\nReference to previous GMIC version:\n\n**Globally-Aware Multiple Instance Classifier for Breast Cancer Screening**\\\nYiqiu Shen, Nan Wu, Jason Phang, Jungkyu Park, S. Gene Kim, Linda Moy, Kyunghyun Cho and Krzysztof J. Geras\\\nMachine Learning in Medical Imaging - 10th International Workshop, MLMI 2019, Held in Conjunction with MICCAI 2019, Proceedings. Springer , 2019. p. 18-26 (Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics); Vol. 11861 LNCS).\n    \n    @inproceedings{shen2019globally, \n    title={Globally-Aware Multiple Instance Classifier for Breast Cancer Screening},\n        author={Shen, Yiqiu and Wu, Nan and Phang, Jason and Park, Jungkyu and Kim, Gene and Moy, Linda and Cho, Kyunghyun and Geras, Krzysztof J},\n        booktitle={Machine Learning in Medical Imaging: 10th International Workshop, MLMI 2019, Held in Conjunction with MICCAI 2019, Shenzhen, China, October 13, 2019, Proceedings},\n        volume={11861},\n        pages={18-26},\n        year={2019},\n        organization={Springer Nature}}\n"
    },
    {
        "repo": "/srimani-programmer/Breast-Cancer-Predictor",
        "language": "Python",
        "readme_contents": "# Breast Cancer Prediction\n> Predicts whether the type of breast cancer is Malignant or Benign\n\n![Issues](https://img.shields.io/github/issues/srimani-programmer/Breast-Cancer-Predictor)\n![Pull Requests](https://img.shields.io/github/issues-pr/srimani-programmer/Breast-Cancer-Predictor)\n![Forks](https://img.shields.io/github/forks/srimani-programmer/Breast-Cancer-Predictor)\n![Stars](https://img.shields.io/github/stars/srimani-programmer/Breast-Cancer-Predictor)\n[![License](https://img.shields.io/github/license/srimani-programmer/Breast-Cancer-Predictor)](https://github.com/srimani-programmer/Breast-Cancer-Predictor/blob/master/LICENSE)\n\n## Please follow the Code of Conduct: [Code of Conduct](https://github.com/srimani-programmer/Breast-Cancer-Predictor/blob/master/CODE_OF_CONDUCT.md)\n# Aim of the Project\n#### > To predict if a breast cancer is Malignant or Benign using Image Dataset as well as Numerical Data\n#### > Apply ML and DL Models to predict the severity of the Breast-Cancer\n#### > Create a Wonderful UI for this project using Front End Languages and Frameworks (Like Bootstrap)\n#### > Create the Backend using Flask Framework.\n#### > Deploy on Cloud and make this wonderful project available to public\n\n#### Note: Kindly do not push any changes to Main or Master Branch. A New Branch named \"New_Pipeline\" is already created and push all the changes to this branch\n#### Don't forget to create an issue before making a PR\n:point_right: Repo Link : [https://github.com/srimani-programmer/Breast-Cancer-Predictor/](https://github.com/srimani-programmer/Breast-Cancer-Predictor/)\n\n## Table of contents\n* [About Project](#about-project)\n* [Languages or Frameworks Used](#languages-or-frameworks-used)\n* [Setup](#project-setup)\n* [Application UI](#Application-ui)\n\n## About Project:\n\nBreast cancer is the most common type of cancer in women. When cancers are found early, they can often be cured. \nSome devices detect breast cancer but many times they lead to false positives, which results \nin patients undergoing painful, expensive surgeries that were not even necessary. These type of cancers are called \n**benign** which do not require surgeries and we can reduce these unnecessary surgeries by using Machine Learning. \nI have taken the dataset of the previous breast cancer patients and train the model to predict whether the cancer is **benign** or **malignant**. These predictions will help doctors to do surgeries only when the cancer is malignant, thus reducing the unnecessary surgeries for women. \n \nFor building the project I have used Wisconsin Breast cancer data which has 569 rows of which 357 are benign and 212 are malignant. \nThe data is prepossessed and scaled. I have trained with Random forest Classifier gives the best accuracy of 95.0%. To provide an easy-to-use interface to doctors I have developed a website that will take the data and display the output with accuracy and time taken to predict.\n\n\n## Languages or Frameworks Used \n\n  * Python: language\n  * NumPy: library for numerical calculations\n  * Pandas: library for data manipulation and analysis\n  * SkLearn: library which features various classification, regression and clustering algorithms\n  * Flask: microframework for building web applications using Python.\n  \n## Project Setup\n  \n  * First Clone the repository.\n  * Create the virtual environment for the project. \n  ```sh\n  $ conda create -n myenv python=3.6\n  ```\n  * Install the required packages using requirements.txt inside the environemnt using pip.\n  ```sh\n  $ pip install -r requirements.txt\n  ```\n  * run the app.py as `python app.py`\n  * Web Application will be hosted at  `127.0.0.1:5000`\n  * Enter the URL in the browser Application will be hosted.\n  * Enter the details of the tumor to detect the type of the cancer with more than 95% accuracy.\n\n## Steps to follow :scroll:\n\n### 0. Star The Repo :star2:\n\nStar the repo by pressing the topmost-right button to start your wonderful journey.\n\n\n### 1. Fork it :fork_and_knife:\n\n\n### 2. Clone it :busts_in_silhouette:\n\n`NOTE: commands are to be executed on Linux, Mac, and Windows`\n\nYou need to clone (download) it to local machine using\n\n```sh\n$ git clone https://github.com/Your_Username/Breast-Cancer-Predictor.git\n```\n\n> This makes a local copy of the repository in your machine.\n\nOnce you have cloned the `Breast-Cancer-Predictor' repository in Github, move to that folder first using change directory command on Linux, Mac, and Windows\n```sh\n# This will change directory to a folder Hacktoberfest_20\n$ cd Breast-Cancer-Predictor\n```\n\nMove to this folder for all other commands.\n\n### 3. Set it up :arrow_up:\n\nRun the following commands to see that *your local copy* has a reference to *your forked remote repository* in Github :octocat:\n\n```sh\n$ git remote -v\norigin  https://github.com/Your_Username/Breast-Cancer-Predictor.git (fetch)\norigin  https://github.com/Your_Username/Breast-Cancer-Predictor.git (push)\n```\n\nNow, let's add a reference to the original [Breast-Cancer-Predictor](https://github.com/srimani-programmer/Breast-Cancer-Predictor/) repository using\n\n```sh\n$ git remote add upstream https://github.com/srimani-programmer/Breast-Cancer-Predictor.git\n```\n\n> This adds a new remote named ***upstream***.\n\nSee the changes using\n\n```sh\n$ git remote -v\norigin    https://github.com/Your_Username/Breast-Cancer-Predictor.git (fetch)\norigin    https://github.com/Your_Username/Breast-Cancer-Predictor.git (push)\nupstream  https://github.com/Remote_Username/Breast-Cancer-Predictor.git (fetch)\nupstream  https://github.com/Remote_Username/Breast-Cancer-Predictor.git (push)\n```\n`In your case, you will see`\n```sh\n$ git remote -V\norigin    https://github.com/Your_Username/Breast-Cancer-Predictor.git (fetch)\norigin    https://github.com/Your_Username/Breast-Cancer-Predictor.git (push)\nupstream  https://github.com/manan-bedi2908/Breast-Cancer-Predictor.git (fetch)\nupstream  https://github.com/manan-bedi2908/Breast-Cancer-Predictor.git (push)\n```\n\n### 4. Sync it :recycle:\n\nAlways keep your local copy of the repository updated with the original repository.\nBefore making any changes and/or in an appropriate interval, run the following commands *carefully* to update your local repository.\n\n```sh\n# Fetch all remote repositories and delete any deleted remote branches\n$ git fetch --all --prune\n\n# Switch to `New_Pipeline` branch\n$ git checkout New_Pipeline\n\n# Reset local `main` branch to match the `upstream` repository's `main` branch\n$ git reset --hard upstream/main\n\n# Push changes to your forked `Breast-Cancer-Predictor` repo\n$ git push -u origin New_Pipeline\n```\n\n### 5. Ready Steady Go... :turtle: :rabbit2:\n\nOnce you have completed these steps, you are ready to start contributing to the project and creating [pull requests](https://github.com/srimani-programmer/Breast-Cancer-Predictor/pulls).\n\n### 6. Checkout to a new branch :bangbang:\n\nWhenever you are going to contribute. Please create a separate branch using command and keep your `main` branch clean (i.e. synced with remote branch).\n\n```sh\n# It will create a new branch with name Branch_Name and switch to branch Folder_Name\n$ git checkout -b New_Pipeline\n```\n\nCreate a separate branch for contribution and try to use the same name of the branch as of folder.\n\nTo switch to the desired branch\n\n```sh\n# To switch from one folder to other\n$ git checkout New_Pipeline\n```\n\nTo add the changes to the branch. Use\n\n```sh\n# To add all files to branch Folder_Name\n$ git add .\n```\n\nType in a message relevant for the code reviewer using\n\n```sh\n# This message get associated with all files you have changed\n$ git commit -m 'relevant message'\n```\n\nNow, Push your awesome work to your remote repository using\n\n```sh\n# To push your work to your remote repository\n$ git push -u origin New_Pipeline\n```\n\n\n(Kindly push all the changes to the \"New_Pipeline\", not main branch)\nFinally, go to your repository in the browser and click on `compare and pull requests`.\nThen add a title and description to your pull request that explains your precious effort.\n\n\n\n\n\n\n*****\n\n<div align=\"center\">\n<h4>Application UI</h4>\n</div>\n\n<div align=\"center\">\n<p>Home Page</p>\n</div>\n\n![Home Page 1](static/images/homepage1.png)\n\n***** \n\n<div align=\"center\">\n<p>Tumor Data form</p>\n</div>\n\n![Home Page 2](static/images/HomePage2.png)\n\n*****\n<div align=\"center\">\n<p>Tumor Data form</p>\n</div>\n\n![Home Page 3](static/images/homepage3.png)\n\n***\n<div align=\"center\">\n<p>Prediction Output</p>\n</div>\n\n![Prediction Page](static/images/predict.png)\n\n\n\n## Awesome contributors :star_struck:\n<a href=\"https://github.com/srimani-programmer/Breast-Cancer-Predictor/graphs/contributors\">\n  <img src=\"https://contributors-img.web.app/image?repo=srimani-programmer/Breast-Cancer-Predictor\" />\n</a>\n\nMade with [contributors-img](https://contributors-img.web.app).\n"
    },
    {
        "repo": "/ResearchKit/ShareTheJourney",
        "language": "Objective-C",
        "readme_contents": "Share the Journey\n=================\n\nShare the Journey is one of the first five apps built using [ResearchKit](https://github.com/ResearchKit/ResearchKit).\n\nSage Bionetworks' goal in this study is to understand the causes of the symptom variations after\nbreast cancer treatment; to learn how mobile devices and sensors can\nhelp us to these symptoms and their progression; and to ultimately\nimprove the quality of life for people after breast cancer treatment.\n\nThe Share the Journey app asks the participant to answer questions\nabout herself, medical history, and current health. The app also\ncollects information while the participant perform specific tasks\nwhile using a mobile phone, such as to provide a journal about her\nsymptoms. Additionally, the app asks permission to collect sensor\ndata from the phone itself.\n\n\nBuilding the App\n================\n\n###Requirements\n\n* Xcode 6.3\n* iOS 8.3 SDK\n\n###Getting the source\n\nFirst, check out the source, including all the dependencies:\n\n```\ngit clone --recurse-submodules https://github.com/ResearchKit/ShareTheJourney.git\n```\n\n###Building it\n\nOpen the project, `BreastCancer.xcodeproj`, and build and run.\n\n\nOther components\n================\n\nSeveral survey instruments used in the shipping app have been\nremoved from the open source version because they are not free\nto use:\n\n* [PAR Q+](http://eparmedx.com) (Exercise Readiness Survey)\n* [PSQI](http://www.sleep.pitt.edu/content.asp?id=1484&subid=2316) (Sleep Quality Survey)\n* [PAOFI](https://www.nntc.org/content/np-battery) (Assessment of Functioning Survey)\n\nThe shipping app also uses OpenSSL to add extra data protection, which\nhas not been included in the published version of the AppCore\nproject. See the [AppCore repository](https://github.com/researchkit/AppCore) for more details.\n\nData upload to [Bridge](http://sagebase.org/bridge/) has been disabled, the logos of the institutions have been removed, and the consent material has been marked as an example.\n\nLicense\n=======\n\nThe source in the ShareTheJourney repository is made available under the\nfollowing license unless another license is explicitly identified:\n\n```\nCopyright (c) 2015, Sage Bionetworks, Inc.\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without modification, \nare permitted provided that the following conditions are met:\n\n1. Redistributions of source code must retain the above copyright notice, this \nlist of conditions and the following disclaimer.\n\n2. Redistributions in binary form must reproduce the above copyright notice,\nthis list of conditions and the following disclaimer in the documentation and/or\nother materials provided with the distribution.\n\n3. Neither the name of the copyright holder nor the names of its contributors \nmay be used to endorse or promote products derived from this software without \nspecific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\nARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n```\n\n"
    },
    {
        "repo": "/Swarbricklab-code/BrCa_cell_atlas",
        "language": "R",
        "readme_contents": "# BrCa_cell_atlas\nThis repository contains code related to data processing and downstream analysis associated with the study \"[A single-cell and spatially resolved atlas of human breast cancers](https://www.nature.com/articles/s41588-021-00911-1)\" at Nature Genetics. \n\n# Data Availability\n## Processed & filtered scRNA-Seq data and CITE data\nAll processed scRNA-seq data and is available for in-browser exploration and download through the Broad Single-Cell portal at https://singlecell.broadinstitute.org/single_cell/study/SCP1039. \nCITE data can also be downloaded from this portal, although it cannot be explored interactively at this point.\nProcessed data is also available on GEO (GSE176078).\n\n## Raw scRNA-Seq data\nRaw scRNA-Seq data from this study has been deposited in the European Genome-Phenome Archive (EGA), which is hosted by the EBI and the CRG, under the accession code EGAS00001005173. \n\n## Spatial transcriptomics data\nAll ST data from this study is available from the Zenodo data repository (DOI: 10.5281/zenodo.4739739 - https://zenodo.org/record/4739739). ST data from the Andersson et al. study can be downloaded from the Zenodo data repository (DOI: 10.5281/zenodo.3957257 - https://zenodo.org/record/4751624#.Yw06i3ZByMI). For deconvolution of Visium data, please see the stereoscope method at https://github.com/almaan/stereoscope from the Andersson et al (2020) study. https://www.nature.com/articles/s42003-020-01247-y\nSee [this issue thread](https://github.com/Swarbricklab-code/BrCa_cell_atlas/issues/3) for a discussion on how to load the spatial data from Zenodo into Seurat.\n\n# Contacts\nAll other relevant data and analysis are available from the authors upon request. For further enquires, please either raise an issue via GitHub or email John Reeves (Data Manager - j.reeves(at)garvan.org.au), Sunny Wu (Lead Author - s.wu(at)garvan.org.au) or Alexander Swarbrick (Lab Head - a.swarbrick(at)garvan.org.au).\n"
    },
    {
        "repo": "/ecobost/cnn4brca",
        "language": "Python",
        "readme_contents": "# cnn4brca\nUsing Convolutional Neural Networks (CNN) for Semantic Segmentation of Breast Cancer Lesions (BRCA). Master's thesis documents. Bibliography, experiments and reports.\n\nMost articles in the Bibliography folder were obtained directly from the authors or via agreements with my home institution. Please consider any copyright infringement before using them.\n\n##### Contact info:\nErick Cobos Tandazo<br>\na01184587@itesm.mx\n\n## Usage\n### Data set\n1. You can obtain the BCDR database [online](http://bcdr.inegi.up.pt/) ([Moura et al.](http://dx.doi.org/10.1007/s11548-013-0838-2)). I used the BCDR-DO1 data set, this one has around 70 patients(~300 digital mammograms) with breast masses and their lesion outlines. [fileOrganization](database_info/file_Organization) has some info on how is this images ordered.\n\n2. To obtain the masks (from the outlines provided in the database) you can use [createMasks.m](database_info/createMask/createMask.m). This reads the mammogram info from a couple of files provided in the database: [sample bcdr_d01_img.csv](database_info/createMask/bcdr_d01_img.csv) and [sample bcdr_d01_outlines.csv](database_info/createMask/bcdr_d01_outlines.csv)\n\n   Output should look like this:  \n   <img src=\"database_info/createMask/img_20_30_1_RCC.png\" width=\"250\"/> <img src=\"database_info/createMask/img_20_30_1_RCC_mask.png\" width=\"250\"/>\n\n3. Use [prepareDB](code/prepareDB.py) to enhance the contrast of the mammograms and downsample them to have a manageable size (2cmx2cm in the mammogram in 128x128).\n\n   Output looks like this:  \n   <img src=\"docs/report/plots/mammogram_resized.png\" width=\"250\" align='center'>\n\n4. Finally you would need to divide the dataset into training, validation and test patients. You would need to produce a .csv with image and label filenames as [this](code/example.csv) for each set.\n\n### Training\n1. You would need to [install Tensorflow](https://www.tensorflow.org/install/)\n2. Run [train](code/train.py) or [train_with_val_split](code/train_with_val_split.py) to train networks. These train the network defined in [model_v3](code/model_v3.py), a fully convolutional network with 10 layers (900K parameters) that uses dillated convolution and is modelled in a ResNet network. Training is done image by image (no batch, but cost is computed in every pixel of the thousand of pixels) and uses dropout among other things\n    Note: Code was written for tensorflow 1.11.0 so it would need to be modified to make work in tf1.0\n\n### Evaluation\n1. You can use [compute_metrics](code/compute_metrics.py) or [compute_FROC](code/compute_FROC.py) to compute evaluation metrics or the FROC curve.\n\nYou are invited to check the code for more details, I tried to document it nicely.\n"
    },
    {
        "repo": "/Adamouization/Breast-Cancer-Detection-Mammogram-Deep-Learning",
        "language": "TeX",
        "readme_contents": "# Breast Cancer Detection in Mammograms Using Deep Learning Techniques [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3985051.svg)](https://doi.org/10.5281/zenodo.3985051?style=for-the-badge) [![GitHub license](https://img.shields.io/github/license/Adamouization/Breast-Cancer-Detection-Mammogram-Deep-Learning?style=for-the-badge)](https://github.com/Adamouization/Breast-Cancer-Detection-Mammogram-Deep-Learning/blob/master/LICENSE) ![Python](https://img.shields.io/badge/python-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54) ![Jupyter Notebook](https://img.shields.io/badge/jupyter-%23FA0F00.svg?style=for-the-badge&logo=jupyter&logoColor=white)\n\n**Table of Contents**\n\n- [Publication Updates](#publication-updates)\n  - [2020](#2020)\n  - [2021](#2021)\n  - [2022](#2022)\n  - [2023](#2023)\n- [What can I find in this repository?](#what-can-i-find-in-this-repository)\n- [Abstract](#abstract)\n- [Usage on a GPU lab machine](#usage-on-a-gpu-lab-machine)\n- [Dataset installation](#dataset-installation)\n    - [mini-MIAS dataset](#mini-mias-dataset)\n    - [DDSM and CBIS-DDSM datasets](#ddsm-and-cbis-ddsm-datasets)\n- [Citation](#citation)\n  - [Published article citation](#published-article-citation)\n  - [Code citation](#code-citation)\n- [License](#license)\n- [Code Authors](#code-authors)\n- [Star History](#star-history)\n- [Contact](#contact)\n\n\n## Publication Updates\n\nI have been working since the end of my Master's in 2020 to publish this dissertation in journal. As of May 2023, this research is published in PLOS ONE, and can be read and cited here: https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0280841. Here are the latest updates of this project:\n\n### 2020\n\n* Completed my Master's degree and submitted my dissertation. Final results: 90%.\n* Began discussions with my supervisors to consider publishing the dissertation in a renown journal.\n\n### 2021\n\n* Elected a journal to aim to publish in: PLOS ONE.\n* Began transforming the project from a dissertation report to a paper (choice of language, trimming down, changing narrative, updating figures).\n* After multiple iterations and feedback from both my supervisors, a first draft was created.\n\n### 2022\n\n* Due to the time that has passed since the initial dissertation project was finished, help was enlisted by including my supervisor's PhD student. The goal was to:\n  * update the literature review with more recent papers,\n  * reproduce the results and update the code/instructions for installation and setup (code can be found in this updated repository: [Adamouization/Breast-Cancer-Detection-Mammogram-Deep-Learning-Publication](https://github.com/Adamouization/Breast-Cancer-Detection-Mammogram-Deep-Learning-Publication)),\n  * have an extra pair of eyes to polish the whole paper and keep the flow consistent.\n* The paper was submitted to PLOS ONE in July 2022.\n* Reviewers got back to us in September 2022 with amendments for the paper to be considered for publication. These amendments included revamping the narrative to highlight the main contribution of the paper, add additional sections underlining the processes and decision-making process, and general amendments for the paper's flow and style to remain consistent throughout.\n* The amended version was sent in October.\n\n### 2023\n\n* The paper, entitled \"*A divide and conquer approach to maximise deep learning mammography classification accuracies*\", was formally accepted for publication in January 2023.\n* In May 2023, the paper was finally published in PLOS ONE. It can be read and cited here: https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0280841\n\n\n## What can I find in this repository?\n\nYou can find the full dissertation project (code + report) for the MSc Artificial Intelligence at the University of St Andrews (2020).\n\nThe publication of this project can be found here:\n* Paper published in PLOS ONE: https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0280841\n* Peer-reviewed code: https://github.com/Adamouization/Breast-Cancer-Detection-Mammogram-Deep-Learning-Publication\n\nThe original dissertation report can be read here: [Breast Cancer Detection in Mammograms using Deep Learning Techniques, Adam Jaamour (2020)](https://github.com/Adamouization/Breast-Cancer-Detection-Mammogram-Deep-Learning/blob/master/Breast%20Cancer%20Detection%20in%20Mammograms%20using%20Deep%20Learning%20Techniques%20-%20Adam%20Jaamour%2C%202020.pdf)\n\nIf you have any questions, issues or message, please either:\n* Post a comment on PLOS ONE: https://journals.plos.org/plosone/article/comments?id=10.1371/journal.pone.0280841\n* Open a new issue in this repository: https://github.com/Adamouization/Breast-Cancer-Detection-Mammogram-Deep-Learning/issues/new\n\n## Abstract\n\nThe objective of this dissertation is to explore various deep learning techniques that can be used to implement a system which learns how to detect instances of breast cancer in mammograms. Nowadays, breast cancer claims 11,400 lives on average every year in the UK, making it one of the deadliest diseases. Mammography is the gold standard for detecting early signs of breast cancer, which can help cure the disease during its early stages. However, incorrect mammography diagnoses are common and may harm patients through unnecessary treatments and operations (or a lack of treatments). Therefore, systems that can learn to detect breast cancer on their own could help reduce the number of incorrect interpretations and missed cases.\n\nConvolution Neural Networks (CNNs) are used as part of a deep learning pipeline initially developed in a group and further extended individually. A bag-of-tricks approach is followed to analyse the effects on performance and efficiency using diverse deep learning techniques such as different architectures (VGG19, ResNet50, InceptionV3, DenseNet121, MobileNetV2), class weights, input sizes, amounts of transfer learning, and types of mammograms.\n\n![CNN Model](https://i.postimg.cc/wxWB8CTP/CNN-architecture.png)\n\nUltimately, 67.08\\% accuracy is achieved on the CBIS-DDSM dataset by transfer learning pre-trained ImagetNet weights to a MobileNetV2 architecture and pre-trained weights from a binary version of the mini-MIAS dataset to the fully connected layers of the model. Furthermore, using class weights to fight the problem of imbalanced datasets and splitting CBIS-DDSM samples between masses and calcifications also increases the overall accuracy. Other techniques tested such as data  augmentation and larger image sizes do not  yield increased accuracies, while the mini-MIAS dataset proves to be too small for any meaningful results using deep learning techniques. These results are compared with other papers using the CBIS-DDSM and mini-MIAS datasets, and with the baseline set during the implementation of a deep learning pipeline developed as a group.\n\n## Usage on a GPU lab machine\n\nClone the repository:\n\n```\ncd ~/Projects\ngit clone https://github.com/Adamouization/Breast-Cancer-Detection-Code\n```\n\nCreate a repository that will be used to install Tensorflow 2 with CUDA 10 for Python and activate the virtual environment for GPU usage:\n\n```\ncd libraries/tf2\ntar xvzf tensorflow2-cuda-10-1-e5bd53b3b5e6.tar.gz\nsh build.sh\n```\n\nActivate the virtual environment:\n\n```\nsource /cs/scratch/<username>/tf2/venv/bin/activate\n```\n\nCreate `output`and `save_models` directories to store the results:\n\n```\nmkdir output\nmkdir saved_models\n```\n\n`cd` into the `src` directory and run the code:\n\n```\nmain.py [-h] -d DATASET [-mt MAMMOGRAMTYPE] -m MODEL [-r RUNMODE] [-lr LEARNING_RATE] [-b BATCHSIZE] [-e1 MAX_EPOCH_FROZEN] [-e2 MAX_EPOCH_UNFROZEN] [-roi] [-v] [-n NAME]\n```\n\nwhere:\n* `-h` is a flag for help on how to run the code.\n* `DATASET` is the dataset to use. Must be either `mini-MIAS`, `mini-MIAS-binary` or `CBIS-DDMS`. Defaults to `CBIS-DDMS`.\n* `MAMMOGRAMTYPE` is the type of mammograms to use. Can be either `calc`, `mass` or `all`. Defaults to `all`.\n* `MODEL` is the model to use. Must be either `VGG-common`, `VGG`, `ResNet`, `Inception`, `DenseNet`, `MobileNet` or `CNN`.\n* `RUNMODE` is the mode to run in (`train` or `test`). Default value is `train`.\n* `LEARNING_RATE` is the optimiser's initial learning rate when training the model during the first training phase (frozen layers). Defaults to `0.001`. Must be a positive float.\n* `BATCHSIZE` is the batch size to use when training the model. Defaults to `2`. Must be a positive integer.\n* `MAX_EPOCH_FROZEN` is the maximum number of epochs in the first training phrase (with frozen layers). Defaults to `100`.\n* `MAX_EPOCH_UNFROZEN`is the maximum number of epochs in the second training phrase (with unfrozen layers). Defaults to `50`.\n* `-roi` is a flag to use versions of the images cropped around the ROI. Only usable with mini-MIAS dataset. Defaults to `False`.\n* `-v` is a flag controlling verbose mode, which prints additional statements for debugging purposes.\n* `NAME` is name of the experiment being tested (used for saving plots and model weights). Defaults to an empty string.\n\n## Dataset installation\n\n#### mini-MIAS dataset\n\n* This example will use the [mini-MIAS](http://peipa.essex.ac.uk/info/mias.html) dataset. After cloning the project, travel to the `data/mini-MIAS` directory (there should be 3 files in it).\n\n* Create `images_original` and `images_processed` directories in this directory: \n\n```\ncd data/mini-MIAS/\nmkdir images_original\nmkdir images_processed\n```\n\n* Move to the `images_original` directory and download the raw un-processed images:\n\n```\ncd images_original\nwget http://peipa.essex.ac.uk/pix/mias/all-mias.tar.gz\n```\n\n* Unzip the dataset then delete all non-image files:\n\n```\ntar xvzf all-mias.tar.gz\nrm -rf *.txt \nrm -rf README \n```\n\n* Move back up one level and move to the `images_processed` directory. Create 3 new directories there (`benign_cases`, `malignant_cases` and `normal_cases`):\n\n```\ncd ../images_processed\nmkdir benign_cases\nmkdir malignant_cases\nmkdir normal_cases\n```\n\n* Now run the python script for processing the dataset and render it usable with Tensorflow and Keras:\n\n```\npython3 ../../../src/dataset_processing_scripts/mini-MIAS-initial-pre-processing.py\n```\n\n#### DDSM and CBIS-DDSM datasets\n\nThese datasets are very large (exceeding 160GB) and more complex than the mini-MIAS dataset to use. They were downloaded by the University of St Andrews School of Computer Science computing officers onto \\textit{BigTMP}, a 15TB filesystem that is mounted on the Centos 7 computer lab clients with NVIDIA GPUsusually used for storing large working data sets. Therefore, the download process of these datasets will not be covered in these instructions.\\\\\n\nThe generated CSV files to use these datasets can be found in the `/data/CBIS-DDSM` directory, but the mammograms will have to be downloaded separately. The DDSM dataset can be downloaded [here](http://www.eng.usf.edu/cvprg/Mammography/Database.html), while the CBIS-DDSM dataset can be downloaded [here](https://wiki.cancerimagingarchive.net/display/Public/CBIS-DDSM#5e40bd1f79d64f04b40cac57ceca9272).\n\n## Citation\n\n### Published article citation\n\n```\n@article{10.1371/journal.pone.0280841,\n    doi = {10.1371/journal.pone.0280841},\n    author = {Jaamour, Adam AND Myles, Craig AND Patel, Ashay AND Chen, Shuen-Jen AND McMillan, Lewis AND Harris-Birtill, David},\n    journal = {PLOS ONE},\n    publisher = {Public Library of Science},\n    title = {A divide and conquer approach to maximise deep learning mammography classification accuracies},\n    year = {2023},\n    month = {05},\n    volume = {18},\n    url = {https://doi.org/10.1371/journal.pone.0280841},\n    pages = {1-24},\n    number = {5},\n}\n```\n\n### Code citation\n```\n@software{adam_jaamour_2020_3985051,\n  author       = {Adam Jaamour and\n                  Ashay Patel and\n                  Shuen-Jen Chen},\n  title        = {{Breast Cancer Detection in Mammograms using Deep \n                   Learning Techniques: Source Code}},\n  month        = aug,\n  year         = 2020,\n  publisher    = {Zenodo},\n  version      = {v1.0},\n  doi          = {10.5281/zenodo.3985051},\n  url          = {https://doi.org/10.5281/zenodo.3985051}\n}\n```\n\n## License \n* see [LICENSE](https://github.com/Adamouization/Breast-Cancer-Detection-and-Segmentation/blob/master/LICENSE) file.\n\n## Code Authors\n\n* Adam Jaamour\n* Ashay Patel\n* Shuen-Jen Chen\n\nThe common pipeline can be found at [DOI 10.5281/zenodo.3975092](https://zenodo.org/record/3975093)\n\n## Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=Adamouization/Breast-Cancer-Detection-Mammogram-Deep-Learning,Adamouization/Breast-Cancer-Detection-Mammogram-Deep-Learning-Publication&type=Date)](https://star-history.com/#Adamouization/Breast-Cancer-Detection-Mammogram-Deep-Learning&Adamouization/Breast-Cancer-Detection-Mammogram-Deep-Learning-Publication&Date)\n\n## Contact\n* Email: adam[at]jaamour[dot]com\n* Website: www.adam.jaamour.com\n* LinkedIn: [linkedin.com/in/adamjaamour](https://www.linkedin.com/in/adamjaamour/)\n"
    },
    {
        "repo": "/yala/Mirai",
        "language": "Python",
        "readme_contents": "# Mirai: Mammography-based model for breast cancer risk [![DOI](https://zenodo.org/badge/315745008.svg)](https://zenodo.org/badge/latestdoi/315745008)\n\n# Introduction\nThis repository was used to develop Mirai, the risk model described in: [Towards Robust Mammography-Based Models for Breast Cancer Risk](https://www.science.org/doi/10.1126/scitranslmed.aba4373). Mirai was designed to predict risk at multiple time points, leverage potentially missing risk-factor information, and produce predictions that are consistent across mammography machines. Mirai was trained on a large dataset from Massachusetts General Hospital (MGH) in the US and was tested on held-out test sets from MGH, Karolinska in Sweden and Chang Gung Memorial Hospital in Taiwan, obtaining C-indices of 0.76 (0.74, 0.80), 0.81 (0.79, 0.82), 0.79 (0.79, 0.83), respectively. Mirai obtained significantly higher five-year ROC AUCs than the Tyrer-Cuzick model (p<0.001) and prior deep learning models, Hybrid DL (p<0.001) and ImageOnly DL (p<0.001), trained on the same MGH dataset. In our paper, we also demonstrate that Mirai was more significantly accurate in identifying high risk patients than prior methods across all datasets. On the MGH test set, 41.5% (34.4, 48.5) of patients who would develop cancer within five-years were identified as high risk, compared to 36.1% (29.1, 42.9) by Hybrid DL (p=0.02) and 22.9% (15.9, 29.6) by Tyrer-Cuzick lifetime risk (p<0.001).\n\nThis code base is meant to achieve a few goals:\n- Provide exact implementation details for the development of Mirai to facilitate review of our paper \n- Enable researchers to validate or further refine Mirai on large datasets \n\nWe note that this code-base is an extension of [OncoNet](https://github.com/yala/OncoNet_Public), which we used to develop Hybrid DL and ImageOnly DL.\n\nThis repository is intended for researchers assessing the manuscript and researching model development and validation.  The code base is not intended to be deployed for generating predictions for use in clinical-decision making or for any other clinical use.  You bear sole responsibility for your use of Mirai.\n\n## Aside on Software Depedencies\nThis code assumes python3.6 and a Linux environment.\nThe package requirements can be install with pip:\n\n`pip install -r requirements.txt`\n\nIf you are familiar with docker, you can also directly leverage the OncoServe [Mirai docker container](https://www.dropbox.com/s/k0wq2z7xqr95y3b/oncoserve_mirai.0.5.0.tar?dl=0) which has all the depedencies preinstalled and the trained Mirai model (see below).\n\n## Preprocessing\nOur code-base operates on PNG images. We converted presentation view dicoms to PNG16 files using the DCMTK library. We used the dcmj2pnm program (v3.6.1, 2015) with +on2 and\u2013min-max-window flags. To this, you can use DCMTK directly or [OncoData](https://github.com/yala/OncoData_Public), our python wrapper for converting dicoms.\n\n## Inclusion Criteria\nMirai expects all four standard \u201cFor Presentation\u201d views of the mammogram.  Specifically, it requires an \u201cL CC\u201d, \u201cL MLO\u201d, \u201cR CC\u201d, \u201cR MLO\u201d. It will not work without all four views or with \u201cFor Processing\u201d mammograms.  As a result, we unfortunately cannot provide risk assessments for patients with only non-standard views or unilateral mammograms. Moreover, we cannot run the model on marked up images (i.e images with some CAD or human annotations).  All of the mammograms used in our study were captured using either the Hologic Selenia or Selenia Dimensions mammography devices. We have yet tested Mirai on other machines.\n\n# Reproducing Mirai\nAs described in the supplementary material of the paper, Mirai was trained in two phases; first, we trained the image encoder in conjunction with the risk factor predictor and additive hazard layer to predict breast cancer independently from each view without using conditional adversarial training. In this stage, we intialialized our image encoder with weights from ImageNet, and augmented our training set with random flips and rotations of the original images. We found that adding an adversarial loss at this stage or training the whole architecture end-to-end prevented the model from converging. In the second stage of training, we froze our image encoder, and trained the image aggregation module, the risk factor prediction module, the additive hazard layer, and the device discriminator in a conditional adversarial training regime. We trained our adversary for three steps for every one step of training Mirai. In each stage, we performed small hyperparameter searches and chose the model that obtained the highest C-index on the development set.\n\nThe grid searches are shown in :\n\n`configs/mirai_base.json` and `configs/mirai_full.json`\n\nThe grid searches were run using our job-dispatcher, as shown bellow.\n\n`python scripts/dispatcher.py --experiment_config_path configs/mirai_base.json --result_path mirai_base_sweep.csv`\n\nWe selected the image encoder with the highest C-index on the development set, and leveraged it for the second stage hyper-parameter sweep.\n\n`python scripts/dispatcher.py --experiment_config_path configs/mirai_full.json --result_path mirai_full_sweep.csv`\n\nWe note that this command run relies on integrations that were specific to the MGH data, and so the exact line above will not run on your system. The configs above are meant to specify exact implementation details and our experimental procedure.\n\n# Using Mirai\nMirai (the trained model) and all code are released under the MIT license. \n\n## Installing Mirai\nPlease see [OncoServe](https://github.com/yala/OncoServe_Public), our framework for prospectively testing mammography-based models in the clinic. OncoServe can be easily installed on premise using Docker, and it provides a simple HTTP interface to get risk assessments for a given patient's dicom files. OncoServe encapsulates all the dependencies and necessary preprocessing.\n\n## Using Mirai Codebase (Validation / Refinement)\nTo use the Mirai code-base research purposes, we recommend using our [OncoServe](https://github.com/yala/OncoServe_Public) docker image. Once you have the docker image, you may enter it as follows:\n\n```\ndocker run -it -v /PATH/TO/DATA_DIR:/data:z learn2cure/oncoserve_mirai:0.5.0 /bin/zsh\n```\nThis command will enter the docker container and make your data directory (with dicoms and outcomes) available to the container at the /data directory. Inside the docker container, you will find this repository in the `/root/OncoNet/` directory. For there, you can run the validation or fine tuning scripts. \n\n### Preprocessing DICOMS with OncoData\nThe `oncoserve_mirai` docker image already contains [OncoData](https://github.com/yala/OncoData_Public), our codebase for preprocessing dicoms. To convert a directory of dicoms into PNGs, follow the following steps:\n```\ncd /root/OncoData\npython scripts/dicom_to_png/dicom_to_png.py --dcmtk --dicom_dir /PATH/TO/DICOMS --png_dir /PATH/TO/PNG_DIR \n```\nNote, OncoData assumes that each dicom file has a `.dcm` suffix. This repo is tested to work well with Hologic dicoms, but may not properly convert dicoms from other manufacturers. \n\n### How validate the model on a large dataset\nTo validate Mirai, you can use the following command: `sh demo/validate.sh`\nThe full bash command (inside the validate.sh file) is:\n```\npython scripts/main.py  --model_name mirai_full --img_encoder_snapshot snapshots/mgh_mammo_MIRAI_Base_May20_2019.p --transformer_snapshot snapshots/mgh_mammo_cancer_MIRAI_Transformer_Jan13_2020.p  --callibrator_snapshot snapshots/callibrators/MIRAI_FULL_PRED_RF.callibrator.p --batch_size 1 --dataset csv_mammo_risk_all_full_future --img_mean 7047.99 --img_size 1664 2048 --img_std 12005.5 --metadata_path demo/sample_metadata.csv --test --prediction_save_path demo/validation_output.csv\n```\n\nAlternatively, you could launch the same validation script using our job-dispatcher with the following command:\n```\npython scripts/dispatcher.py --experiment_config_path configs/validate_mirai.json --result_path finetune_results.csv\n```\n\nWhat you need to validate the model:\n- Install the dependencies (see above)\n- Get access to the snapshot files (in snapshots folder of docker container)\n- Convert your dicoms to PNGs (see above)\n- Create a CSV file describing your dataset. For an example, see `demo/sample_metadata.csv`. We note that all the columns are required.\n    - `patient_id`: ID string for this patient. Is used to link together mammograms for one patient.\n    - `exam_id`: ID string for this mammogram. Is used to link together several files for one mammogram. Note, this code-base assumes that \"patient_id + exam_id\" is the unique key for a mammogram.\n    - `laterality`: Laterality of the mammogram. Can only take values 'R' or 'L' for right and left.\n    - `view`: View of the dicom. Can only take values 'MLO' or 'CC'. Other views are not supported.\n    - `file_path`: Absolute path to the PNG16 image for this view of the mammogram\n    - `years_to_cancer`: Integer the number of years from this mammogram that the patient was diagnosed with breast cancer. If the patient doesn't develop cancer during the observed data, enter 100. If the cancer was found on this mammogram, enter 0.\n    - `years_to_last_followup`: Integer reflecting how many years from the mammogram we know the patient is cancer free. For example, if a patient had a negative mammogram in 2010 (and this row corresponds to that mammogram), and we have negative followup until 2020, then enter 10.\n    - `split_group`: Can take values `train`, `dev` or `test` to note the training, validation and testing samples.\n\nBefore running `validate.sh`, make sure to replace `demo/sample_metadata.csv` with the path to your metadata path and to replace `demo/validation_output.csv` to wherever you want predictions will be saved.\n\nAfter running `validate.sh`, our code-base will print out the AUC for each time-point and save the predictions for each mammogram in `prediction_save_path`. For an example of the output file format, see `demo/validation_output.csv`. The key `patient_exam_id` is defined as `patient_id \\tab exam_id`.\n\n### How to refine the model\nTo finetune Mirai for research purposes, you can use the following commands: `sh demo/finetune.sh`\nThe full bash command (inside the validate.sh file) is:\n\n```\npython scripts/dispatcher.py --experiment_config_path configs/fine_tune_mirai.json --result_path finetune_results.csv\n```\n\nIt create a grid search over possible fine-tuning hyperparameters (see `configs/finetune_mirai.json`) and launches jobs across the available GPUs (as defined in `available_gpus`). The results will be summarized in `finetune_results.csv` or wherever you set `results_path`. We note that each job launches just just a shell command. By editing `configs/finetune_mirai.json` or creating your own config json file, you can explore any hyper-parameters or architecture supported in the code base.\n\nWhat finetune the model, you will need the same dependencies, preprocessing and CSV file as listed above to validate Mirai. We recommend you first evaluate Mirai before you try to finetune it.\n\n"
    },
    {
        "repo": "/akshaybahadur21/BreastCancer_Classification",
        "language": "Python",
        "readme_contents": "# Breast Cancer Classifier (Logistic Regression) \ud83d\udd2c\n[![](https://img.shields.io/github/license/sourcerer-io/hall-of-fame.svg?colorB=ff0000)](https://github.com/akshaybahadur21/BreastCancer_Classification/blob/master/LICENSE.txt)  [![](https://img.shields.io/badge/Akshay-Bahadur-brightgreen.svg?colorB=ff0000)](https://akshaybahadur.com)\n\nThis code helps you classify malignant and benign tumors using Logistic Regression\n\n## Code Requirements \ud83e\udd84\n\nThe example code is in Matlab ([R2016](https://in.mathworks.com/help/matlab/) or higher will work). \n\nYou can install Conda for python which resolves all the dependencies for machine learning.\n\n## Description \ud83e\uddea\nLogistic regression is named for the function used at the core of the method, the logistic function.\n\nThe logistic function, also called the sigmoid function was developed by statisticians to describe properties of population growth in ecology, rising quickly and maxing out at the carrying capacity of the environment. It\u2019s an S-shaped curve that can take any real-valued number and map it into a value between 0 and 1, but never exactly at those limits.\n\n1 / (1 + e^-value)\n\n<img src=\"https://github.com/akshaybahadur21/BreastCancer_Classification/blob/master/Logistic-Function.png\">\n\nFor more information, [see](https://en.wikipedia.org/wiki/Logistic_regression)\n\n## Some Notes \ud83d\uddd2\ufe0f\n1) Dataset- UCI-ML\n2) I have used only 2 features out of 32 to classify.\n\n## Results \ud83d\udcca\n<img src=\"https://github.com/akshaybahadur21/BreastCancer_Classification/blob/master/breast_cancer.gif\">\n\n## Execution \ud83d\udc09\nTo run the code, type `run breast_cancer.m`\n\n```\nrun breast_cancer.m\n```\n\n## Python  Implementation \ud83d\udc68\u200d\ud83d\udd2c\n\n1) Dataset- UCI-ML\n2) I have used 30 features to classify\n3) Instead of 0=benign and 1=malignant, I have used 1=benign and 2=malignant\n\n## Results \ud83d\udcca\n<img src=\"https://github.com/akshaybahadur21/BreastCancer_Classification/blob/master/b_cancer_python.gif\">\n\n## Execution \ud83d\udc09\nTo run the code, type `python B_Cancer.py`\n\n```\npython B_Cancer.py\n```\n\n\n\n"
    },
    {
        "repo": "/Dipeshtamboli/Image-Classification-and-Localization-using-Multiple-Instance-Learning",
        "language": "Python",
        "readme_contents": "# Introduction\nDeep learning in histopathology has developed an interest over the decade due to its improvements in classification and localization tasks. Breast cancer is a prominent cause of death in women. \n\n\nComputer-Aided Pathology is essential to analyze microscopic histopathology images for diagnosis with an\nincreasing number of breast cancer patients. The convolutional neural network, a deep learning algorithm, provides significant results in classification among cancer and non-cancer tissue images but lacks in providing interpretation. Here in this blog, I am writing the streamlined version of the paper **\"Breast cancer histopathology image classification and localization using multiple instance learning\"** published in **WIECON-2019** in which we have aimed to provide a better interpretation of classification results by providing localization on microscopic histopathology images. We frame the image classification problem as weakly supervised multiple instance learning problems and use attention on instances to localize the tumour and normal regions in an image. **Attention-based multiple instance learning (A-MIL)** is applied on **BreakHis** and **BACH** datasets. The classification and visualization results are compared with other recent techniques. A method used in this paper produces better localization results without compromising classification accuracy.\n\n# About Grad-CAM Image Visualizations\n\nIn the era of deep learning, understanding of the model's decision is important and the GradCAM is one of the first and good methods to visualize the outcome. Here is the paper and the following are the results taken from paper directly.\n\n<!--       <center> ![hey](/images/amil/grad_cam.png) </center>\n      <center>This is an image</center>\n -->\n   \t\n![Highlighting the part in the input image responsible for classification in that category. Image is taken from the paper directly.](https://dipeshtamboli.github.io/images/amil/grad_cam.png)\n\nHere, in the image, you can see the highlighted portion corresponding to the parts of the image which is responsible for the classification. Like in the first image of Torch, GradCAM is highlighting the portion in the image where the torch is present. Similarly, in the Car Mirror image, it's highlighting the portion where the car mirror is present. Thus this explains the reason behind the decision taken by the model.\n\n![This is the image where the model's output is the cat and GradCAM is highlighting the portion responsible for that decision. Image is taken from the paper directly.](https://dipeshtamboli.github.io/images/amil/dog.png)\n\nThis is the image where the model's output is the cat and GradCAM is highlighting the portion responsible for that decision. Image is taken from the paper directly.Now, these are the results where we see that the GradCAM and Guided GradCAM gives us the portion which is important for decision making. But it doesn't work well on the medical images(especially Histopathology images).\n\n\n\n![GradCAM and Guided-GradCAM is not highlighting the useful portion of the\u00a0image.](https://dipeshtamboli.github.io/images/amil/grad_amil.png)\n\nSo we have proposed another visualization technique for it. It's an attention-based visualization method where we are doing multiple instance learning.\n\n# Attention-based multiple instance learning\u00a0(A-MIL)\n\nIn this method, we have cropped an image in small square patches and made a bag of it. This bag of images will act like a batch. We are fee\nAMIL ArchitectureFirst making a bag of the input image by taking the small patches from it.\nPassing it to the feature extractor which is basically a convolutional neural network block.\nThen we are passing the Instance level features to the classifier for getting Instance level attention.\nHere we are getting the attention weights which we are further using for attention aggregation to get the bag level features.\n\n![The figure shows localization in sample malignant image using Attention - Multiple Instance Learning. A-MIL accurately highlights affected gland and ignores background region](https://dipeshtamboli.github.io/images/amil/amil_arc.png)\n\n\nThen we are applying Dense layer for the classification of the Benign, Malignant or Invasiveconsidering the\n\nSo in the end, we have cropped image patches and their attention weights. We multiplied each attention weight with the corresponding patch and stitch the whole image to get the visualization of the complete input image. With this method, we are neither compromising the accuracy nor made the model complicated. This is just adding transparency to the whole process.\n\n![The figure shows localization in sample malignant image using Attention - Multiple Instance Learning. A-MIL accurately highlights affected gland and ignores background region](https://dipeshtamboli.github.io/images/amil/amil.png)\n\n\nThis is the comparison between the visualization of GradCAM and AMIL method. Here we cropped two portions from the image which is important for the classification and applied GradCAM on it. In another scene, AMIL visualization is there which is properly highlighting the useful portion.\nComparison of the visualization output of GradCAM and\u00a0A-MILAnother result of AMIL visualization of BACH image.\n\n\n![Another result from the BACH\u00a0dataset.](https://dipeshtamboli.github.io/images/amil/result.png)\n\n***********************\n\n[Same article is also available on Medium](https://medium.com/@dipeshtamboli/a-sota-method-for-visualization-of-histopathology-images-1cc6cc3b76f3)   \nAnd also on [my blog](https://dipeshtamboli.github.io/blog/2019/Visualization-of-Histopathology-images/)\n\n<button style=\"background-color:azure;color:white;width:200px;\nheight:40px;\">[Arxiv link](https://arxiv.org/abs/2003.00823)</button> | <button style=\"background-color:azure;color:white;width:240px;\nheight:40px;\">[IEEE Xplore link](https://ieeexplore.ieee.org/abstract/document/9019916)</button> | <button style=\"background-color:azure;color:white;width:300px;\nheight:40px;\">[Harvard digital library link](https://ui.adsabs.harvard.edu/abs/2020arXiv200300823P/abstract)</button>\n\n***********************\n\n\n# How to run the code\n\nHere, in the folder AMIL_project, we have following folders:\n\n## my_network\n\t-In this, we have trained a model using this dataset on our own architecture. Accuracies are comparable to the VGG_pretrained and ResNet_pretrained model.\n\t-In this folder, we have\n\t\t-net.py\n\t\t\t-use \"python net.py\" to run the code(without quotes)\n\t\t\t-this is the code which will train the model, and test it on the validation set\n\t\t\t-this code will save following things in corresponding zoom level folder\n\t\t\t\t-model(in pickel and pytorch format)\n\t\t\t\t-terminal logs\n\t\t\t\t-tensorboard run logs\n\t\t\t\t-text file summarizing the run\n\t\t-run_for_all_zoom.sh\n\t\t\t- use \"bash run_for_all_zoom.sh\" to run this script(without quotes)\n\t\t\t- this script will run vgg_pre.py for all the zoom level and for all the epochs\n\t\t\t- u can keep the number of epoch = 10\t\n## ResNet\n\t-In this, we have used a pre-trained ResNet model and trained last fully connected layer using this dataset.\n\t-In this folder, we have\n\t\t-resnet_pre.py\n\t\t\t-use \"python resnet_pre.py\" to run the code(without quotes)\n\t\t\t-this is the code which will train the model, and test it on the validation set\n\t\t\t-this code will save following things in corresponding zoom level folder\n\t\t\t\t-model(in pickel and pytorch format)\n\t\t\t\t-terminal logs\n\t\t\t\t-tensorboard run logs\n\t\t\t\t-text file summarizing the run\n\t\t-run_for_all_zoom.sh\n\t\t\t- use \"bash run_for_all_zoom.sh\" to run this script(without quotes)\n\t\t\t- this script will run vgg_pre.py for all the zoom level and for all the epochs\n\t\t\t- u can keep the number of epoch = 10\t\n## VGG\n\t-In this, we have used a pre-trained VGG model and trained last fully connected layer using this dataset.\n\t-In this folder, we have\n\t\t-vgg_pre.py\n\t\t\t-use \"python vgg_pre.py\" to run the code(without quotes)\n\t\t\t-this is the code which will train the model, and test it on the validation set\n\t\t\t-this code will save following things in corresponding zoom level folder\n\t\t\t\t-model(in pickel and pytorch format)\n\t\t\t\t-terminal logs\n\t\t\t\t-tensorboard run logs\n\t\t\t\t-text file summarizing the run\n\t\t-run_for_all_zoom.sh\n\t\t\t- use \"bash run_for_all_zoom.sh\" to run this script(without quotes)\n\t\t\t- this script will run vgg_pre.py for all the zoom level and for all the epochs\n\t\t\t- u can keep the number of epoch = 10\t\n\n## AMIL_codes\n\t-In this folder, we have\n\t\t-amil_model.py\n\t\t\t-it contains attention model(architecture)\n\t\t-patch_data.py\n\t\t\t-data loader (takes images as input and crop it to 28*28 and creates a bag)\n\t\t-train_n_test.py\n\t\t\t-use \"python train_n_test.py\" to run the code(without quotes)\n\t\t\t-code which trains the AMIL and then test it on the validation set and saves visualization in the AMIL_visualization folder\n\t\t\t-this code will save following things in corresponding zoom level folder\n\t\t\t\t-model(pytorch format)\n\t\t\t\t-terminal logs\n\t\t\t\t-tensorboard run logs\n\t\t\t\t-text file summarizing the run\t\t\t\n\t\t\t\t-visualization of test images\n\t\t-run_for_all_zoom.sh\n\t\t\t- use \"bash run_for_all_zoom.sh\" to run this script(without quotes)\n\t\t\t- this script will run vgg_pre.py for all the zoom level and for all the epochs\n\t\t\t- u can keep the number of epoch = 20\t\n\n## grad_cam\n\t-In this folder, we have \"inputs\" folder where you have to put test image\n\t-In folder \"src\", in misc_functions.py, on line number 253, you have to put the name of test image\n\t-run the code \"python guided_gradcam.py\" without quotes.\n\t-this will produce resultant visualization images in \"results\" folder\n\n## Kaggle_Data:\nwe are using Breakhis dataset from the Kaggle datasets(link to the dataset)   \n-download the dataset from the following link:\n[Kaggle Dataset](https://www.kaggle.com/kritika397/breast-cancer-dataset-from-breakhis/downloads/fold1.zip/1)  \n-rename it to Kaggle_Data   \n-We will use this data for Resnet architecture, vgg architecture and mynet architecture   \n\n## AMIL_Data:\n\t-Here, for attention based multiple instance learning, we will re-arrange the dataset in the given format(readme_data_format.txt)\n\n###\tHere, dataset is in this structure:\n\t\tfold1\n\t\t\t-test\n\t\t\t\t-100X\n\t\t\t\t\t-B_100X\n\t\t\t\t\t\t-(images)\n\t\t\t\t\t-M_100X\n\t\t\t\t\t\t-(images)\n\t\t\t\t-200X\n\t\t\t\t\t-B_200X\n\t\t\t\t\t\t-(images)\n\t\t\t\t\t-M_200X\n\t\t\t\t\t\t-(images)\n\t\t\t\t-400X\n\t\t\t\t\t-B_400X\n\t\t\t\t\t\t-(images)\n\t\t\t\t\t-M_400X\n\t\t\t\t\t\t-(images)\n\t\t\t\t-40X\n\t\t\t\t\t-B_40X\n\t\t\t\t\t\t-(images)\n\t\t\t\t\t-M_40X\n\t\t\t\t\t\t-(images)\n\t\t\t-train\n\t\t\t\t-100X\n\t\t\t\t\t-B_100X\n\t\t\t\t\t\t-(images)\n\t\t\t\t\t-M_100X\n\t\t\t\t\t\t-(images)\n\t\t\t\t-200X\n\t\t\t\t\t-B_200X\n\t\t\t\t\t\t-(images)\n\t\t\t\t\t-M_200X\n\t\t\t\t\t\t-(images)\n\t\t\t\t-400X\n\t\t\t\t\t-B_400X\n\t\t\t\t\t\t-(images)\n\t\t\t\t\t-M_400X\n\t\t\t\t\t\t-(images)\n\t\t\t\t-40X\n\t\t\t\t\t-B_40X\n\t\t\t\t\t\t-(images)\n\t\t\t\t\t-M_40X\n\t\t\t\t\t\t-(images)\n\n###\tNow, we have to convert it in the following format:\n\t\tdata_breakhis\n\t\t\t-100X\n\t\t\t\t-train\n\t\t\t\t\t-0\n\t\t\t\t\t\t-images\n\t\t\t\t\t-1\n\t\t\t\t\t\t-images\n\t\t\t\t-test\n\t\t\t\t\t-0\n\t\t\t\t\t\t-images\n\t\t\t\t\t-1\n\t\t\t\t\t\t-images\t\t\t\t\n\t\t\t-200X\n\t\t\t\t-train\n\t\t\t\t\t-0\n\t\t\t\t\t\t-images\n\t\t\t\t\t-1\n\t\t\t\t\t\t-images\n\t\t\t\t-test\n\t\t\t\t\t-0\n\t\t\t\t\t\t-images\n\t\t\t\t\t-1\n\t\t\t\t\t\t-images\t\t\t\t\t\t\t\n\t\t\t-400X\n\t\t\t\t-train\n\t\t\t\t\t-0\n\t\t\t\t\t\t-images\n\t\t\t\t\t-1\n\t\t\t\t\t\t-images\n\t\t\t\t-test\n\t\t\t\t\t-0\n\t\t\t\t\t\t-images\n\t\t\t\t\t-1\n\t\t\t\t\t\t-images\t\t\t\t\t\t\t\n\t\t\t-40X\n\t\t\t\t-train\n\t\t\t\t\t-0\n\t\t\t\t\t\t-images\n\t\t\t\t\t-1\n\t\t\t\t\t\t-images\n\t\t\t\t-test\n\t\t\t\t\t-0\n\t\t\t\t\t\t-images\n\t\t\t\t\t-1\n\t\t\t\t\t\t-images\t\t\t\t\t\t\t\n\t-rearrange the folders and rename it to AMIL_Data\n\n\n************************************************\nTo cite this:   \nPlain Text:   \nA. Patil, D. Tamboli, S. Meena, D. Anand and A. Sethi, \"Breast Cancer Histopathology Image Classification and Localization using Multiple Instance Learning,\" 2019 IEEE International WIE Conference on Electrical and Computer Engineering (WIECON-ECE), Bangalore, India, 2019, pp. 1-4.\n\nBibTex:   \n@INPROCEEDINGS{9019916,  author={A. {Patil} and D. {Tamboli} and S. {Meena} and D. {Anand} and A. {Sethi}},  booktitle={2019 IEEE International WIE Conference on Electrical and Computer Engineering (WIECON-ECE)},  title={Breast Cancer Histopathology Image Classification and Localization using Multiple Instance Learning},   year={2019},  volume={},  number={},  pages={1-4},}\n"
    },
    {
        "repo": "/viritaromero/Breast-Cancer-Detection",
        "language": "Jupyter Notebook",
        "readme_contents": "# Breast-Cancer-Detection\n\n# Creating an AI app for Breast Cancer Detection\n![alt text](http://www.innovationandtech.ae/wp-content/uploads/2018/01/Cancer-Prognosis-Prediction-using-AI-810x324.jpg)\n\nBreast cancer has the second highest mortality rate in women next to lung cancer. As per clinical statistics, 1 in every 8 women is diagnosed with breast cancer in their lifetime. However, periodic clinical checkups and self-tests help in early detection and thereby significantly increase the chances of survival. Invasive detection techniques cause rupture of the tumor, accelerating the spread of cancer to adjoining areas. Hence, there arises the need for a more robust, fast, accurate, and efficient noninvasive cancer detection system (Selvathi, D & Aarthy Poornila, A. (2018). Deep Learning Techniques for Breast Cancer Detection Using Medical Image Analysis).\n\nEarly detection can give patients more treatment options. In order to detect signs of cancer, breast tissue from biopsies is stained to enhance the nuclei and cytoplasm for microscopic examination. Then, pathologists evaluate the extent of any abnormal structural variation to determine whether there are tumors.\n\nArchitectural Distortion (AD) is a very subtle contraction of the breast tissue and may represent the earliest sign of cancer. Since it is very likely to be unnoticed by radiologists, several approaches have been proposed over the years but none using deep learning techniques. \n\nAI will become a transformational force in healthcare and soon, computer vision models will be able to get a higher accuracy when researchers have the access to more medical imaging datasets.\n\nWe will develop a computer vision model to detect breast cancer in histopathological images. Two classes will be used in this project: **Benign and Malignant.**\n\n# Breast Cancer Histopathological Database (BreakHis)\n\nThe Breast Cancer Histopathological Image Classification (BreakHis) is composed of 7,909 microscopic images of breast tumor tissue collected from 82 patients using different magnifying factors (40X, 100X, 200X, and 400X). To date, it contains 2,480 benign and 5,429 malignant samples (700X460 pixels, 3-channel RGB, 8-bit depth in each channel, PNG format). This database has been built in collaboration with the P&D Laboratory\u200a-\u200aPathological Anatomy and Cytopathology, Parana, Brazil.\n\nThe dataset BreaKHis is divided into two main groups: benign tumors and malignant tumors. Histologically benign is a term referring to a lesion that does not match any criteria of malignancy\u200a-\u200ae.g., marked cellular atypia, mitosis, disruption of basement membranes, metastasize, etc. Normally, benign tumors are relatively \"innocents\", presents slow growing and remains localized. Malignant tumor is a synonym for cancer: lesion can invade and destroy adjacent structures (locally invasive) and spread to distant sites (metastasize) to cause death.\n\nThe dataset currently contains four histological distinct types of benign breast tumors: adenosis (A), fibroadenoma (F), phyllodes tumor (PT), and tubular adenona (TA); and four malignant tumors (breast cancer): carcinoma (DC), lobular carcinoma (LC), mucinous carcinoma (MC) and papillary carcinoma (PC).\n\nWe are going to determine if there exists cancer or not.\n\n![alt text](https://www.researchgate.net/publication/319974998/figure/fig2/AS:541235083309056@1506051907719/a-c-Indicative-cases-of-H-E-breast-cancer-histological-images-from-our-dataset-image.png)\n\n\n# Instructions\n\nThe project is broken down into multiple steps:\n\n    Load and preprocess the image dataset\n    Train the image classifier on your dataset\n    Use the trained classifier to predict image content\n\nEverything you need to recreate this project is on the jupyter notebook. Everything was coded in Google Colab, because of its GPU. The dataset was uploaded to Google Drive, so you can download it directly (the code to download it is in the notebook). For more details, the notebook includes the instructions to follow.\n\nIf you want to load the trained model, the code to download it, is in the notebook, in the \"Load the checkpoint\" section. The model was also uploaded to Google drive, so you can download it.\n\nThis project is updated to be compatible with PyTorch 0.4.0\n\nRead more about this project here: https://medium.com/datadriveninvestor/detecting-breast-cancer-in-histopathological-images-using-deep-learning-a66552aef98\n"
    },
    {
        "repo": "/AFAgarap/wisconsin-breast-cancer",
        "language": "Python",
        "readme_contents": "On Breast Cancer Detection: An Application of Machine Learning Algorithms on the Wisconsin Diagnostic Dataset\n===\n\n![](https://img.shields.io/badge/DOI-cs.LG%2F1711.07831-blue.svg)\n[![DOI](https://zenodo.org/badge/103154598.svg)](https://zenodo.org/badge/latestdoi/103154598)\n![](https://img.shields.io/badge/license-Apache--2.0-blue.svg)\n[![PyPI](https://img.shields.io/pypi/pyversions/Django.svg)]()\n\n*Note*: This repository is retired and will not be ported to use TF2. However, you may use this as a reference in doing so.\n\n*This paper was presented at the 2nd International Conference on Machine Learning and Soft Computing (ICMLSC) in Phu Quoc Island, Vietnam last February 2-4, 2018.*\n\nThe full paper on this project may be read at [arXiv.org](http://arxiv.org/abs/1711.07831).\n\n## Abstract\nThis paper presents a comparison of six machine learning (ML) algorithms: <a href=\"https://github.com/AFAgarap/gru-svm\">\nGRU-SVM</a><a href=\"http://arxiv.org/abs/1709.03082\">[4]</a>, Linear Regression, Multilayer Perceptron (MLP),\nNearest Neighbor (NN) search, Softmax Regression, and Support Vector Machine (SVM) on the Wisconsin Diagnostic Breast\nCancer (WDBC) dataset <a href=\"https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)\">[22]</a>\nby measuring their classification test accuracy and their sensitivity and specificity values. The said dataset consists\nof features which were computed from digitized images of FNA tests on a breast mass[22]. For the implementation of\nthe ML algorithms, the dataset was partitioned in the following fashion: 70% for training phase, and 30% for the\ntesting phase. The hyper-parameters used for all the classifiers were manually assigned. Results show that all the\npresented ML algorithms performed well (all exceeded 90% test accuracy) on the classification task. The MLP algorithm\nstands out among the implemented algorithms with a test accuracy of ~99.04% Lastly, the results are comparable\nwith the findings of the related studies[<a href=\"https://www.ijcit.com/archives/volume1/issue1/Paper010105.pdf\">18</a>\n, <a href=\"https://link.springer.com/chapter/10.1007%2F0-387-34224-9_58?LI=true\">23</a>].\n\n## Citation\nTo cite the paper, kindly use the following BibTex entry:\n```\n@inproceedings{Agarap:2018:BCD:3184066.3184080,\n author = {Agarap, Abien Fred M.},\n title = {On Breast Cancer Detection: An Application of Machine Learning Algorithms on the Wisconsin Diagnostic Dataset},\n booktitle = {Proceedings of the 2Nd International Conference on Machine Learning and Soft Computing},\n series = {ICMLSC '18},\n year = {2018},\n isbn = {978-1-4503-6336-5},\n location = {Phu Quoc Island, Viet Nam},\n pages = {5--9},\n numpages = {5},\n url = {http://doi.acm.org/10.1145/3184066.3184080},\n doi = {10.1145/3184066.3184080},\n acmid = {3184080},\n publisher = {ACM},\n address = {New York, NY, USA},\n keywords = {artificial intelligence, artificial neural networks, classification, linear regression, machine learning, multilayer perceptron, nearest neighbors, softmax regression, supervised learning, support vector machine, wisconsin diagnostic breast cancer dataset},\n}\n```\n\nTo cite the repository/software, kindly use the following BibTex entry:\n```\n@misc{abien_fred_agarap_2017_1098533,\n  author       = {Abien Fred Agarap},\n  title        = {AFAgarap/wisconsin-breast-cancer: v0.1.0-alpha},\n  month        = dec,\n  year         = 2017,\n  doi          = {10.5281/zenodo.1098533},\n  url          = {https://doi.org/10.5281/zenodo.1098533}\n}\n```\n\n## Machine Learning (ML) Algorithms\n\n* <a href=\"https://github.com/AFAgarap/wisconsin-breast-cancer/blob/master/main_gru_svm.py\">GRU-SVM</a>\n* <a href=\"https://github.com/AFAgarap/wisconsin-breast-cancer/blob/master/main_linear_regression.py\">Linear Regression</a>\n* <a href=\"https://github.com/AFAgarap/wisconsin-breast-cancer/blob/master/main_mlp.py\">Multilayer Perceptron</a>\n* <a href=\"https://github.com/AFAgarap/wisconsin-breast-cancer/blob/master/main_nearest_neighbor.py\">Nearest Neighbor</a>\n* <a href=\"https://github.com/AFAgarap/wisconsin-breast-cancer/blob/master/main_logistic_regression.py\">Softmax Regression</a>\n* <a href=\"https://github.com/AFAgarap/wisconsin-breast-cancer/blob/master/main_svm.py\">L2-SVM</a>\n\n## Results\nAll experiments in this study were conducted on a laptop computer with Intel Core(TM) i5-6300HQ CPU @ 2.30GHz x 4,\n16GB of DDR3 RAM, and NVIDIA GeForce GTX 960M 4GB DDR5 GPU.\n\n![](results/training_accuracy.png)\n\n**Figure 1. Training accuracy of the machine learning algorithms on breast cancer detection using WDBC.**\n\nFigure 1 shows the training accuracy of the ML algorithms: (1) GRU-SVM finished its training in 2 minutes and 54\nseconds with an average training accuracy of 90.6857639%, (2) Linear Regression finished its training in 35 seconds\nwith an average training accuracy of 92.8906257%, (3) MLP finished its training in 28 seconds with an average training\naccuracy of 96.9286785%, (4) Softmax Regression finished its training in 25 seconds with an average training accuracy\nof 97.366573%, and (5) L2-SVM finished its training in 14 seconds with an average training accuracy of 97.734375%.\nThere was no recorded training accuracy for Nearest Neighbor search since it does not require any training, as the norm\nequations (L1 and L2) are directly applied on the dataset to determine the \u201cnearest neighbor\u201d of a given data\npoint p_{i} \u2208 p.\n\n<br>\n\n**Table 1. Summary of experiment results on the machine learning algorithms.**\n\n|Parameter|GRU-SVM|Linear Regression|MLP|L1-NN|L2-NN|Softmax Regression|L2-SVM|\n|---------|-------|-----------------|---|-----|-----|------------------|------|\n|Accuracy|93.75%|96.09375%|99.038449585420729%|93.567252%|94.736844%|97.65625%|96.09375%|\n|Data points|384000|384000|512896|171|171|384000|384000|\n|Epochs|3000|3000|3000|1|1|3000|3000|\n|FPR|16.666667%|10.204082%|1.267042%|6.25%|9.375%|5.769231%|6.382979%|\n|FNR|0|0|0.786157%|6.542056%|2.803738%|0|2.469136%|\n|TPR|100%|100%|99.213843%|93.457944%|97.196262%|100%|97.530864%|\n|TNR|83.333333%|89.795918%|98.732958%|93.75%|90.625%|94.230769%|93.617021%|\n\nTable 1 summarizes the results of the experiment on the ML algorithms. The parameters recorded were test accuracy,\nnumber of data points (`epochs * dataset_size`), epochs, false positive rate (FPR), false negative rate (FNR), true\npositive rate (FPR), and true negative rate (TNR). All code implementations of the algorithms were written using Python\nwith TensorFlow as the machine intelligence library.\n\n## License\n```buildoutcfg\nCopyright 2017 Abien Fred Agarap\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n```\n"
    },
    {
        "repo": "/cheersyouran/cancer-detector",
        "language": "Python",
        "readme_contents": "## Multi-Instance-Learning to check breast cancer\n\n    |-- data\n    |   |-- L_CC.csv : L_CC dataset predictions\n    |   |-- L_ML.xlsx : L_ML dataset predictions\n    |   |-- R_CC.csv : R_CC dataset predictions\n    |   |-- R_ML.csv : R_ML dataset predictions\n    |\n    |-- CONFIG.py: Used for setting global parameters \n    |\n    |-- datafile.py: Used for reading/generating train/validation/test dataset\n    |\n    |-- model.py: CNN model\n    |\n    |-- run.py: Main function\n    |\n    |-- utils.py: Helper functions\n    |\n    |-- output_test_result.py: Merge the above data files\n\n## Result\n<img src=\"https://raw.githubusercontent.com/cheersyouran/cancer-detector/master/pic/1.png\" alt=\"2\" align=center />\n"
    },
    {
        "repo": "/bupt-ai-cz/BALNMP",
        "language": "Python",
        "readme_contents": "# Predicting Axillary Lymph Node Metastasis in Early Breast Cancer Using Deep Learning on Primary Tumor Biopsy Slides ![visitors](https://visitor-badge.glitch.me/badge?page_id=bupt-ai-cz.BALNMP)\n[Grand-Challenge](https://bcnb.grand-challenge.org/) | [Arxiv](https://arxiv.org/abs/2112.02222) | [Dataset Page](https://bupt-ai-cz.github.io/BCNB/) | [![Tweet](https://img.shields.io/twitter/url/http/shields.io.svg?style=social)](https://twitter.com/intent/tweet?text=Codes%20and%20Data%20for%20Our%20Paper:%20\"Predicting%20Axillary%20Lymph%20Node%20Metastasis%20in%20Early%20Breast%20Cancer\"%20&url=https://github.com/bupt-ai-cz/BALNMP)\n\nThis repo is the official implementation of our paper \"Predicting Axillary Lymph Node Metastasis in Early Breast Cancer Using Deep Learning on Primary Tumor Biopsy Slides\".\n\nOur paper is accepted by [Frontiers in Oncology](https://www.frontiersin.org/articles/10.3389/fonc.2021.759007/full), and you can also get access our paper from [Arxiv](https://arxiv.org/abs/2112.02222) or [MedRxiv](https://www.medrxiv.org/content/10.1101/2021.10.10.21264721).\n\n## News\n- We launched a [Grand Challenge: BCNB](https://bcnb.grand-challenge.org/) to promote relevant research.\n- We released our data. Please visit [homepage](https://bupt-ai-cz.github.io/BCNB/) to get the downloading information.\n- Paper codes are released, please see [code](./code) for more details.\n\n## Abstract\n\n- Objectives: To develop and validate a deep learning (DL)-based primary tumor biopsy signature for predicting axillary lymph node (ALN) metastasis preoperatively in early breast cancer (EBC) patients with clinically negative ALN.\n\n- Methods: A total of 1,058 EBC patients with pathologically confirmed ALN status were enrolled from May 2010 to August 2020. A DL core-needle biopsy (DL-CNB) model was built on the attention-based multiple instance-learning (AMIL) framework to predict ALN status utilizing the DL features, which were extracted from the cancer areas of digitized whole-slide images (WSIs) of breast CNB specimens annotated by two pathologists. Accuracy, sensitivity, specificity, receiver operating characteristic (ROC) curves, and areas under the ROC curve (AUCs) were analyzed to evaluate our model.\n\n- Results: The best-performing DL-CNB model with VGG16_BN as the feature extractor achieved an AUC of 0.816 (95% confidence interval (CI): 0.758, 0.865) in predicting positive ALN metastasis in the independent test cohort. Furthermore, our model incorporating the clinical data, which was called DL-CNB+C, yielded the best accuracy of 0.831 (95% CI: 0.775, 0.878), especially for patients younger than 50 years (AUC: 0.918, 95% CI: 0.825, 0.971). The interpretation of DL-CNB model showed that the top signatures most predictive of ALN metastasis were characterized by the nucleus features including density (*p* = 0.015), circumference (*p* = 0.009), circularity (*p* = 0.010), and orientation (*p* = 0.012).\n\n- Conclusion: Our study provides a novel DL-based biomarker on primary tumor CNB slides to predict the metastatic status of ALN preoperatively for patients with EBC.\n\n## Setup\n\n### Clone this repo\n\n```bash\ngit clone https://github.com/bupt-ai-cz/BALNMP.git\n```\n\n### Environment\n\nCreate environment and install dependencies.\n\n```bash\nconda create -n BALNMP python=3.6 -y\nconda activate BALNMP\npip install -r code/requirements.txt\n```\n\n### Dataset\n\nFor your convenience, we have provided preprocessed clinical data in `code/dataset`, please download the processed WSI patches from [here](https://drive.google.com/file/d/1wY5KIVixdwzZZq2m0IoqmBLp0YlwBAz6/view?usp=sharing) and unzip them by the following scripts:\n\n```bash\ncd code/dataset\n# download paper_patches.zip\nunzip paper_patches.zip\n```\n\n## Training\n\nOur codes have supported the following experiments, whose results have been presented in our [paper and supplementary material](https://arxiv.org/abs/2112.02222).\n\n> experiment_index:\n> \n> 0. N0 vs N+(>0)\n> 1. N+(1-2) vs N+(>2)\n> 2. N0 vs N+(1-2) vs N+(>2)\n> 3. N0 vs N+(1-2)\n> 4. N0 vs N+(>2)\n\nTo run any experiment, you can do as this:\n\n```bash\ncd code\nbash run.sh ${experiment_index}\n```\n\nFurthermore, if you want to try other settings, please see `train.py` for more details.\n\n## Paper results\n\nThe results in our paper are computed based on the [cut-off value in ROC](https://en.wikipedia.org/wiki/Youden%27s_J_statistic#:~:text=Youden%27s%20index%20is,as%20informedness.%5B3%5D). For your convenient reference, we have recomputed the classification results with argmax prediction rule, where the threshold for binary classification is 0.5, and the detailed recomputed results are [here](./recompute_results.md).\n\n### The performance in prediction of ALN status (N0 vs. N(+))\n\n<div align=\"center\">\n    <img src=\"imgs/N0 vs. N(+).png\" alt=\"N0 vs. N(+)\"/>\n</div>\n\n### The performance in prediction of ALN status (N0 vs. N + (1-2))\n\n<div align=\"center\">\n    <img src=\"imgs/N0 vs. N + (1-2).png\" alt=\"N0 vs. N + (1-2)\"/>\n</div>\n\n### The performance in prediction of ALN status (N0 vs. N + (>2))\n\n<div align=\"center\">\n    <img src=\"imgs/N0 vs. N + (\uff1e2).png\" alt=\"N0 vs. N + (\uff1e2)\"/>\n</div>\n\n## Implementation details\n\n### Data preparation\n\nIn our all experiments, the patch number (*N*) of each bag is fixed as 10, however, the bag number (*M*) for each WSI is not fixed and is dependent on the resolution of a WSI. According to our statistical results, the bag number (*M*) of WSIs varies from 1 to 300, which is not fixed for a WSI during training and testing. The process of dataset preparation is shown in the following figure, and the details are as follows:\n\n- Firstly, we cut out annotated tumor regions for each WSI, and there may exist multiple annotated tumor regions in a WSI.\n\n- Then, each extracted tumor region is cropped into amounts of non-overlapping square patches with a resolution of 256 \\* 256, and patches with a blank ratio greater than 0.3 are filtered out.\n\n- Finally, for each WSI, a bag is composed of randomly sampled 10 (*N*) patches, and the left patches which can not be grouped into a bag will be discarded.\n\nThe 5 clinical characteristics used in our experiments are age (numerical), tumor size (numerical), ER (categorical), PR (categorical), and HER2 (categorical), which are provided in our BCNB Dataset, and you can access them from our [BCNB Dataset](https://bupt-ai-cz.github.io/BCNB/).\n\n<div align=\"center\">\n    <img src=\"imgs/a.png\" alt=\"a\"/>\n</div>\n\n### Model testing\n\nAs mentioned above, a WSI is split into multiple bags, and each bag is inputted into the MIL model to obtain predicted probabilities. So for obtaining the comprehensive predicted results of a WSI during testing, we compute the average predicted probabilities of all bags to achieve \"Result Merging\".\n\n<div align=\"center\">\n    <img src=\"imgs/c.png\" alt=\"c\"/>\n</div>\n\n## Demo software\n\nWe have also provided software for easily checking the performance of our model to predict ALN metastasis.\n\nPlease download the software from [here](https://drive.google.com/drive/folders/18f0rEmV3dfdZsnFY2mfbF-MMtk9JkjZY?usp=sharing), and check the `README.txt` for usage. Please note that this software is only used for demo, and it cannot be used for other purposes.\n\n<div align=\"center\">\n    <img src=\"imgs/demo-software.png\" alt=\"demo-software\" height=\"25%\" width=\"25%\" />\n</div>\n\n## Citation\n\nIf this work helps your research, please cite this paper in your publications.\n\n```\n@article{xu2021predicting,\n  title={Predicting axillary lymph node metastasis in early breast cancer using deep learning on primary tumor biopsy slides},\n  author={Xu, Feng and Zhu, Chuang and Tang, Wenqi and Wang, Ying and Zhang, Yu and Li, Jie and Jiang, Hongchuan and Shi, Zhongyue and Liu, Jun and Jin, Mulan},\n  journal={Frontiers in oncology},\n  volume={11},\n  pages={759007},\n  year={2021},\n  publisher={Frontiers Media SA}\n}\n```\n\n## Contact\n\nIf you encounter any problems, please open an issue without hesitation, and you can also contact us with the following:\n\n- email: tangwenqi@bupt.edu.cn, czhu@bupt.edu.cn, drxufeng@mail.ccmu.edu.cn\n\n## Acknowledgements\n\nThis project is based on the following open-source projects. We thank their authors for making the source code publically available.\n\n- [AttentionDeepMIL](https://github.com/AMLab-Amsterdam/AttentionDeepMIL)"
    },
    {
        "repo": "/mrdvince/breast_cancer_detection",
        "language": "Python",
        "readme_contents": "# Breast Cancer Histopathological Dataset (BreakHis)\n\nA computer vision model to detect breast cancer in histopathological images. The two classes include:\n\n- Benign\n- and Malignant.\n\nFrom https://www.kaggle.com/ambarish/breakhis description:\n\nThe Breast Cancer Histopathological Image Classification (BreakHis) is composed of 7,909 microscopic images of breast tumor tissue collected from 82 patients using different magnifying factors (40X, 100X, 200X, and 400X). To date, it contains 2,480 benign and 5,429 malignant samples (700X460 pixels, 3-channel RGB, 8-bit depth in each channel, PNG format). This database has been built in collaboration with the P&D Laboratory\u200a-\u200aPathological Anatomy and Cytopathology, Parana, Brazil.\n\nThe dataset BreaKHis is divided into two main groups: benign tumors and malignant tumors. Histologically benign is a term referring to a lesion that does not match any criteria of malignancy\u200a-\u200ae.g., marked cellular atypia, mitosis, disruption of basement membranes, metastasize, etc. Normally, benign tumors are relatively \"innocents\", presents slow growing and remains localized. Malignant tumor is a synonym for cancer: lesion can invade and destroy adjacent structures (locally invasive) and spread to distant sites (metastasize) to cause death.\n\nThe dataset currently contains four histological distinct types of benign breast tumors: adenosis (A), fibroadenoma (F), phyllodes tumor (PT), and tubular adenona (TA); and four malignant tumors (breast cancer): carcinoma (DC), lobular carcinoma (LC), mucinous carcinoma (MC) and papillary carcinoma (PC).\n\nWe are going to determine if there exists cancer or not.\n\n<img src=\"images/mal_ben.png\"/>\n\n\n# Getting Started\n\nThe project is broken down into the following steps:\n<ol>\n    <li>  Load and preprocess the image dataset</li>\n    <li> Train the image classifier on your dataset </li>\n    <li> Use the trained classifier to predict image content </li> </ol>\n\n### Dependencies\n\nTo set up your python environment to run the code in this repository, follow the instructions below.\n\n1. Create (and activate) a new environment with Python 3.6.\n\n\t- __Linux__ or __Mac__: \n\t```bash\n\tconda create --name bcdp python=3.6\n\tconda activate bcdp\n\t```\n\t- __Windows__: \n\t```bash\n\tconda create --name bcdp python=3.6 \n\tactivate bcdp\n\t```\n\n2. Clone the repository (if you haven't already!), and navigate to the `breast-cancer-detection` folder.  Then, install several dependencies.\n```bash\ngit clone https://github.com/mrdvince/breast-cancer-detection.git\ncd breast-cancer-detection\npip install -r requirements.txt\n```\n\n4. Create a `data` in the root of the project\n\n- Download the dataset from [kaggle](https://www.kaggle.com/ambarish/breakhis)\n- Extract the downloaded zip file to the data folder\n\n5. Run `python train.py --config config.json` on the cli with the the environment activated.\n\n6. Config (optional)\n\n- Modify the config json file to experiemnt with different parameters\n- `Note: ` Check out this repository for a better understanding on how the folders and files are structured https://github.com/victoresque/pytorch-template\n\n- **_Visiualization_**\n        - By default tensorboard is set to `true` in the config file. \n        - Once training is done visit run `tensorboard --logdir saved/log/` on the cli and visit `http://localhost:6006` on the browser.\n\n## Training log metrics\nModel used is a densent 121 model(pretrained) and is included in the current repo in the saved folder\nCan be used directly or train a different by defining a different model in the models folder and updating it the config file.\n\n#### Training and validation accuracies (logged on tensorboard)\n<img src=\"images/train_val_acc.png\"/>\n\n#### Training and validation loss (logged on tensorboard)\n<img src=\"images/train_val_loss.png\"/>\n"
    },
    {
        "repo": "/dangnh0611/kaggle_rsna_breast_cancer",
        "language": null,
        "readme_contents": "Below you can find an outline of how to reproduce my solution for the [RSNA Screening Mammography Breast Cancer Detection](https://www.kaggle.com/competitions/rsna-breast-cancer-detection) competition. \nIf you run into any trouble with the setup/code or have any questions please contact me at `dangnh0611@gmail.com`.\n\nSolution write up: https://www.kaggle.com/competitions/rsna-breast-cancer-detection/discussion/392449\n\n**Notes:**\n- Copy of the trained models can not be upload since the total size is > 2GB. So I create a kaggle dataset to store theme: https://www.kaggle.com/datasets/dangnh0611/rsna-breast-cancer-detection-best-ckpts\n\nPlease download those trained models and put in `assets/trained/`:\n```bash\n# this assume that kaggle api is installed: https://github.com/Kaggle/kaggle-api\nkaggle datasets download -d dangnh0611/rsna-breast-cancer-detection-best-ckpts -p assets/trained\nunzip rsna-breast-cancer-detection-best-ckpts.zip -d assets/trained/\nrm assets/trained/rsna-breast-cancer-detection-best-ckpts.zip\n```\n\n\n# TABLE OF CONTENTS\n- [TABLE OF CONTENTS](#table-of-contents)\n- [1. ARCHIVE CONTENTS](#1-archive-contents)\n- [2. HARDWARE](#2-hardware)\n- [3. DATA SETUP](#3-data-setup)\n- [4. SOLUTION PIPELINE](#4-solution-pipeline)\n- [5. SOLUTION REPRODUCING](#5-solution-reproducing)\n  - [5.1. Use trained models to make predictions](#51-use-trained-models-to-make-predictions)\n    - [5.1.1. Convert trained YOLOX to TensorRT](#511-convert-trained-yolox-to-tensorrt)\n    - [5.1.2. Convert trained 4 x Convnext-small models to TensorRT](#512-convert-trained-4-x-convnext-small-models-to-tensorrt)\n    - [5.1.3. Submission](#513-submission)\n  - [5.2. Keep trained YOLOX, re-train Convnext-small classification models](#52-keep-trained-yolox-re-train-convnext-small-classification-models)\n    - [5.2.1. Convert trained YOLOX to TensorRT](#521-convert-trained-yolox-to-tensorrt)\n    - [5.2.2. Prepair datasets to train classification models](#522-prepair-datasets-to-train-classification-models)\n    - [5.2.3. Perform 4-folds splitting on competition data](#523-perform-4-folds-splitting-on-competition-data)\n    - [5.2.4. Training 4 x Convnext-small classification models](#524-training-4-x-convnext-small-classification-models)\n    - [5.2.5. Checkpoints selection](#525-checkpoints-selection)\n    - [5.2.6. Convert selected best Convnext models to TensorRT](#526-convert-selected-best-convnext-models-to-tensorrt)\n    - [5.2.7. Submission](#527-submission)\n  - [5.3. Re-train all parts from scratch](#53-re-train-all-parts-from-scratch)\n    - [5.3.1. Prepair dataset for training YOLOX ROI detector](#531-prepair-dataset-for-training-yolox-roi-detector)\n    - [5.3.2. Retrain YOLOX for breast ROI detection](#532-retrain-yolox-for-breast-roi-detection)\n    - [5.3.3. Prepair datasets to train classification models](#533-prepair-datasets-to-train-classification-models)\n    - [5.3.4. Perform 4-folds splitting on competition data](#534-perform-4-folds-splitting-on-competition-data)\n    - [5.3.5. Training 4 x Convnext-small classification models](#535-training-4-x-convnext-small-classification-models)\n    - [5.3.6. Checkpoints selection](#536-checkpoints-selection)\n    - [5.3.7. Convert selected best Convnext models to TensorRT](#537-convert-selected-best-convnext-models-to-tensorrt)\n    - [5.3.8. Submission](#538-submission)\n\n\n\n# 1. ARCHIVE CONTENTS\n- `assets`: contain neccessary data files, trained models\n    - `assets/data/`: csv label for external datasets (BMCD and CMMD), breast ROI box annotation in YOLOv5 format\n    - `assets/public_pretrains/`: publicly available pretrains\n    - `assets/trained/`: trained models, used for winning submission\n- `datasets/`: where to store datasets (competition + external), expected to contain both raw and cleaned version.\n    - `datasets/raw/`: raw version of competion data + all external datasets: BMCD, CDD-CESM, CMMD, MiniDDSM, Vindr. For how to correctly structure datasets, please refer to [docs/DATASETS.md](docs/DATASETS.md)\n- `docker/`: Dockerfile\n- `docs/`: documentations\n- `src/`: contain almost source code for this project\n    - `src/roi_det`: for training breast ROI detection model (YOLOX)\n    - `src/pytorch-image-models`: for training classification model (Convnext-small)\n    - `src/submit`: code to generate predictions (submission)\n    - `src/tools`: contain python scripts, bash scripts to prepair datasets, training and convert models,..\n    - `src/utils`: Utilities for dicom processing,..\n- `SETTINGS.json`: define relative paths for IO\n\n\n---\n\n\n`SETTINGS.json` defines base paths for IO:\n\n-  `RAW_DATA_DIR`: Where to store raw dataset, including both competition dataset and external datasets.\n- `PROCESSED_DATA_DIR`: Where to store processed/cleaned datasets\n- `MODEL_CHECKPOINT_DIR`: Store intermediate checkpoints during training\n- `MODEL_FINAL_SELECTION_DIR`: Where to store final (best) models used for submission\n- `SUBMISSION_DIR`: Where to store final submission/inference results\n- `ASSETS_DIR`: Store trained models, manually annotated datasets/files. This must not be changed and define here for easier looking up only.\n- `TEMP_DIR`: Where to store intermediate results/files\n\n\n# 2. HARDWARE\nThe following machine were used to create the final solution: [NVIDIA DGX A100](https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/nvidia-dgx-a100-datasheet.pdf). Most of my experiments can be done using 1-3 A100 GPUs.\nHowever, final results can be easily reproduced using a single A100 GPU (40GB GPU Memory).\n- OS: Ubuntu 18.04\n- NVIDIA Driver version: 450.80.02\n- CUDA 11.6, CUDNN 8.8\n- Dependencies (recommended): see [docker/Dockerfile](docker/Dockerfile)\n- Pip packages listed in [requirements.txt](requirements.txt)\n\n\n# 3. DATA SETUP\nRefer to [docs/DATASETS.md](docs/DATASETS.md) for details on how to correctly setup datasets.\n\n\n# 4. SOLUTION PIPELINE\nThere are some stages to reproduce the entire solutions. I will briefly describe it for easier further understanding.\n1. Train a YOLOX on some of competition images for breast ROI detection\n    - Convert competition dicom files to 8-bits png images\n    - Convert detection labels in YOLOv5 format to COCO format (YOLOX accepts COCO format without any modifications)\n    - Train a YOLOX-nano 416x416 model on those images (521 train images, 50 val images)\n    - Convert trained YOLOX model from Torch to TensorRT engine.\n2. Using trained YOLOX TensorRT engine to crop breast ROI region, save to disk as 8-bits pngs\n    - Clean and re-structure raw datasets (competition data + external data) in an unified way (standardize the format/structure)\n    - Dicom decoding --> ROI detection (YOLOX) --> ROI crop --> normalization --> save to disk\n3. Train Convnext-small model for classification using those saved ROI images\n    - Do a 4-folds splits on competition data.\n    - Train 4 Convnext-small model on each folds\n    - Select best checkpoint for each fold\n    - Convert those models from Torch to TensorRT\n4. Inference on test data (submission)\n\n\n# 5. SOLUTION REPRODUCING\nAll the following instructions assume that datasets (competition + external data) are all set up.\nThere are 4 options to reproduce the solutions:\n1. Use trained models\n    - No training, just use trained models in `assets/trained` to make predictions\n\n\n2. Do not re-train YOLOX, fully reproduce Convnext-small classification models\n    - Skip re-train the YOLOX part, use (my) trained YOLOX for further steps\n    - Re-train 4x Convnext-small classification models. This part can be 100% reproduced (give you identical models/training log/result) without any randomness.\n    - This method should give 100% identical score on both CV/LB/PB\n\n3. Re-train all parts (reproduce from scratch)\n    - Won't use any of (my) trained models in any parts, but re-train all of theme from scratch\n    - This may not give 100% identical results/scores. The reason is that YOLOX can't be fully reproduced to get EXACTLY same model as used in winning submission. More details [here](docs/RANDOMNESS.md)\n    - Note that dataset used for training Convnext-small classification models is generated base on YOLOX's prediction, so changes in YOLOX will cause changes in Convnext-small classification models --> Convnext-small classification models will also be unreproducible (in a 100% way).\n    - But in general, it should give nearly identical results/scores within a reasonable margin.\n\n\n\n\n------------------------------\n\n\n\n\n## 5.1. Use trained models to make predictions\n\n### 5.1.1. Convert trained YOLOX to TensorRT\n\nA YOLOX-nano 416 engine which was optimized for NVIDIA A100 is provided at [assets/trained/yolox_nano_416_roi_trt_a100.pth](assets/trained/yolox_nano_416_roi_trt_a100.pth).\nHowever, the recommended way is to convert it to TensorRT, optimized for your environment/hardware:\n```bash\nPYTHONPATH=$(pwd)/src/roi_det/YOLOX:$PYTHONPATH python3 src/roi_det/YOLOX/tools/trt.py \\\n    -expn trained_yolox_nano_416_to_tensorrt \\\n    -f src/roi_det/YOLOX/exps/projects/rsna/yolox_nano_bre_416.py \\\n    -c assets/trained/yolox_nano_416_roi_torch.pth \\\n    --save-path assets/trained/yolox_nano_416_roi_trt.pth \\\n    -b 1\n```\nBehaviors:\n- Create new directory `{MODEL_CHECKPOINT_DIR}/yolox_roi_det/trained_yolox_nano_416_to_tensorrt/`.\n- The converted YOLOX TensorRT engine will also be saved to `./assets/trained/yolox_nano_416_roi_trt.pth`\n\n### 5.1.2. Convert trained 4 x Convnext-small models to TensorRT\n```bash\nPYTHONPATH=$(pwd)/src/pytorch-image-models/:$PYTHONPATH python3 src/tools/convert_convnext_tensorrt.py --mode trained\n```\nBehaviours: Save a 4-folds combined TensorRT engine to `./assets/trained/best_ensemble_convnext_small_batch2_fp32.engine'`.\n\nIt takes 5-10 minutes for Kaggle's P100 GPU to finish, but take about 1 hour for A100 GPU (my case). \n\n\n### 5.1.3. Submission\n```bash\nPYTHONPATH=$(pwd)/src/pytorch-image-models/:$PYTHONPATH python3 src/submit/submit.py --mode trained --trt\n```\n\nBehaviours:\n- Create a temporary directory storing 8-bits png images at `{TEMP_DIR}/pngs/` and expected to be removed once inference done. \n- Save submission csv result to `{SUBMISSION_DIR}/submission.csv`\n\n\n------------------------------\n\n\n## 5.2. Keep trained YOLOX, re-train Convnext-small classification models\n\n### 5.2.1. Convert trained YOLOX to TensorRT\n\nA YOLOX-nano 416 engine which was optimized for NVIDIA A100 is provided at [assets/trained/yolox_nano_416_roi_trt_a100.pth](assets/trained/yolox_nano_416_roi_trt_a100.pth).\nHowever, the recommended way is to convert it to TensorRT, optimized for your environment/hardware:\n```bash\nPYTHONPATH=$(pwd)/src/roi_det/YOLOX:$PYTHONPATH python3 src/roi_det/YOLOX/tools/trt.py \\\n    -expn trained_yolox_nano_416_to_tensorrt \\\n    -f src/roi_det/YOLOX/exps/projects/rsna/yolox_nano_bre_416.py \\\n    -c assets/trained/yolox_nano_416_roi_torch.pth \\\n    --save-path assets/trained/yolox_nano_416_roi_trt.pth \\\n    -b 1\n```\nBehaviors:\n- Create new directory `{MODEL_CHECKPOINT_DIR}/yolox_roi_det/trained_yolox_nano_416_to_tensorrt/`.\n- The converted YOLOX TensorRT engine will also be saved to `./assets/trained/yolox_nano_416_roi_trt.pth`\n\n### 5.2.2. Prepair datasets to train classification models\n```bash\npython3 src/tools/prepair_classification_dataset.py --num-workers 8 --roi-yolox-engine-path assets/trained/yolox_nano_416_roi_trt.pth\n```\nBehaviors:\n- Create a `stage1_images` in each raw dataset directory: `{RAW_DATA_DIR}/{dataset_name}/stage1_images` for the intermediate stage.\n- Create a new directory `{PROCESSED_DATA_DIR}/classification/` contains 8-bits png images `{PROCESSED_DATA_DIR}/classification/{dataset_name}/cleaned_images/` and cleaned label file `{PROCESSED_DATA_DIR}/classification/{dataset_name}/cleaned_label.csv` for each dataset.\n\n### 5.2.3. Perform 4-folds splitting on competition data\n```bash\npython3 src/tools/cv_split.py\n```\nBehaviors: Create new directory and saving csv files in `{PROCESSED_DATA_DIR}/rsna-breast-cancer-detection/cv/v2/`\n\n### 5.2.4. Training 4 x Convnext-small classification models\n```bash\npython3 src/tools/make_train_bash_script.py --mode fully_reproduce\n```\nThis will save a file named `_train_script_auto_generated.sh` in current directory, which include commands and instructions to train Convnext-small classification models.\nTo reproduce using single GPU, simply run\n```\nsh ./_train_script_auto_generated.sh\n```\nThis could take 8 days to finish training (around 2 days for each fold).\n\nOr if you have multiple GPUs and want to speed up training, simply follow instructions in the generated train script `_train_script_auto_generated.sh` and run each command in parallel using different GPUs. For more details on the training process, take a look at [my write up](https://www.kaggle.com/competitions/rsna-breast-cancer-detection/discussion/392449), part `4.3.Training`\n\nBehaviours:\n- This assumes that directory `{MODEL_CHECKPOINT_DIR}/timm_classification/` is empty before start any train commands\n- Saving checkpoints/logs to `{MODEL_CHECKPOINT_DIR}/timm_classification/`, contains 6 sub-directories named \n    - `fully_reproduce_train_fold_2`\n    - `fully_reproduce_train_fold_3`\n    - `stage1_fully_reproduce_train_fold_0`\n    - `stage1_fully_reproduce_train_fold_1`\n    - `stage2_fully_reproduce_train_fold_0`\n    - `stage2_fully_reproduce_train_fold_1`\n\n\n### 5.2.5. Checkpoints selection\n```bash\npython3 src/tools/select_classification_best_ckpts.py --mode fully_reproduce\n```\n\nBehaviours: \n- This could overwrite convnext checkpoint files in `{MODEL_FINAL_SELECTION_DIR}/`\n- Select and copy the 4 best checkpoints for each folds to `{MODEL_FINAL_SELECTION_DIR}/`:\n    - `{MODEL_FINAL_SELECTION_DIR}/best_convnext_fold_0.pth.tar`\n    - `{MODEL_FINAL_SELECTION_DIR}/best_convnext_fold_1.pth.tar`\n    - `{MODEL_FINAL_SELECTION_DIR}/best_convnext_fold_2.pth.tar`\n    - `{MODEL_FINAL_SELECTION_DIR}/best_convnext_fold_3.pth.tar`\n\n\n### 5.2.6. Convert selected best Convnext models to TensorRT\n```bash\nPYTHONPATH=$(pwd)/src/pytorch-image-models/:$PYTHONPATH python3 src/tools/convert_convnext_tensorrt.py --mode reproduce\n```\nBehaviours: Save a 4-folds combined TensorRT engine to `{MODEL_FINAL_SELECTION_DIR}/best_ensemble_convnext_small_batch2_fp32.engine'`.\n\nIt takes 5-10 minutes for Kaggle's P100 GPU to finish, but take about 1 hour for A100 GPU (my case). \n\n\n### 5.2.7. Submission\n```bash\nPYTHONPATH=$(pwd)/src/pytorch-image-models/:$PYTHONPATH python3 src/submit/submit.py --mode partial_reproduce --trt\n```\n\nBehaviours:\n- Create a temporary directory storing 8-bits png images at `{TEMP_DIR}/pngs/` and expected to be removed once inference done. \n- Save submission csv result to `{SUBMISSION_DIR}/submission.csv`\n \n\n\n------------------------------\n\n\n\n\n## 5.3. Re-train all parts from scratch\n\n\n### 5.3.1. Prepair dataset for training YOLOX ROI detector\n```bash\npython3 src/tools/prepair_roi_det_dataset.py --num-workers 4\n```\nBehaviors:\n- Copy mannual annotated breast ROI box in YOLOv5 format from `./assets/data/roi_det_yolov5_format/` to `{PROCESSED_DATA_DIR}/roi_det_yolox/yolov5_format/`\n- Decode 571 dicom files in competition dataset to 8-bits png, stored at `{PROCESSED_DATA_DIR}/roi_det_yolox/yolov5_format/images/`\n- Convert from YOLOv5 format to COCO format, stored at `{PROCESSED_DATA_DIR}/roi_det_yolox/coco_format/`\n\n### 5.3.2. Retrain YOLOX for breast ROI detection\n```bash\nsh src/tools/train_and_convert_yolox_trt.sh\n```\nBehaviors:\n- Train YOLOX, saving checkpoints to `{MODEL_CHECKPOINT_DIR}/yolox_roi_det/yolox_nano_416_reproduce/`\n- (Optional) Perform evaluation on best checkpoint, print results\n- Convert newly trained best checkpoint to TensorRT, stored in `{MODEL_CHECKPOINT_DIR}/yolox_roi_det/yolox_nano_416_reproduce/`\n- Copy best Torch checkpoint to `{MODEL_FINAL_SELECTION_DIR}/yolox_nano_416_roi_torch.pth`\n- Copy the converted best TensorRT engine in previous step to `{MODEL_FINAL_SELECTION_DIR}/yolox_nano_416_roi_trt.pth`\n\n\n### 5.3.3. Prepair datasets to train classification models\nThis will use newly trained YOLOX in previous step as breast ROI extractor.\n```bash\npython3 src/tools/prepair_classification_dataset.py --num-workers 8\n```\nBehaviors:\n- Create a `stage1_images` in each raw dataset directory: `{RAW_DATA_DIR}/{dataset_name}/stage1_images` for the intermediate stage.\n- Create a new directory `{PROCESSED_DATA_DIR}/classification/` contains 8-bits png images `{PROCESSED_DATA_DIR}/classification/{dataset_name}/cleaned_images/` and cleaned label file `{PROCESSED_DATA_DIR}/classification/{dataset_name}/cleaned_label.csv` for each dataset.\n\n### 5.3.4. Perform 4-folds splitting on competition data\n```bash\npython3 src/tools/cv_split.py\n```\nBehaviors: Create new directory and saving csv files in `{PROCESSED_DATA_DIR}/rsna-breast-cancer-detection/cv/v2/`\n\n### 5.3.5. Training 4 x Convnext-small classification models\n```bash\npython3 src/tools/make_train_bash_script.py --mode fully_reproduce\n```\nThis will save a file named `_train_script_auto_generated.sh` in current directory, which include commands and instructions to train Convnext-small classification models.\nTo reproduce using single GPU, simply run\n```\nsh ./_train_script_auto_generated.sh\n```\nThis could take 8 days to finish training (around 2 days for each fold).\n\nOr if you have multiple GPUs and want to speed up training, simply follow instructions in the generated train script `_train_script_auto_generated.sh` and run each command in parallel using different GPUs. For more details on the training process, take a look at [my write up](https://www.kaggle.com/competitions/rsna-breast-cancer-detection/discussion/392449), part `4.3.Training`\n\nBehaviours:\n- This assumes that directory `{MODEL_CHECKPOINT_DIR}/timm_classification/` is empty before start any train commands\n- Saving checkpoints/logs to `{MODEL_CHECKPOINT_DIR}/timm_classification/`, contains 6 sub-directories named \n    - `fully_reproduce_train_fold_2`\n    - `fully_reproduce_train_fold_3`\n    - `stage1_fully_reproduce_train_fold_0`\n    - `stage1_fully_reproduce_train_fold_1`\n    - `stage2_fully_reproduce_train_fold_0`\n    - `stage2_fully_reproduce_train_fold_1`\n\n\n### 5.3.6. Checkpoints selection\n```bash\npython3 src/tools/select_classification_best_ckpts.py --mode fully_reproduce\n```\nBehaviours: \n- This could overwrite convnext checkpoint files in `{MODEL_FINAL_SELECTION_DIR}/`\n- Select and copy the 4 best checkpoints for each folds to `{MODEL_FINAL_SELECTION_DIR}/`:\n    - `{MODEL_FINAL_SELECTION_DIR}/best_convnext_fold_0.pth.tar`\n    - `{MODEL_FINAL_SELECTION_DIR}/best_convnext_fold_1.pth.tar`\n    - `{MODEL_FINAL_SELECTION_DIR}/best_convnext_fold_2.pth.tar`\n    - `{MODEL_FINAL_SELECTION_DIR}/best_convnext_fold_3.pth.tar`\n\n\n### 5.3.7. Convert selected best Convnext models to TensorRT\n```bash\nPYTHONPATH=$(pwd)/src/pytorch-image-models/:$PYTHONPATH python3 src/tools/convert_convnext_tensorrt.py --mode reproduce\n```\nBehaviours: Save a 4-folds combined TensorRT engine to `{MODEL_FINAL_SELECTION_DIR}/best_ensemble_convnext_small_batch2_fp32.engine'`.\n\nIt takes 5-10 minutes for Kaggle's P100 GPU to finish, but take about 1 hour for A100 GPU (my case). \n\n\n### 5.3.8. Submission\n```bash\nPYTHONPATH=$(pwd)/src/pytorch-image-models/:$PYTHONPATH python3 src/submit/submit.py --mode reproduce --trt\n```\n\nBehaviours:\n- Create a temporary directory storing 8-bits png images at `{TEMP_DIR}/pngs/` and expected to be removed once inference done. \n- Save submission csv result to `{SUBMISSION_DIR}/submission.csv`\n\n"
    },
    {
        "repo": "/gscdit/Breast-Cancer-Detection",
        "language": "Jupyter Notebook",
        "readme_contents": "# Breast Cancer Detection\nBreast Cancer Detection Using Machine Learning\n\n<img src=\"https://cdn-images-1.medium.com/max/2600/1*gNcFEL1cpGpDC4vo1zUAWA.png\" />\n\n# What is Breast Cancer?\n\nCancer occurs when changes called mutations take place in genes that regulate cell growth. The mutations let the cells divide and multiply in an uncontrolled, chaotic way. The cells keep on proliferating, producing copies that get progressively more abnormal. In most cases, the cell copies eventually end up forming a tumor.\n\nBreast cancer occurs when a malignant (cancerous) tumor originates in the breast. As breast cancer tumors mature, they may metastasize (spread) to other parts of the body. The primary route of metastasis is the lymphatic system which, ironically enough, is also the body's primary system for producing and transporting white blood cells and other cancer-fighting immune system cells throughout the body. Metastasized cancer cells that aren't destroyed by the lymphatic system's white blood cells move through the lymphatic vessels and settle in remote body locations, forming new tumors and perpetuating the disease process.\n\nBreast cancer is not just a woman's disease. It is quite possible for men to get breast cancer, although it occurs less frequently in men than in women. Our discussion will focus primarily on breast cancer as it relates to women but it should be noted that much of the information is also applicable for men.\n\n# Facts And Figures\n\nBreast cancer is the most commonly occurring cancer in women and the second most common cancer overall. There were over 2 million new cases in 2018.\n\n**Prevalence**\n\n1) Asia\n \n   Percentage of world population: 59 \n   Percentage of new breast cancer cases: 39\n   Percentage of breast cancer deaths: 44  \n\n2) Africa\n\n   Percentage of world population: 15\n   Percentage of new breast cancer cases: 8\n   Percentage of breast cancer deaths: 12\n   \n3) U.S. and Canada\n\n   Percentage of world population: 5\n   Percentage of new breast cancer cases: 15\n   Percentage of breast cancer deaths: 9\n   \n(Data from Global Cancer Facts and Figures, 3rd Edition, page 37)\n\n**Incidence rates per 100,000 women**\n\n1) Countries with highest incidence:\n   The Netherlands: 95.3\n   France: 94.6\n   U.S: (white people only - other races have lower incidence): 90.6\n\n2) Countries with lowest incidence:\n\n   Thailand: 25.6\n   Algeria: 29.8\n   India: 30.9\n   \n(Data from Global Cancer Facts and Figures, 3rd Edition, page 42)\n\nThe American Cancer Society's estimates for breast cancer in the United States for 2019 are: \n\n- About 268,600 new cases of invasive breast cancer will be diagnosed in women. \n\n- About 62,930 new cases of carcinoma in situ (CIS) will be diagnosed (CIS is non-invasive and is the earliest form of breast cancer).\n\n- About 41,760 women will die from breast cancer.\n\n# Role Of Machine Learning In Detection Of Breast Cancer\n\nA mammogram is an x-ray picture of the breast. It can be used to check for breast cancer in women who have no signs or symptoms of the disease. It can also be used if you have a lump or other sign of breast cancer.\n\nScreening mammography is the type of mammogram that checks you when you have no symptoms. It can help reduce the number of deaths from breast cancer among women ages 40 to 70. But it can also have drawbacks. Mammograms can sometimes find something that looks abnormal but isn't cancer. This leads to further testing and can cause you anxiety. Sometimes mammograms can miss cancer when it is there. It also exposes you to radiation. You should talk to your doctor about the benefits and drawbacks of mammograms. Together, you can decide when to start and how often to have a mammogram.\n\nNow while its difficult to figure out for physicians  by seeing only images of x-ray that weather the tumor is toxic or not training a machine learning model according to the identification of tumour can be of great help.\n\n# Project Description\n\nThe Project is Inspired by the Original Publication of...\n\n1)Do\u00e7. Dr. Ahmet MERT\nM\u00fchendislik ve Do\u011fa Bilimleri Fak\u00fcltesi > Mekatronik M\u00fchendisli\u011fi B\u00f6l\u00fcm\u00fc\n\n2)Dr. Erdem Bilgili\nPiri Reis University\n\n3)Dr. Aydin Akan\nIzmir Katip Celebi University, Izmir, Turkey\n\nThe Projects Features Detection of Breast Cancer Using Machine Learning.\nIt has been tested that while there exists several machine learning models,Support Vector Machine or SVM in short is reported to have highest accuracy of (approximately 97%) in detecting breast cancer.\n\nThe dataset used in this project is from Breast Cancer Wisconsin (Diagnostic) Data Set, however it can be directly accessed from Scikit learn library's collection of datasets as... \n\nsklearn.datasets.load_breast_cancer\n\n...aslo csv file of data has been externally loaded in the repo :)\n\n# RESULTS \nAn accuracy of 96% was achieved by using SVM model and after normalization technique after optimisation of C and Gamma parameters it was increased to a value of a 97%.\n\n# How to access this project.\n\nFork the Repository and clone it in ur PC , voila its urs now use it your own way i hope u will do even cooler things ;)\n\n# Final Note \n\nTo conclude i would like to say that Machine Learning has inspired me for doing great things by learning about great things this project is one of my starters project in this domain and with it iam able to experience not only life of an Enginner but a Physican as well. Doing this project was a pleasure for me and finding out about Death rate due to Breast Cancer really painful , a lot of information i gathered which i could have never known about and loads of learning happened in between so if you are doing this Project i really hope you too will enjoy playing with the dataset ,rejoice your imagination of \"Whatif this Could Happen\" and unleash the creativity and potential that resides within you.\n\nFinally thanks for having me with you for quiet a lot of your precious time hope to see you next with real goods stuffs ahead , feel free to connect with me I WON'T BITE and would love collaborating with you,you can find my contact information in my Github Profile only.\n\nAlso if you enjoyed this and you are not a sadist then dont forget to leave a star, you know those star and Green square really satisfy me :)\n\nUntil we Meet Next HAPPY LEARNING \u2764\ufe0f.\n"
    },
    {
        "repo": "/microsoft/Machine-Learning-Patient-Risk-Analyzer-SA",
        "language": "C#",
        "readme_contents": "![](Resource_Deployment/img/banner.png)\n\n# About this repository \nMachine Learning Patient Risk Analyzer Solution Accelerator is an end-to-end (E2E) healthcare app that leverages ML prediction models (e.g., Diabetes Mellitus (DM) patient 30-day re-admission, breast cancer risk, etc.) to demonstrate how these models can provide key insights for both physicians and patients.  Patients can easily access their appointment and care history with infused cognitive services through a conversational interface.  \n  \nIn addition to providing new insights for both doctors and patients, the app also provides the Data Scientist/IT Specialist with one-click experiences for registering and deploying a new or existing model to Azure Kubernetes Clusters, and best practices for maintaining these models through Azure MLOps.\n\n## Prerequisites\nIn order to successfully complete this solution accelerator, you will need to have access to and/or provisioned the following resources:\n\n1. Access to an [Azure Subscription ](http://portal.azure.com)\n2. [Power Apps](http://www.powerapps.com) License (or free trial)\n3. [PowerShell 7.1](https://docs.microsoft.com/en-us/powershell/scripting/install/installing-powershell?view=powershell-7.1)\n4. [Azure CLI](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli)\n5. [Docker Desktop](https://hub.docker.com/editions/community/docker-ce-desktop-windows) - Required for Debugging in local or containerizing codes in the Deployment process\n\n## Architecture Overview \nThe architecture diagram below details what you will be building for this Solution Accelerator.\n\n![Architecture Diagram](./Resource_Deployment/img/ReferenceArchitecture.png)\n\n## Azure Development and Analytics Platforms \nThe directions provided for this repository assume fundemental working knowledge of Azure, Azure Synapse Analytics, Azure Machine Learning and Azure Cognitive Services\n1. [Azure Machine Learning](https://azure.microsoft.com/en-us/services/machine-learning/)\n2. [Azure Synapse Analytics](https://azure.microsoft.com/en-us/services/synapse-analytics/)\n3. [Azure Cognitive Service](https://azure.microsoft.com/en-us/services/cognitive-services/)\n4. [Azure Kubernetes Service](https://azure.microsoft.com/en-us/services/kubernetes-service/)\n5. [Azure Cosmos DB](https://azure.microsoft.com/en-us/services/cosmos-db)\n6. [Power Apps](https://docs.microsoft.com/en-us/powerapps/)\n7. [Power Virtual Agent](https://powervirtualagents.microsoft.com/)\n\n\n## Getting Started\nStart by deploying the [resources](./Resource_Deployment/ResourceDeployment.md) needed for this solution,\n\n1. Press Deploy Azure :  \n\n    [![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fmicrosoft%2FMachine-Learning-Patient-Risk-Analyzer-SA%2Fmain%2FResource_Deployment%2Fazuredeploy.json)\n\n2. Clone repository and navigate to the root of the directory\n3. Go to [Deployment Guide](./Resource_Deployment/README.md) under [./Resource_Deployment](./Resource_Deployment) for the steps you need to take to deploy this solution\n\n## Folders\n## [Resource Deployment](./Resource_Deployment/README.md)\nThe resources in this folder can be used to deploy the required resources into your Azure Subscription. You can do this in the Azure Portal\n\n## [Analytics Deployment](./Analytics_Deployment) \nThis folder contains the Notebooks needed to complete this solution accelerator. Once you have deployed all the required resources from [Resource Deployment](./Resource_Deployment/README.md), run through the Notebooks following the instructions in Resource Deployment.\n\n## [Backend Deployment](./Backend_Deployment) \nThis folder contains all API services consumed via Power Automate.  \nAs Architecture diagram shows all of services will be compiled and deployed in Azure Kuebernetes service with [deployment script](./Backend_Deployment/deployapplications.ps1)\n\n## [Frontend Deployment](./Frontend_Deployment) \nThis folder contains Dialog based Patient User Interface Applications(Power Virtual Agent) and related services.\n\n## DISCLAIMER\nBy accessing this code, you acknowledge that the code is not designed, intended, or made available: (1) as a medical device(s); (2) for the diagnosis of disease or other conditions, or in the cure, mitigation, treatment or prevention of a disease or other conditions; or (3) as a substitute for professional medical advice, diagnosis, treatment, or judgment. Do not use this code to replace, substitute, or provide professional medical advice, diagnosis, treatment, or judgement. You are solely responsible for ensuring the regulatory, legal, and/or contractual compliance of any use of the code, including obtaining any authorizations or consents, and any solution you choose to build that incorporates this code in whole or in part.\n\n## Microsoft Open Source Code of Conduct\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\n\nResources:\n\n- [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/)\n- [Microsoft Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/)\n- Contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with questions or concerns\n\n## License\nCopyright (c) Microsoft Corporation\n\nAll rights reserved.\n\nMIT License\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"\"Software\"\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED AS IS, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n"
    },
    {
        "repo": "/sayakpaul/Breast-Cancer-Detection-using-Deep-Learning",
        "language": "Jupyter Notebook",
        "readme_contents": "**Context**: \n\nInvasive Ductal Carcinoma (IDC) is the most common subtype of all breast cancers. To assign an aggressiveness grade to a whole mount sample, pathologists typically focus on the regions which contain the IDC. As a result, one of the common pre-processing steps for automatic aggressiveness grading is to delineate the exact regions of IDC inside of a whole mount slide.\n\n**About the dataset**:\n\nThe original dataset consisted of 162 whole mount slide images of Breast Cancer (BCa) specimens scanned at 40x. From that, 277,524 patches of size 50 x 50 were extracted (198,738 IDC negative and 78,786 IDC positive). Each patch\u2019s file name is of the format: u_xX_yY_classC.png \u2014 > example 10253_idx5_x1351_y1101_class0.png . Where u is the patient ID (10253_idx5), X is the x-coordinate of where this patch was cropped from, Y is the y-coordinate of where this patch was cropped from, and C indicates the class where 0 is non-IDC and 1 is IDC.\n\n**Inspiration**:\n\nBreast cancer is the most common form of cancer in women, and invasive ductal carcinoma (IDC) is the most common form of breast cancer. Accurately identifying and categorizing breast cancer subtypes is an important clinical task, and automated methods can be used to save time and reduce error.\n\n**Adrian Rosebrock** of **PyImageSearch** has [this wonderful tutorial](https://www.pyimagesearch.com/2019/02/18/breast-cancer-classification-with-keras-and-deep-learning/) on this same topic as well. Be sure to check that out if you have not. I decided to use the `fastai` library and to see how could I improve the predictive performance by incorporating modern deep learning practices. \n"
    },
    {
        "repo": "/ezgisubasi/breast-cancer-gene-expression",
        "language": "Jupyter Notebook",
        "readme_contents": "# breast-cancer-gene-expression\n\n<p align=\"center\"> \n   <img width=\"800\" alt=\"Ekran Resmi 2021-06-28 01 15 28\" src=\"https://user-images.githubusercontent.com/52889449/123560979-6b55d280-d7ae-11eb-8e97-82b997e5ab56.png\">\n</p>\n\nThis project aims to predict people who will survive breast cancer using machine learning models with the help of clinical data and gene expression profiles of the patients. Using machine learning models on gene data helps us better understand the disease and determine how well treatment methods work.\n\n# What is Breast Cancer?\n\nBreast cancer is a type of cancer that occurs when some cells in the breast grow excessively. This type of cancer causes the greatest number of cancer-related deaths among women. There are 5 known types of breast cancer based on the occurrence of the tumor as you can see in the figure. These genes can be also passed from parent to child. Therefore, breast cancer is also a genetically transmitted cancer type.\n\n<p align=\"center\">    \n  <img width=\"300\" alt=\"Ekran Resmi 2021-06-28 01 31 00\" src=\"https://user-images.githubusercontent.com/52889449/123561347-83c6ec80-d7b0-11eb-8653-3aa61952e1fe.png\">\n   <img width=\"450\" alt=\"Ekran Resmi 2021-06-28 01 27 32\" src=\"https://user-images.githubusercontent.com/52889449/123561291-131fd000-d7b0-11eb-8481-1a0e362d6c2c.png\">   \n</p>\n\n# Dataset\n\nThis dataset includes 31 clinical attributes, mRNA levels z-score for 331 genes, and mutation in 175 genes for 1904 breast cancer patients. \n\n# Importance of the Genes\n\n**BRCA1 and BRCA2:** The most common cause of hereditary breast cancer is an inherited mutation in the BRCA1 or BRCA2 gene. In normal cells, these genes help make proteins that repair damaged DNA. Mutated versions of these genes can lead to abnormal cell growth, which can lead to cancer.\n\n**Other Genes:** These gene mutations are much less common, and most of them do not increase the risk of breast cancer as much as the BRCA genes. Some of them are ATM, TP53, CHEK2, PTEN, CDH1, STK11 and PALB2.\n\n# Data Preprocessing\n \nIn the preprocessing part, first I cleaned the data and according to skew-distribution fill the missing values with the mode of them. After that, I applied Label Encoding and One-Hot Encoding to the categorical clinic data respectively.\n\n## Label Encoding:\n\nThe following categorical features are ordinal data encoded with the label encoding method.\n\n* cellularity\n* cancer_type_detailed\n* type_of_breast_surgery\n* her2_status_measured_by_snp6\n* pam50_+_claudin-low_subtype\n* her2_status_measured_by_snp6\n* tumor_other_histologic_subtype\n* integrative_cluster\n* 3-gene_classifier_subtype\n* death_from_cancer\n\n## One-Hot Encoding:\n\nThe following categorical features are nominal data encoded with the one-hot encoding method.\n\n* er_status_measured_by_ihc\n* er_status, her2_status\n* inferred_menopausal_state\n* primary_tumor_laterality\n* pr_status\n* oncotree_code\n\n# Data Visualization\n\nIn order to better understand the data and make some inferences, I analyzed using the following data visualization techniques.\n\n## Correlation Matrix\n\n<p align=\"center\"> \n<img width=\"700\" alt=\"Ekran Resmi 2021-06-27 19 12 24\" src=\"https://user-images.githubusercontent.com/52889449/123551809-f8cbff00-d77b-11eb-99fc-2fe436a8a038.png\">\n</p>\n\n## Treatment Types & Survivals\n\n<p align=\"center\"> \n  <img width=\"650\" alt=\"Ekran Resmi 2021-06-27 19 12 41\" src=\"https://user-images.githubusercontent.com/52889449/123551763-cde1ab00-d77b-11eb-94b8-87aaab431208.png\">  \n</p>\n\n## Survived Patients by Treatment Groups\n\n<p align=\"center\"> \n  <img width=\"450\" alt=\"Ekran Resmi 2021-06-27 19 12 58\" src=\"https://user-images.githubusercontent.com/52889449/123551766-d0440500-d77b-11eb-99b4-2442bd28ee51.png\">\n</p>\n\n\n# Choosing Best PCA\n\nPCA is an algorithm that reduces the dimension of datasets that have a high number of features. When dealing with the PCA, the scikit-learn library can be used to determine the number of components of the newly created data set. As you can see in the below plot, to get 95% of variance explained I need 375 principal components.\n\n<p align=\"center\"> \n <img width=\"580\" alt=\"Ekran Resmi 2021-06-27 19 13 14\" src=\"https://user-images.githubusercontent.com/52889449/123551772-d5a14f80-d77b-11eb-945d-ea587124ecd6.png\">\n</p>\n\n\n# Accuracy Scores\n\n\n   | Model                | Train Score  | Test Score   |\n   | :-------------:      | :----------: | :-----------:|\n   |  KNN                 | 0.75         | 0.73         |\n   |  Logistic Regression | 0.82         | 0.89         |\n   |  Random Forest       | 1.0          | 0.62         |\n   |  Decision Tree       | 0.69         | 0.62         |\n   |  Extra Trees         | 0.93         | 0.58         |\n   |  AdaBoost            | 1.0          | 0.65         |\n\n\n\n   <img width=\"430\" alt=\"Ekran Resmi 2021-06-28 00 54 17\" src=\"https://user-images.githubusercontent.com/52889449/123560540-7ce9ab00-d7ab-11eb-827a-092226e5500a.png\">\n\n\n# References:\n\n1) https://www.nature.com/articles/s41523-018-0056-8\n2) https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5461908/\n"
    },
    {
        "repo": "/Elhamkesh/Breast-Cancer-Scikitlearn",
        "language": "Jupyter Notebook",
        "readme_contents": "# Breast Cancer Scikit Learn\n\nSimple tutorial on Machine Learning with Scikit-Learn. \nThe goal is to get basic understanding of various techniques.\n\n## Description\n\nI use the \"Wisconsin Breast Cancer\" which is a default, preprocessed and cleaned datasets comes with scikit-learn. The target is to classify tumor as 'malignant' or 'benign' and code is written in Python using Jupyter notebook (CancerML.ipynb)\n\nTechniques:\n* KNN\n* Logistic Regression\n* Decision Tree\n* Random Forests\n* Neural Network\n* and SVM\n\n\n\n\n\n\n\n\n\n\n"
    },
    {
        "repo": "/Anki0909/BreakHist-Dataset-Image-Classification",
        "language": "Python",
        "readme_contents": "The tables below gives accuracy of each model for each magnification zoom presents in the dataset upto three decimal units. The values in brackets are F1 score\n\n**CNN models with FCN at the end**\n\n| Magnification/CNN Model -> | VGG-16 | VGG-19 | Xception | Resnet | Inception | Inception-Resnet-V3 |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| 40X | 0.802 (0.803) | 0.652 (0.685) | 0.831 (0.831) | 0.859 (0.858) | 0.853 (0.858) | 0.818 (0.813) |\n| 100X | 0.867 (0.877) | 0.709 (0.708) | 0.786 (0.794) | 0.911 (0.917) | 0.834 (0.827) | 0.845 (0.837) |\n| 200X | 0.841 (0.839) | 0.749 (0.756) | 0.812 (0.813) | 0.857 (0.853) | 0.799 (0.806) | 0.854 (0.859) |\n| 400X | 0.871 (0.869) | 0.799 (0.799) | 0.761 (0.758) | 0.903 (0.907) | 0.799 (0.796) | 0.842 (0.844) |\n\n\n**CV score on Logistic Regression Model trained on features extracted from CNN models**\n\n| Magnification/CNN Model -> | VGG-16 | VGG-19 | Xception | Resnet | Inception | Inception-Resnet-V3 |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| 40X | 0.685 (0.675) | 0.565 (0.547) | 0.858 (0.856) | 0.908 (0.906) | 0.839 (0.836) | 0.854 (0.850) |\n| 100X | 0.732 (0.725) | 0.633 (0.623) | 0.840 (0.837) | 0.902 (0.900) | 0.826 (0.822) | 0.863 (0.862) |\n| 200X | 0.864 (0.862) | 0.725 (0.718) | 0.940 (0.954) | 0.959 (0.958) | 0.919 (0.917) | 0.961 (0.960) |\n| 400X | 0.952 (0.952) | 0.876 (0.874) | 0.982 (0.982) | 0.983 (0.983) | 0.983 (0.983) | 0.982 (0.982) |\n\n**CV score on Linear Support Vector Machine Model trained on features extracted from CNN models**\n\n| Magnification/CNN Model -> | VGG-16 | VGG-19 | Xception | Resnet | Inception | Inception-Resnet-V3 |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| 40X | 0.644 (0.640) | 0.543 (0.530) | 0.857 (0.856) | 0.905 (0.905) | 0.855 (0.853) | 0.851 (0.849) |\n| 100X | 0.711 (0.704) | 0.603 (0.595) | 0.830 (0.829) | 0.895 (0.894) | 0.826 (0.822) | 0.864 (0.863) |\n| 200X | 0.848 (0.847) | 0.700 (0.693) | 0.943 (0.942) | 0.961 (0.961) | 0.916 (0.916) | 0.958 (0.958) |\n| 400X | 0.950 (0.949) | 0.868 (0.867) | 0.983 (0.983) | 0.983 (0.983) | 0.983 (0.983) | 0.980 (0.980) |\n"
    },
    {
        "repo": "/Piyush-Bhardwaj/Breast-cancer-diagnosis-using-Machine-Learning",
        "language": "Python",
        "readme_contents": "# Breast-cancer-diagnosis-using-Machine-Learning\nMachine learning is widely used in bio informatics and particularly in breast cancer diagnosis.\nIn this project, we have used certain classification methods such as K-nearest neighbors (K-NN) and Support Vector Machine (SVM) which is a supervised learning method to detect breast cancer. \nCancer diagnosis is one of the most studied problems in the medical domain.\nSeveral researchers have focused in order to improve performance and achieved to obtain satisfactory results.\nEarly detection of cancer is essential for a rapid response and better chances of cure. \nUnfortunately, early detection of cancer is often dif\ufb01cult because the symptoms of the disease at the beginning are absent.\nThus, it is necessary to discover and interpret new knowledge to prevent and minimize the risk adverse consequences.\n\n\nTo understand this problem more precisely, tools are needed to help oncologists to choose\nthe treatment required for healing or prevention of recurrence by reducing the harmful effects of certain treatments and their costs. \nIn arti\ufb01cial intelligent, machine learning is a discipline which allows the machine to evolve through a process.\nWisconsin Diagnostic Breast Cancer (WDBC) dataset obtained by the university of Wisconsin Hospital is used to classify tumors as benign or malignant.\n\n#k- Nearest Neighbour (k-NN) classification technique:\n\n\nk-NN is a non- parametric method used for classification. In this classification, the output is a class membership.\nAn object is classified by a majority vote of its neighbors,\nwith the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). \nIf k = 1, then the object is simply assigned to the class of that single nearest neighbor. It is the simplest algorithm among all the machine learning algorithms.\n\n#Support Vector Machine (SVM) classification Technique:\n\n\nSupport Vector Machine (SVM) is a supervised machine learning algorithm which can be used for both  classification or regression challenges. However, it is mostly used in classification problems. In this algorithm, we plot each data item as a point in n-dimensional space (where n is number of features you have) with the value of each feature being the value of a particular coordinate. Then, we perform classification by finding the hyper-plane that differentiate the two classes very well\n\n#Outcome\n\n\n1.\tIt can be seen that as the training data size increases, SVM performs better than kNN and has more accuracy.\n\n2.\tkNN is quite a good classifier but its performance depends on the value of k. It gives poor results for lower values of k and best results as the value of k increases.\n\n3.\tPCA is more sensitive to SVM than kNN .As the value of Principle Component (PC) is increased, SVM gives better results and accuracy score is more than kNN.\n\n4.\tWhen the value of PC=1 and K=9, we get the highest accuracy score (97.95).\n\n"
    },
    {
        "repo": "/nyukat/GLAM",
        "language": "Jupyter Notebook",
        "readme_contents": "# Weakly-supervised High-resolution Segmentation of Mammography Images for Breast Cancer Diagnosis\n\n## Introduction\nThis is an implementation of the GLAM (Global-Local Activation Maps) model as described in [our paper](https://openreview.net/pdf?id=nBT8eNF7aXr).  In this work, we introduce a novel neural network architecture to perform weakly-supervised segmentation of high-resolution images. The proposed model selects regions of interest via coarse-level localization, and then performs fine-grained segmentation of\nthose regions.\nWe apply this model to breast cancer diagnosis with screening mammography, and validate it on a large clinically-realistic dataset. Measured by Dice similarity score, our approach outperforms existing methods by a large margin in terms of localization performance of benign and malignant lesions, relatively improving the performance by 39.6\\% and 20.0\\%, respectively. \nThe inference pipeline of the model is shown below.\n\n<p align=\"center\">\n  <img width=\"793\" src=\"figures/Inference_framework.png\">\n</p>\n\nHighlights of GLAM:\n\n- **Weakly-supervised High-resolution Segmentation**: Despite being trained with only image-level labels, GLAM is able to generate fine-grained pixel-level saliency maps (shown below) that provide interpretability for high-resolution images.\n\n<p align=\"center\">\n  <img width=\"793\" src=\"figures/Visualization_example.png\">\n</p>\nWe provide this implementation that allows users to use one of our pretrained models and to obtain predictions and saliency maps for breast cancer detection. The model is implemented in PyTorch. \n\n* Input: A mammography image that is cropped to 2944 x 1920 and is saved as 16-bit png file. Within this repository, we provide four sample exams (images are stored in `sample_data/images` directory and their corresponding exam list is stored in `sample_data/exam_list_before_cropping.pkl`). Each exam includes four images, corresponding to the  following views: L-CC, R-CC, L-MLO and R-MLO. These exams contain original mammography images and therefore need to be preprocessed (see the Preprocessing section). \n\n* Output: The GLAM model generates a set of predictions for each image, consisting of values indicating model's confidence in benignity and malignancy of the findings. Predictions are saved into a csv file `$OUTPUT_PATH/predictions.csv` that contains the following fields: image_index, benign_pred, malignant_pred, benign_label, malignant_label. In addition, each input image is associated with a visualization file saved under `$OUTPUT_PATH/visualization`. The images (from left to right) represent:\n  * input mammography with ground truth annotation (green=benign, red=malignant) if there is a segmentation mask in the input,\n  * patch map that illustrates the locations of selected patches (blue squares),\n  * saliency map for benign class,\n  * saliency map for malignant class,\n  * selected patches.\n\nThe code in this repository is partially based on the implementation of [GMIC](https://github.com/nyukat/GMIC).\n\n## Prerequisites\n\n* Python (3.6)\n* PyTorch (1.1.0)\n* torchvision (0.2.2)\n* NumPy (1.14.3)\n* SciPy (1.0.0)\n* H5py (2.7.1)\n* imageio (2.4.1)\n* pandas (0.22.0)\n* opencv-python (3.4.2)\n* tqdm (4.19.8)\n* matplotlib (3.0.2)\n\n## License\n\nThis repository is licensed under the terms of the GNU AGPLv3 license.\n\n## How to run the code\n\nOne way to set up the environment is to use conda. With conda installed on your system, please run the following line to setup the environment: \n(Note that you need to first cd to the project directory)\n\n`conda env create -f environment.yml` \n\nOnce you have installed and activated all the dependencies, you can execute the following command to automatically run the entire pipeline:\n\n    bash ./run.sh <INPUT_FOLDER> <OUTPUT_FOLDER> <MODEL_NAME> <DEVICE_TYPE> <GPU_NUMBER>\n  \nwhere the arguments represent:\n* `INPUT_FOLDER` - Folder with all input files.\n* `OUTPUT_FOLDER` - Folder, where visualization files and predictions will be saved.\n* `MODEL_NAME` - Name of the model to use. Valid values include {`model_joint`, `model_sep`}, the former one is the joint-trained model, the latter one is the model that trained without joint training.\n* `DEVICE_TYPE` - Either `gpu` or `cpu`.\n* `GPU_NUMBER` - Which gpu to use, e.g. `0`, does not matter if `DEVICE_TYPE==cpu`, default: `0`.\n   \nAdditionally, there are several parameters in `run.sh` that, most likely, will not need to be changed:\n* `MODEL_PATH`: The path where the model weights are saved.\n* `IMAGES_FOLDER`: The directory where images are saved, based on the `INPUT_FOLDER`.\n* `INITIAL_EXAM_LIST_PATH`: Path to initial exam list pkl file.\n* `CROPPED_IMAGE_FOLDER`: The directory where cropped mammograms are saved.\n* `CROPPED_EXAM_LIST_PATH`: Path to exam list pkl file created by cropping.\n* `SEG_FOLDER`: The directory where ground truth segmenations are saved.\n* `FINAL_EXAM_LIST_PATH`: Path to final exam list pkl file created after choosing best centers.\n* `NUM_PROCESSES`: Number of processes used for multiprocessing of inputs. Default: `10`.\n* `RUN_TIME`: This function calculates current time, which will be used to define `OUTPUT_FINAL_FOLDER`.\n* `OUTPUT_FINAL_FOLDER`: Subfolder of `OUTPUT_FOLDER`, named according to the current `RUN_TIME`.\n\nThe default command runs `model_sep` model on `cpu` with the sample data included in this repository. It can be run by either using the default values or by providing them explicitly:\n\n    bash ./run.sh\n    \nor\n    \n    bash ./run.sh sample_data sample_output model_sep cpu\n\nAfter running one of the above commands, you should obtain the outputs for the sample exams provided in the repository. They can be found in \n`sample_output/<current_time>/predictions.csv` by default). Note that the output scores are not calibrated. \n\nimage_index  |  benign_pred  |  malignant_pred  |  benign_label  |  malignant_label\n-------------|---------------|------------------|----------------|-----------------\n0_L-CC       |  0.3141       |  0.0111          |  0             |  0\n0_R-CC       |  0.4512       |  0.1458          |  1             |  0\n0_L-MLO      |  0.3604       |  0.0723          |  0             |  0\n0_R-MLO      |  0.4853       |  0.1042          |  1             |  0\n1_L-CC       |  0.1149       |  0.0202          |  0             |  0\n1_R-CC       |  0.3709       |  0.3190          |  0             |  1\n1_L-MLO      |  0.1107       |  0.0233          |  0             |  0\n1_R-MLO      |  0.3751       |  0.3492          |  0             |  1\n2_L-CC       |  0.1720       |  0.0564          |  0             |  0\n2_R-CC       |  0.3367       |  0.0516          |  1             |  0\n2_L-MLO      |  0.1275       |  0.0196          |  0             |  0\n2_R-MLO      |  0.3789       |  0.0724          |  1             |  0\n3_L-CC       |  0.4849       |  0.3341          |  0             |  1\n3_R-CC       |  0.3396       |  0.1092          |  0             |  0\n3_L-MLO      |  0.4716       |  0.2484          |  0             |  1\n3_R-MLO      |  0.2068       |  0.0445          |  0             |  0\n\nYou should also find the visualization of the saliency maps under `sample_output/<current_time>/visualization/`. \n\n\nTIP: If you'd like to run individual Python scripts, please include the path to this repository in your `PYTHONPATH`:  \n \n    export PYTHONPATH=$(pwd):$PYTHONPATH\n    \n## Data\n\n`sample_data/images` contains 4 exams each of which includes 4 original mammography images (L-CC, L-MLO, R-CC, R-MLO). All mammography images are saved in png format. The original 12-bit mammograms are saved as rescaled 16-bit images to preserve the granularity of the pixel intensities, while still being correctly displayed in image viewers.\n\n`sample_data/segmentation` contains the binary pixel-level segmentation labels for some exams. All segmentations are saved as png images.\n\n`sample_data/exam_list_before_cropping.pkl` contains a list of exam information. Each exam is represented as a dictionary with the following format:\n\n```python\n{'horizontal_flip': 'NO',\n  'L-CC': ['0_L-CC'],\n  'L-MLO': ['0_L-MLO'],\n  'R-MLO': ['0_R-MLO'],\n  'R-CC': ['0_R-CC'],\n  'cancer_label': {\n    'benign': 1,\n    'right_benign': 1, \n    'malignant': 0, \n    'left_benign': 0, \n    'unknown': 0, \n    'right_malignant': 0,\n    'left_malignant': 0},\n  'L-CC_benign_seg': ['0_L-CC_benign'],\n  'L-CC_malignant_seg': ['0_L-CC_malignant'],\n  'L-MLO_benign_seg': ['0_L-MLO_benign'],\n  'L-MLO_malignant_seg': ['0_L-MLO_malignant'],\n  'R-MLO_benign_seg': ['0_R-MLO_benign'],\n  'R-MLO_malignant_seg': ['0_R-MLO_malignant'],\n  'R-CC_benign_seg': ['0_R-CC_benign'],\n  'R-CC_malignant_seg': ['0_R-CC_malignant']}\n```\nIn their original formats, images from `L-CC` and `L-MLO` views face right, and images from `R-CC` and `R-MLO` views face left. We horizontally flipped `R-CC` and `R-MLO` images so that all four views face right. Values for `L-CC`, `R-CC`, `L-MLO`, and `R-MLO` are list of image filenames without extensions and directory name. Segmentation information is optional and, if provided, will be used in visualization.\n\n### Preprocessing\n\nHere, we will describe the preprocessing pipeline used in `run.sh`. The following commands crop mammograms and calculate information about augmentation windows.\n\n#### Cropping mammograms\n```bash\npython3 src/cropping/crop_mammogram.py \\\n    --input-data-folder $IMAGES_FOLDER \\\n    --output-data-folder $CROPPED_IMAGE_FOLDER \\\n    --exam-list-path $INITIAL_EXAM_LIST_PATH  \\\n    --cropped-exam-list-path $CROPPED_EXAM_LIST_PATH  \\\n    --num-processes $NUM_PROCESSES\n```\n`src/cropping/crop_mammogram.py` crops the mammogram around the breast and discards the background in order to improve image loading time and time to run segmentation algorithm and saves each cropped image to `$CROPPED_IMAGE_FOLDER/<image_id>.png` using h5py. It also adds additional information for each image and creates a new image list to `$CROPPED_IMAGE_LIST_PATH` while discarding images which it fails to crop. \n\nCropping adds below information to the exams in the pkl file:\n\n```python\n{'window_location':\n {'L-CC': [(0, 2023, 0, 801)],\n 'R-CC': [(0, 2144, 0, 801)],\n 'L-MLO': [(0, 2730, 0, 981)],\n 'R-MLO': [(0, 2650, 0, 1006)]},\n'rightmost_points': {'L-CC': [((948, 957), 750)], \n 'R-CC': [((1232, 1249), 751)],\n 'L-MLO': [((1733, 1749), 930)], \n 'R-MLO': [((1571, 1648), 956)]}, \n'bottommost_points': {'L-CC': [(1972, (100, 100))], \n 'R-CC': [(2093, (101, 101))], \n 'L-MLO': [(2679, (100, 101))],\n 'R-MLO': [(2599, (101, 102))]}, \n'distance_from_starting_side': {'L-CC': [0], \n 'R-CC': [0],\n 'L-MLO': [0], \n 'R-MLO': [0]}\n}\n```\n  \n  <!--- Optional --verbose argument prints out information about each image. The additional information includes the following:\n- `window_location`: location of cropping window w.r.t. original dicom image so that segmentation map can be cropped in the same way for training.\n- `rightmost_points`: rightmost nonzero pixels after correctly being flipped.\n- `bottommost_points`: bottommost nonzero pixels after correctly being flipped.\n- `distance_from_starting_side`: records if zero-value gap between the edge of the image and the breast is found in the side where the breast starts to appear and thus should have been no gap. Depending on the dataset, this value can be used to determine wrong value of `horizontal_flip`.-->\n\n\n#### Calculating optimal centers\n```bash\npython3 src/optimal_centers/get_optimal_centers.py \\\n    --cropped-exam-list-path $CROPPED_EXAM_LIST_PATH \\\n    --data-prefix $CROPPED_IMAGE_FOLDER \\\n    --output-exam-list-path $FINAL_EXAM_LIST_PATH \\\n    --num-processes $NUM_PROCESSES\n```\n`src/optimal_centers/get_optimal_centers.py` outputs new exam list with additional metadata to `$FINAL_EXAM_LIST_PATH`. The additional information is `best_center` - optimal center point of the window for each image. The augmentation windows drawn with `best_center` as exact center point could go outside the boundary of the image. This usually happens when the cropped image is smaller than the window size. In this case, we pad the image and shift the window to be inside the padded image in augmentation. Refer to [the data report](https://cs.nyu.edu/~kgeras/reports/datav1.0.pdf) for more details. Example of `best_center` info:\n\n```python\n{'best_center':\n {'L-CC': [(1011, -211)],\n 'R-CC': [(1072, -210)],\n 'L-MLO': [(1206, -31)],\n 'R-MLO': [(1126, -5)]}\n}\n```\n\n#### Outcomes of preprocessing\nAfter the preprocessing step, you should have the following files in the `$INPUT_FOLDER` directory:\n- cropped_images: a folder that contains the cropped images corresponding to all images in the `$INPUT_FOLDER/images`.\n- data.pkl: the final pickle file of a data list that includes the preprocessing metadata for each image and exam.\n\n### Inference with classifier\nInference is run by the following command:\n```bash\npython3 src/scripts/run_model.py \\\n    --model-path \"${MODEL_PATH}\" \\\n    --data-path \"${FINAL_EXAM_LIST_PATH}\" \\\n    --image-path \"${CROPPED_IMAGE_FOLDER}\" \\\n    --segmentation-path \"${SEG_FOLDER}\" \\\n    --output-path \"${OUTPUT_FINAL_FOLDER}\" \\\n    --device-type \"${DEVICE_TYPE}\" \\\n    --gpu-number \"${GPU_NUMBER}\" \\\n    --model-index \"${MODEL_NAME}\" \\\n    --visualization-flag\n```\n\nIt creates predictions of GLAM in `$OUTPUT_FOLDER/<current_time>` folder. In addition, it generates visualization of outputs.\n\n### Adding your own dataset\n\nTo evaluate GLAM on your own dataset, you need to provide a folder consisting of the following data:\n- A folder named `images/` containing all images.\n- (optional) A folder named `segmentation/` containing segmentations of chosen images.\n- A pkl file `exam_list_before_cropping.pkl` containing information about images, as in the Data section.\n\n\n## Reference\n\nIf you found this code useful, please cite our paper:\n\n**Weakly-supervised High-resolution Segmentation of Mammography Images for Breast Cancer Diagnosis**\\\nKangning Liu, Yiqiu Shen, Nan Wu, Jakub Chledowski, Carlos Fernandez-Granda, and Krzysztof J. Geras.  \nMedical Imaging with Deep Learning\n(2021).\n    \n    @inproceedings{liu2021weakly,\n      title={Weakly-supervised High-resolution Segmentation of Mammography Images for Breast Cancer Diagnosis},\n      author={Liu, Kangning and Shen, Yiqiu and Wu, Nan and Chledowski, Jakub and Fernandez-Granda, Carlos and Geras, Krzysztof J},\n      booktitle={Medical Imaging with Deep Learning},\n      year={2021}\n    }\n"
    },
    {
        "repo": "/raviolli77/machineLearning_breastCancer_Python",
        "language": "HTML",
        "readme_contents": "# Machine Learning Techniques on Breast Cancer Wisconsin Data Set\n\n**Contributor**:\n+ Raul Eulogio\n\nI created this repo as a way to get better acquainted with **Python** as a language and as a tool for data analysis. But it eventually became in exercise in utilizing various programming languages for machine learning applications.\n\nI employed four **Machine Learning** techniques:\n+ **Kth Nearest Neighbor**\n+ **Random Forest**\n+ **Neural Networks**:  \n\nIf you would like to see a walk through of the analysis on [inertia7](https://www.inertia7.com/projects/3) includes running code as well as explanations for exploratory analysis. This [project](https://www.inertia7.com/projects/95) contains an overview of *random forest*, explanations for other algorithms in the works.\n\nThe repository includes the *scripts* folder which contains scripts for these programming languages (in order of most detailed):\n+ *Python*\n+ *R*\n+ *PySpark*\n\nThis repo is primarily concerned with the *python* iteration.\n\nThe multiple *python* script is broken into 5 sections (done by creating a script for each section) in the following order:\n+ **Exploratory Analysis**\n+ **Kth Nearest Neighbors**\n+ **Random Forest**\n+ **Neural Networks**\n+ **Comparing Models**\n\n**NOTE**: The files `data_extraction.py`, `helper_functions.py`, and `produce_model_metrics.py` are used to abstract functions to make code easier to read. These files do a lot of the work so if you are interested in how the scripts work definitely check them out.\n\n## Running .py Script\nA `virtualenv` is needed where you will download the necessary packages from the `requirements.txt` using:\n\n\tpip3 install -r requirements.txt\n\nOnce this is done you can run the scripts using the usual terminal command:\n\n\t$ python3 exploratory_analysis.py\n\n**NOTE**: You can also run it by making script executable as such:\n\n\t$ chmod +x exploratory_analysis.py\n\n\n**Remember**: You must have a *shebang* for this to run i.e. this must be at the very beginning of your script:\n\n\t#!/usr/bin/env python3\n\nthen you would simply just run it (I'll use **Random Forest** as an example)\n\n\t$ ./random_forest.py\n\n## Conclusions\nOnce I employed all these methods, we were able to utilize various machine learning metrics. Each model provided valuable insight. *Kth Nearest Neighbor* helped create a baseline model to compare the more complex models. *Random forest* helps us see what variables were important in the bootstrapped decision trees. And *Neural Networks* provided minimal false negatives which results in false negatives. In this context it can mean death.  \n\n### Diagnostics for Data Set\n\n\n| Model/Algorithm      | Test Error Rate | False Negative for Test Set | Area under the Curve for ROC | Cross Validation Score        | Hyperparameter Optimization |\n|----------------------|-----------------|-----------------------------|------------------------------|-------------------------------|-----------------------|\n| Kth Nearest Neighbor | 0.07  | 5 | 0.980 | Accuracy:  0.925 (+/-  0.025) | Optimal *K* is 3 |\n| Random Forest        | 0.035 | 3 | 0.996 | Accuracy:  0.963 (+/-  0.013) | {'max_features': 'log2', 'max_depth': 3, 'bootstrap': True, 'criterion': 'gini'}\t|\n| Neural Networks      | 0.035 | 1 | 0.982 | Accuracy:  0.967 (+/-  0.011) | {'hidden_layer_sizes': 12, 'activation': 'tanh', 'learning_rate_init': 0.05} |\n\n\n\n#### ROC Curves for Data Set\n<img src=\"https://raw.githubusercontent.com/raviolli77/machineLearning_breastCancer_Python/master/reports/images/roc_all.png\" style=\"width: 100px;\"/>\n\n#### ROC Curves zoomed in\n<img src=\"https://raw.githubusercontent.com/raviolli77/machineLearning_breastCancer_Python/master/reports/images/roc_all_zoom.png\" style=\"width: 100px;\"/>\n\nThe ROC Curves are more telling of **Random Forest** being a better model for predicting.\n\nAny feedback is welcomed!\n\nThings to do:\n+ Create **Jupyter Notebook** for *KNN* and *NN* (1/25/2018)\n+ Unit test scripts \n"
    },
    {
        "repo": "/almaan/her2st",
        "language": "Jupyter Notebook",
        "readme_contents": "# Spatial Deconvolution of HER2 positive Breast Tumors Reveals Novel Intercellular Relationships\n\n<div align=\"center\"><span style=\"font-size:10px;font-style:italic\">Alma Andersson, Ludvig Larsson, Linnea Stenbeck, Fredrik Salm\u00e9n, Anna Ehinger, Sunny Wu, Ghamdan Al-Eryani, Daniel L. Roden, Alex Swarbrick, \u00c5ke Borg, Jonas Fris\u00e9n, Camilla Engblom, Joakim Lundeberg</div></span>\n\n## Description\n\n\nAll data, results and code related to the [paper](biorxiv) can be found herewithin. For results that represent as large sets of images or tables, we provide scripts that are self-contained within this repository (i.e., all you need to do is to run them to reproduce our images) that allow you to reproduce these.\n\nWe have compiled a `shiny` app that allows you to explore the data and results interactively from your own browser, to see how you deploy and orient this tool go to the section \"[Shiny App](#shiny-app)\" below.\n\n## Data access\nA lot of people have inquired about access to the data used in this manuscript,\nall data is accessible at\n[this](https://zenodo.org/record/3957257#.Y4LB-rLMIfg) Zenodo repository. **However**, the\ndata is _encrypted_ using [7z](https://7-zip.org/7z.html). To decrypt these files, use\nthe following passwords:\n\n* count matrices and images: `zNLXkYk3Q9znUseS`\n* meta data and spot selection: `yUx44SzG6NdB32gY`\n\nFor further questions, please see contact details  below.\n\n## Structure\n* `data/`\n    * `ST-cnts/` : contains data for the 36 breast cancer sections used in this study\n    * `ST-imgs/` : contains the associated HE-images for the 36 sections used in this study\n    * `ST-spotfiles/` : contains tables with selected spots under tissue used to subset the raw gene count matrices\n    * `go-sets/` : GO gene sets. Named go-{GO-ID}.tsv\n    * `public.yaml` : yaml file with links to the publicly available data sets that we've used\n* `res/`\n    * `ST-pat/`\n        * `img/` : images showing the pathologist's annotations\n        * `lbl/` : meta files (tsv) where each spot is labeled according to the pathologist's annotations\n    * `ST-cluster/`\n        * `lbl/` : meta files (tsv) where each spot is labeled by membership of the expression based clusters\n        * `markers/` : marker genes for each cluster\n        * `fea/` : functional enrichment analysis results for each cluster\n        * `pat-enr/` : enrichment of clusters within the pathologist's regions\n        * `core-sig/` : core signature for tumor and immune populations\n        * `motivation.xlsx` : motivations regarding how the clusters were annotated\n    * `ST-deconv/` \n        * `props/{major,minor,subset}.zip` : spot-wise proportion estimates for respective tier, obtained using stereoscope \n        * `pat-enr/{major,minor,subset}.zip` : enrichment of cell types within the pathologist's regions\n        * `cluster-enr/{major,minor,subset}.zip` : enrichment of cell types within the expression based clusters \n        * `corrs/{major,minor,subset}.zip` : correlation plots for all cell types within respective tier\n    * `ST-SW-enr/`\n        * `raw/` : spot-wise pathway enrichment; values are given for within spot each section \n        * `viz/` : visualization of the raw spot-wise pathway enrichment\n    * `TLS-pred/`\n        * `coef-full.tsv` : coefficient values (including intercept) for all genes in the model to predict TLS-score\n        * `tls-signature.tsv` : the tls-signature presented in the paper\n        * `tls-fea.tsv` : functional enrichment results for the tls-signature\n* `scripts/`\n    * `Seurat_analysis_HER2_BC_samples.Rmd`: markdown file outlining the gene-expression based clustering and associated analysis.\n    * `* corrplot.R` : script to use correlation plot. Uses bootstrapping to estimate confidence. (Figure 3 and 4)\n    *  `* enriched-region.py` : estimates enrichment of types in defined regions by a permuation test (Figure 3 and 4). Produces palette-plots.\n    * `* spw-enr.py` : calculates spot-wise enrichment of gene set using Fisher's exact test (Figure 4)\n    * `* rank-plot.py` : generates a rank-plot where genes to highlight can be provided by a text file or as an argument.\n\n    * `* viz-tls-score.py` : visualized, B-cell and T-cell proportions together with calculated TLS-score\n    * `* tls.py` :  fits the TLS-model. Takes as input a set of count matices and associated proportion values for these. \n    * `* fea.py` :  conduct functional enrichment analysis using g:Profiler for a set of provided genes. Generates tsv file with all pathways and an image. (Figure 5)\n    * `* predict.py` : predict TLS-score for a ST/Visium section. Takes count data for the section to apply the model to and a coefficient file (generated by tls.py)\n    * `* proportion-highlight.py`: plots proportion estimates with specific regions highlighted. Uses a design-file to customize the character of the plot (see `figure3-config.yaml` below). Used to generate Figure subplots 3A and 3B.\n    * `mac-t_cell-interaction.ipynb` : notebook outlining the analysis of the interferon pathways in the presence of certain macrophage and t-cell subsets. Used to generate Supplementary Figures 15-17.\n* `rsc/`\n    * `figure3-config.yaml`: configuration file to `scripts/proportion-highlight.py` used to generate parts of Figure 3A and B.\n* `app/`\n    * `data/` : pre-processed count matrices, cell type proportion tables and images used for shiny application\n\n`*` := with CLI (command line interface). Do ```./script.py -h``` to see the different options that can be used.\n\n<hr>\n\n## Shiny App \n\nWhile all the data and results are available as raw files, we have also constructed a tool that allows you to **interactively explore** these. The tool will open up in your default browser, but it runs locally (i.e. you host all the files on your own computer). See below for instructions regarding how to setup and orient the tool.\n\n\n### Setup\n\nBegin by cloning this repository; to do so open a terminal, then enter a directory where you'd like to clone the repository into (here `MY_DIR`) and then do:\n\n```sh\ncd MY_DIR\ngit clone https://github.com/almaan/her2st.git .\n```\n\nNext, we'll make sure that all the necessary packages (e.g., `shiny`) are installed. We've prepared an installation script for you - this will not overwrite you current package versions - that should take care of everything. However, if you're missing some dependencies (C++ backends) you might have to do a manual install. To check for and install missing packages; from the folder that you cloned this repo into, do:\n\n```sh\ncd her2st/app\n./install-packages.R\n\n```\n\n_NOTE_ : Make sure `install-packages.R` have the proper permissions (e.g., by doing `chmod +x install-packages.R` on a Linux computer) before running it.\n\nIf everything went as expected you should see a message like:\n\n```sh\n\n----\nSuccessful installs:\n----\nggplot2\njpeg\nMatrix\ngrid\nmagrittr\nmagick\nzeallot\nshiny\nshinyWidgets\nshinycssloaders\nshinydashboard\nRColorBrewer\nviridis\ndplyr\nextrafont\n\n----\nFailed installs:\n----\nNONE\n\n```\n\nIn case you have any failed installs, try to install these manually and troubleshoot what dependencies you might be missing. \n\nOnce all the packages are installed - we are ready to run the app!\n\n### Usage \n\nTo launch the app, do:\n\n```sh\ncd MY_DIR/her2st/app\n./launch.R\n```\n_NOTE_ : Just as for `install-packages.R` make sure `launch.R` have the proper permissions before running it.\n\nThis should open a new tab or browser-window, with an address like `127.0.0.1:XXXX` looking like:\n\n<img src=imgs/shiny-guide.png alt=\"shiny app screenshot\"><br>\n<br>\n\nAs you can see you have some different options to choose from (indicated with dashed red boxes and a number). To elaborate some regarding these:\n\n1. **Select section** - click on the patient you want to visualize a section from, a drop-down menu will appear listing the different sections you can choose from.\n2. **Gene** - Select a gene of interest (GOI) to visualize the spatial expression of, overlaid on the tissue. You can either type the name your GOI or click the arrow to prompt a drop-down menu of genes to select from.\n3. **Cell type** -  Select a tier and cell type (from a drop-down menu) to visualize the spatial distribution of this type (the proportion estimates) overlaid on the tissue.\n4. **Opacity** - Allows you to set the transparency of the markers\n5. **Colors** - Allows you to set the colormap used to visualize the values (default is Green)\n6. **Edgecolor** - Toggle whether edgecolor should be on or off\n\n_NOTE_: Only one of the features in 2 and 3 can be visualized at a time, the last one selected will be displayed.\n\n## Contact\n\nFor questions related to the material presented at this site we recommend you to contact either:\n\n* Alma Andersson : alma.andersson [at] differentiable [dot] net\n* Ludvig Larsson : ludvig.larsson [at] scilifelab [dot] se\n\nIf you have code-specific questions, e.g., regarding the `shiny` app, opening an issue is the best way to communicate this if a quick response is desired.\n"
    },
    {
        "repo": "/akshaybahadur21/Breast-Cancer-Deep-Learning",
        "language": "Python",
        "readme_contents": "# Breast Cancer Classifier (Deep Learning) \ud83d\udd2c\n\n[![](https://img.shields.io/github/license/sourcerer-io/hall-of-fame.svg?colorB=ff0000)](https://github.com/akshaybahadur21/Breast-Cancer-Deep-Learning/blob/master/LICENSE.txt)  [![](https://img.shields.io/badge/Akshay-Bahadur-brightgreen.svg?colorB=ff0000)](https://akshaybahadur.com)\n\nThis code helps you classify malignant and benign tumors using Deep Learning.\n\n## Code Requirements \ud83e\udd84\nYou can install Conda for python which resolves all the dependencies for machine learning.\n\n## Description \ud83e\uddea\nDeep learning (also known as deep structured learning or hierarchical learning) is part of a broader family of machine learning methods based on learning data representations, as opposed to task-specific algorithms. Learning can be supervised, semi-supervised or unsupervised.\n\nDeep learning models are loosely related to information processing and communication patterns in a biological nervous system, such as neural coding that attempts to define a relationship between various stimuli and associated neuronal responses in the brain.\n\n## Python  Implementation \ud83d\udc68\u200d\ud83d\udd2c\n\n1) Dataset- UCI-ML\n2) I have used 30 features to classify\n3) Instead of 0=benign and 1=malignant, I have used 1=benign and 2=malignant\n\n## Results \ud83d\udcca\n\n<img src=\"https://github.com/akshaybahadur21/Breast-Cancer-Deep-Learning/blob/master/b_cancer_dl.gif\">\n\n## Execution \ud83d\udc09\nTo run the code, type `python B_Cancer_DL.py`\n\n```\npython B_Cancer_DL.py\n```\n\n\n\n"
    },
    {
        "repo": "/st186/Detection-of-Breast-Cancer-using-Neural-Networks",
        "language": "Matlab",
        "readme_contents": "# Detecting Breast Cancer using Neural Nets\n\n## What is the Project all about?\nIn India and over the world, Cancer has become a deadly disease and more and more people are suffering from Cancer and a survey says one in every 30 women suffer from this disease in their lifetime and so basically the project was first thought of because of the increase in cases of breast cancer and one thing which is very important that if we can detect the Cancer at an early stage then there is an increased chances of it getting cured.So this project lays a foundation in making the detection of the cancer automated so that more and more people can get it diagonised early so as get cured.\n\n## How it is implemented?\n\nThe signs of detection are Masses and micro calcification clusters which are important in early detection of breast cancer.\n\nMicro calcification are nothing but tiny mineral deposits within the breast tissue. They look similar to small white colored spots. They may or may not be caused by cancer.\n\nMasses can be many things, including cysts (fluid-filled sacs) and non-cancerous solid tumors, but they could also be cancerous.\n\nThe difficulty in cancer detection is that the abnormalities from normal breast tissues are hard to read because of their subtle appearance and ambiguous margins.Automated tools which can help radiologist in early detection of breast cancer.\n\nFurther we have classified the cancer into three categories after its detection- Normal,Malignant,Benign.\n\n## Methodology\n\nWe\u200b \u200bhave\u200b \u200bused\u200b \u200badaptive\u200b \u200bmean\u200b \u200bfilter\u200b \u200bto\u200b \u200bremove\u200b \u200bnoise\u200b \u200bfrom\u200b \u200bimage.\u200b \u200bsince\u200b \u200bit\u200b \u200bis\u200b \u200bbetter among\u200b \u200ball\u200b \u200bthe\u200b \u200bspatial\u200b \u200bfilters\u200b \u200band\u200b \u200bdistinguish\u200b \u200bfine\u200b \u200bdetails\u200b \u200bfrom\u200b \u200bnoise.\u200b\n\u200bThe\u200b \u200bAdaptive Median\u200b \u200bFilter\u200b \u200bperforms\u200b \u200bspatial\u200b \u200bprocessing\u200b \u200bto\u200b \u200bdetermine\u200b \u200bwhich\u200b \u200bpixels\u200b \u200bin\u200b \u200ban\u200b \u200bimage have\u200b \u200bbeen\u200b \u200baffected\u200b \u200bby\u200b \u200bimpulse\u200b \u200bnoise.\u200b \u200bThe\u200b \u200bAdaptive\u200b \u200bMedian\u200b \u200bFilter\u200b \u200bclassifies\u200b \u200bpixels as\u200b \u200bnoise\u200b \u200bby\u200b \u200bcomparing\u200b \u200beach\u200b \u200bpixel\u200b \u200bin\u200b \u200bthe\u200b \u200bimage\u200b \u200bto\u200b \u200bits\u200b \u200bsurrounding\u200b \u200bneighbor\u200b \u200bpixels. \n\nThe\u200b \u200bsize\u200b \u200bof\u200b \u200bthe\u200b \u200bneighborhood\u200b \u200bis\u200b \u200badjustable,\u200b \u200bas\u200b \u200bwell\u200b \u200bas\u200b \u200bthe\u200b \u200bthreshold\u200b \u200bfor\u200b \u200bthe comparison.\u200b \u200bA\u200b \u200bpixel\u200b \u200bthat\u200b \u200bis\u200b \u200bdifferent\u200b \u200bfrom\u200b \u200ba\u200b \u200bmajority\u200b \u200bof\u200b \u200bits\u200b \u200bneighbors,\u200b \u200bas\u200b \u200bwell\u200b \u200bas being\u200b \u200bnot\u200b \u200bstructurally\u200b \u200baligned\u200b \u200bwith\u200b \u200bthose\u200b \u200bpixels\u200b \u200bto\u200b \u200bwhich\u200b \u200bit\u200b \u200bis\u200b \u200bsimilar,\u200b \u200bis\u200b \u200blabeled\u200b \u200bas impulse\u200b \u200bnoise.\n\n\u200bThese\u200b \u200bnoise\u200b \u200bpixels\u200b \u200bare\u200b \u200bthen\u200b \u200breplaced\u200b \u200bby\u200b \u200bthe\u200b \u200bmedian\u200b \u200bpixel\u200b \u200bvalue\u200b \u200bof the\u200b \u200bpixels\u200b \u200bin\u200b \u200bthe\u200b \u200bneighborhood\u200b \u200bthat\u200b \u200bhave\u200b \u200bpassed\u200b \u200bthe\u200b \u200bnoise\u200b \u200blabeling\u200b \u200btest.we\u200b \u200bare initially\u200b \u200bconverting\u200b \u200bthe\u200b \u200bimage\u200b \u200binto\u200b \u200bgrayscale\u200b \u200bimage\u200b \u200busing\u200b \u200brgb2gray()\u200b \u200bfunction\u200b \u200bthen \u200b\u200bapplying\u200b \u200badaptive\u200b \u200bmean\u200b \u200bfiltering\u200b \u200bto\u200b \u200bthe\u200b \u200bresulting\u200b \u200bimage\u200b \u200band\u200b \u200bthen\u200b \u200bconverted\u200b \u200bthe image\u200b \u200binto\u200b \u200bunsigned\u200b \u200binteger\u200b \u200b8\u200b \u200busing\u200b \u200bunit8()\u200b \u200bfunction.\n\n\u200bIn\u200b \u200bthis\u200b \u200bway\u200b \u200bwe\u200b \u200bpreprocessed image.then\u200b \u200bwe\u200b \u200bperformed\u200b \u200bGMM\u200b \u200bsegmentation(Gaussian\u200b \u200bMixture\u200b \u200bModel)\u200b \u200bon\u200b \u200bthe preprocessed\u200b \u200bimage\u200b \u200bwith\u200b \u200bnumber\u200b \u200bof\u200b \u200bregions\u200b \u200b2\u200b \u200band\u200b \u200bnumber\u200b \u200bof\u200b \u200bGMM\u200b \u200bcomponents 2\u200b \u200band\u200b \u200bmaximum\u200b \u200bnumber\u200b \u200biterations\u200b \u200b10.\u200b \u200bwe\u200b \u200bperformed\u200b \u200bk-means\u200b \u200bsegmentation\u200b \u200bwith k=2.\u200b \u200bthen\u200b \u200bwe\u200b \u200bImplemented\u200b \u200bHMRF-EM\u200b \u200b(Hidden\u200b \u200bMarkov\u200b \u200bRandom\u200b \u200bField\u200b \u200bModel)\u200b \u200band its\u200b \u200bExpectation-Maximization\u200b \u200bAlgorithm. \n \n\n**The picture decribes the difference between Malignant and Benign tissues in Breast**\n\n![Difference between Malignant and Benign tissues in Breast](https://raw.githubusercontent.com/st186/Breast-cancer-detection-using-Neural-networks/fca5059fa43d76a5cce9a39968f6b0d5e1051cfd/cancer2.PNG)\n\n## Block Diagram of the Project\n\n![Preview](https://raw.githubusercontent.com/st186/Breast-cancer-detection-using-Neural-networks/7cccc44b3ce4a51219e97df4f18da6147367996a/cancer.PNG)\n\n## How to make the project work?\n\nOpen the project in matlab and then run guidemo and then a gui mode window will open and then just follow the steps there.For further information check the screenshots.\n\n## References \n\nAnuj Kumar Singh and Bhupendra Gupta \u201cA novel approach for breast cancer detection and segmentation in mammography \u201d  Expert System With Applications 42(2015)990-1002.\n\nJ. Dheeba, N.Albert Singh, S. Tamil Selvi \u201cComputer-aided detection of breast cancer on mammograms: A swarm intelligence optimized wavelet neural network approach\u201d Journal of Biomedical Informatics (2014).\n\n## Team Members \n\nMadhusudan Verma,\nNagaraja Tarun,\nSubham Tewari(me)\n\n## Screenshots\n\n**Now you have to browse the image of the mammograms and give it as an input**\n![Preview](https://raw.githubusercontent.com/st186/Breast-cancer-detection-using-Neural-networks/457045e96b1177a41e9f641c5319499b79234bf5/Screenshot%20(12).png)\n\n**In this step adaptive mean filtering is done**\n![Preview](https://raw.githubusercontent.com/st186/Breast-cancer-detection-using-Neural-networks/457045e96b1177a41e9f641c5319499b79234bf5/Screenshot%20(13).png)\n\n**GMM Segmentation is done**\n![Preview](https://raw.githubusercontent.com/st186/Breast-cancer-detection-using-Neural-networks/457045e96b1177a41e9f641c5319499b79234bf5/Screenshot%20(14).png)\n\n**So you can see one as the output in the right side which depicts that the cancer is benign**\n![Preview](https://raw.githubusercontent.com/st186/Breast-cancer-detection-using-Neural-networks/457045e96b1177a41e9f641c5319499b79234bf5/Screenshot%20(15).png)\n"
    },
    {
        "repo": "/CRYPTOcoderAS/Breast-Cancer-Detection-ML-Project",
        "language": "HTML",
        "readme_contents": "# Breast-Cancer-Detection-App\n Breast Cancer Detection App Using Machine Learning XGBoost Classifier\n"
    },
    {
        "repo": "/rishiswethan/Cancer-detection-using-CNN",
        "language": "Python",
        "readme_contents": "# Breast-cancer-detection-using-CNN\n\nBreast cancer constitutes a leading cause of cancer-related deaths worldwide. Accurate diagnosis of cancer from eosin-stained images remains a complex task, as medical professionals often encounter discrepancies in reaching a final verdict. Computer-Aided Diagnosis (CAD) systems offer a means to reduce cost and enhance the efficiency of this intricate process. Traditional classification approaches rely on problem-specific feature extraction methods based on domain knowledge. To address the numerous challenges posed by feature-based techniques, deep learning methods have emerged as significant alternatives.\n\nWe propose a method for the classification of hematoxylin and eosin-stained breast biopsy images using Convolutional Neural Networks (CNNs). Our method classifies images into four categories: normal tissue, benign lesion, in situ carcinoma, and invasive carcinoma, as well as a binary classification of carcinoma and non-carcinoma. The network architecture is meticulously designed to extract information at various scales, encompassing both individual nuclei and the overall tissue organization. This design enables the seamless integration of our proposed system with whole-slide histology images. Our method achieves an accuracy of 77.8% for the four-class classification and demonstrates a sensitivity of 95.6% for cancer cases.\n\nTo use this project:\n\n1. You'll need python3 to run the program\n\n2. I've included the preprocessed image data. You can download it from [here](https://drive.google.com/open?id=17LR9ssbENit-3vsEAM63FptNasB5AHrr). Now place the 5 files that you just downloaded with the folder with the `.py` file\n\n3. Use `pip install package-name` to install the below packages\n\n4. You need to have the following python packages installed\n\t* keras\n\t* tensorflow (Both CPU or GPU version should do)\n\t* PIL\n\t* numpy\n\n5. You can modify the default hyparameters by modifying the variables between the `#` in the first few lines line\n\nTo run the program, navigate to the folder in command line and use the following command,\n```\npython BreastCancer.py\n```\nI've also included a pretrained model. To test your own image or one of the samples using it, paste the image in the folder with the `.py` file and rename it as `my_image.jpg`, then during execution choose to test your own image by following the on screen commands\n"
    },
    {
        "repo": "/mvab/mendelian-randomization-breast-cancer",
        "language": "R",
        "readme_contents": "## Deciphering how early life adiposity influences breast cancer risk using Mendelian randomization\n\nThis repository contains code and materials from a project I've done as a part of my PhD at the University of Bristol.\n\n_Publication in Communications Biology: [https://www.nature.com/articles/s42003-022-03272-5](https://www.nature.com/articles/s42003-022-03272-5)_\n\n_Related blogpost about setting up multivariable MR the way it was done in this project: [https://marinalearning.netlify.app/2021/03/22/setting-up-multivariable-mendelian-randomization-analysis/](https://marinalearning.netlify.app/2021/03/22/setting-up-multivariable-mendelian-randomization-analysis/)_\n\n_Cite this codebase:_ [![DOI](https://zenodo.org/badge/299559508.svg)](https://zenodo.org/badge/latestdoi/299559508)\n\n\n**This README contains:**\n\n- e-poster summarising the project research (2 Nov 2020)\n- project background and aims\n- outline of the project work/code stored in this repo\n\n\n### E-poster created for NCRI Virtual conference 2020\n\n<img src=\"e-poster_NCRI.png\" width=\"1000\"> \n\n\n\n### Project Background \n\nMultiple observational studies and a recent MR study (Richardson et al., 2020) have shown that early life adiposity may be protective against later-life breast cancer. The biological mechanism underlying this effect is unclear but is likely to independent of adiposity in adulthood. In this project, the aim is to identify potential mediators of the protective effect of high childhood BMI on breast cancer risk. \n\nAs a part of this project I have performed the following (depending on data availability):\n\n  -\tExtensive literature study into potential mediators that may be influenced by childhood BMI and affect breast cancer risk \n  -\tMR of childhood BMI on the identified potential mediators (step 1 of 2-step MR)\n  -\tMR of the identified potential mediators on breast cancer (step 2 of 2-step MR)\n  -\tMultivariable MR analysis on mediators that show causal effects in the 2-step MR\n  -\tMediation analysis using the product and difference methods to calculate mediated direct and indirect effects of the individual mediators on breast cancer risk\n\nThe five main categories of potential mediators considered:\n\n  1.\tHormones (IGF1, oestradiol, SHBG, testosterone (free/total/bioavailable)\n  2.\tPhysical traits (mammographic density, breast size)\n  3.\tReproductive traits (age at menarche, age at first birth, number of births, age at menopause)\n  4.  Glycaemic traits (fasting insulin, fasting glucose, HOMA-B, Hba1c)\n\n\n\n### This repository\n\nMain analysis scripts and metadata (see details below):\n\n```\n\u251c\u2500\u2500 set_paths.R\n\u251c\u2500\u2500 01_process_gwas_summary.Rmd\n\u251c\u2500\u2500 02_mr_BMI_to_mediators.Rmd\n\u251c\u2500\u2500 03_mr_mediators-to-BC.Rmd\n\u251c\u2500\u2500 04_mvmr_run_analysis.Rmd\n\u251c\u2500\u2500 05_mvmr_create_plots.Rmd\n\u251c\u2500\u2500 06_mediation_analysis.Rmd\n\u251c\u2500\u2500 functions.R\n\u251c\u2500\u2500 functions_mvmr.R\n\u251c\u2500\u2500 metadata\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 data_lookup.csv\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 pheno_correlations.csv\n\n```\n\nSupplementary scripts:\n\n```\n\u251c\u2500\u2500 code_supplementary\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 extract_from_GWAScatalog.Rmd\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 review_mediators_in_MRBase.Rmd\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 create_pheno_covariance_matrix.Rmd\n\n```\n\n\n\n\n\n### How to reproduce this analysis, i.e. Main Workflow\n\n\n1.  Script `set_paths.R` is imported by all other scripts, and is used to set the environment of where the project is run.\n\n2. Rmd `01_process_gwas_summary.Rmd` is required for processing data that comes as text files (i.e. GWAS summary stats) from the IEU GWAS pipeline or other sources. This script has to be run to convert raw data into `outcome` data frames and to extract instruments from each GWAS (in `exposure` format) and save them to be used directly in MR analysis in subsequent scripts. The names of raw files, tidy outcome data frames, and exposure instruments are all get saved in the metadata file `data_lookup.csv` upon generation. (NB the metadata file has to contain raw file names and the desired output prefixes before running this Rmd).\n\n3. Rmd `02_mr_BMI_to_mediators.Rmd` runs univariable MR of Childhood and Adult BMI on all mediators specified in metadata file (`data_lookup.csv`). The code has to be run interactively per trait category. The results merged by trait category will be stored in `Results` directory outside the codebase. After the analysis, forest plots can be created for each trait category. To recreate the plots, don't need to rerun the full analysis, can just read in the merged files. The plots will be saved in the codebase in `figures/`. \n\n\n4. Rmd `03_mr_mediators-to-BC.Rmd` is used to run univariable MR of all mediators (and BMI) on Breast cancer (`ieu-a-1126`). The code has to be run interactively per trait category, the results are stored in `Results` outside the codebase. After the analysis, forest plots can be created for each trait category. To recreate the plots, don't need to rerun the full analysis, can just read in the merged files. The plots will be saved in the codebase in `figures/`. \n\n5. Rmd `04_mvmr_run_analysis.Rmd` runs four types of MVMR with each mediator, but first we run MVMR with BMIs only (Analysis 0).  \n\n\t*\t (Analysis 0) multivariable MR: childhood BMI + adult BMI -> Breast Cancer \n\t*\t (Analysis 1) multivariable MR: childhood BMI + adult BMI -> mediator\n\t*\t (Analysis 2) multivariable MR: childhood BMI + mediator -> Breast Cancer \n\t*\t (Analysis 3) multivariable MR: childhood BMI + adult BMI + mediator -> Breast Cancer \n\t*\t (Analysis 4) multivariable MR: childhood BMI + childhood height + mediator -> Breast Cancer \n\n\n\tThe code has to be run interactively per trait category, the results are stored in `Results/trair_category/mediator/` outside the codebase. The analysis is structured as a large for-loop that will perform all four MVMR for each mediator in the selected trait category when individually specified T/F outside the loop. After all analyses have been performed, the mediators within each trait category are collated into a single dataframe and saved in `Results/trait_category/merged/` directory.\n\n  MVMR analysis is performed using modified code from `TwoSampleMR` package and also `MVMR` package for comparison. MVMR sensitivity tests are done using `MVMR` package and phenotypic correlations estimated using `metaCCA` package (see `code_supplementary/create_pheno_covariance_matrix.Rmd`)\n\n6. Rmd `05_mvmr_create_plots.Rmd` creates forest plots from all MVMR analyses for each trait category separately and by trait category. The plots are saved to `figures` within the codebase. The code also creates summary plot of all direct Childhood BMI estimates from Analyses 2 & 3.\n\n\n7. Rmd `06_mediation_analysis.Rmd` contains a workflow for performing MR mediation analysis using Difference and Product method, with CIs calculation using Delta and Propagation of Errors methods. The script also contains methods for plotting the calculated indirect estimates (+CIs) as forest plots. \n\n\n\n\n_Supplementary scripts_\n\n`extract_from_GWAScatalog.Rmd` was used for looking up traits (breast size and mammographic density) in GWAS catalogue to extract the SNPs to use as instruments fro this traits. However, for most studies it ended up bbeing easier (more reliable) to just get the data from supplementary information provided with the papers. The code loads raw data, cleans it, and saves in the format that is ready to be used as instruments in MR analysis.\n\n`create_pheno_covariance_matrix.Rmd` contains the workflow for applying the phenotypic correlation function from `metaCCA` package to calculate pheno_cor scores between any two GWAS traits. Tho pheno_cor score is required for MVMR sensitivity tests (F-statistics calculation). The process is time intensive, so all pheno_cor values needed for this project have been pre-calculated and are now stored in `metadata/pheno_correlations.csv`.\n"
    },
    {
        "repo": "/kanchitank/Medibuddy-Smart-Disease-Predictor",
        "language": "Jupyter Notebook",
        "readme_contents": "# Medibuddy: Smart Disease Predictor\n\n## Sample images of the web application\n\n### Home Page\n<img src=\"images/Sample_Web_App_Images/sample1.png\" alt=\"My cool logo\"/>\n<br>\n\n### Diabetes Predictor\n<img src=\"images/Sample_Web_App_Images/sample2.png\" alt=\"My cool logo\"/>\n<br>\n\n### Breast Cancer Predictor\n<img src=\"images/Sample_Web_App_Images/sample3.png\" alt=\"My cool logo\"/>\n<br>\n\n### Malaria Predictor\n<img src=\"images/Sample_Web_App_Images/sample4.png\" alt=\"My cool logo\"/>\n<br>\n\n### Negative Result Page\n<img src=\"images/Sample_Web_App_Images/sample5.png\" alt=\"My cool logo\"/>\n<br>\n\n### Positive Result Page\n<img src=\"images/Sample_Web_App_Images/sample6.png\" alt=\"My cool logo\"/>\n"
    },
    {
        "repo": "/advikmaniar/ML-Healthcare-Web-App",
        "language": "Python",
        "readme_contents": "# ML-Healthcare-Web-App\n\nThis is an interactive Machine Learning Web App \"ML in Healthcare\" developed using Python and StreamLit. It uses ML algorithms to build powerful and accurate models to predict the risk (High / Low) of the user of having a Heart Attack or Breast Cancer based on the user's specific attributes like age, sex, heart rate, blood sugar, etc.\n\n<h2><b> View App Here: </b></h2>\n\n[![StreamLit App](https://static.streamlit.io/badges/streamlit_badge_white.svg)](https://share.streamlit.io/advikmaniar/ml-healthcare-web-app/main/ML_Healthcare.py)\n\n<hr>\n\nThis applications has two basic sections:\n\n<h2>1) - Model Building </h2>\nIn this section 7 different models are built using different ML algorithms. They are: \n\n```\n1. Logistic Regression \n2. KNN\n3. SVM \n4. Decision Trees \n5. Random Forest \n6. Gradient Boosting \n7. XGBoost\n```\nThe models are trained using data from https://archive.ics.uci.edu/ml/index.php, particularly the [Heart Attack Prediction](https://github.com/advikmaniar/ML-Heathcare-Web-App/blob/main/Data/heart.csv) and [Breast Cancer (Wisconsin)](https://github.com/advikmaniar/ML-Heathcare-Web-App/blob/main/Data/BreastCancer.csv) datasets.\n\nAn interactive side-dashboard is created using the streamlit `st.sidebar` call which enables the user to do the following:\n1. Choose dataset - `Heart Attack / Breast Cancer`\n2. Choose algorithm - `Logistic Regression , KNN , SVM , Decision Trees , Random Forest , Gradient Boosting , XGBoost.`\n3. Change the important parameters for each model - `Learning Rate, Random State, Regularization Coeff, Gamma, Kernel, n_estimators` etc. \n\nAfter training using the parameters selected by the user, the tuned model is built and ready to be tested on our testing data. The classification plot and confusion matrix is displayed for the model selected along with the model metrics: `Accuracy, Precision, Recall, F1-Score, Mean Squared Error, Execution Time`. The user can observe real-time changes in the plots and metrics as they change the model parameters further. \n> **This is a great way to understand the different ML algorithms and how they are influenced by tuning the hyperparameters.**\n> \n![image](https://user-images.githubusercontent.com/72503778/123002403-85b73700-d3cf-11eb-80a1-71262561b9c8.png)\n\nThe 7 models (optimum tuning) performed as follows: <br>\n`Criterion: Accuracy`\nModel | Accuracy (Heart Attack / Breast Cancer)\n------------ | -------------\nLogistic Regression | **91.803% / 100.0%**\nKNN | **86.89% / 96.49%**\nSVM | **93.44% / 100.0%**\nDecision Trees | **52.56% / 60.53%**\nRandom Forest | **90.164% / 98.24%**\nGradient Boosting | **88.53% / 96.49%**\nXGBoost | **95.08% / 94.737%**\n\n<h2>2) - User Prediction </h2>\nIn this section, the user can use any model developed above to predict their status (High Risk / Low Risk) using their own values. (Either for Heart Attack or Breast Cancer)\n\n![image](https://user-images.githubusercontent.com/72503778/123003157-6d93e780-d3d0-11eb-81fc-8dd6abe89efa.png)\n\n![image](https://user-images.githubusercontent.com/72503778/123003260-93b98780-d3d0-11eb-9ff0-bb27da6a105e.png)\n\n\nView the final video [here](https://github.com/advikmaniar/ML-Healthcare-Web-App/blob/main/Results/Video.mp4).\n<hr>\n\n<h1> Thank You! </h1>\n\n<hr>\n\n\n\n"
    },
    {
        "repo": "/javismiles/Deep-Learning-predicting-breast-cancer-tumor-malignancy",
        "language": "Jupyter Notebook",
        "readme_contents": "# Predicting Cancer Malignancy with a 2 layer neural network coded from scratch in Python.\n\n![The Loss Landscape](https://github.com/javismiles/Deep-Learning-predicting-breast-cancer-tumor-malignancy/blob/master/images/loss-landscape-deep-learning-animation-cover2.gif?raw=true)\n\n**Access the code with this link<br>\n<a href=\"https://github.com/javismiles/Deep-Learning-predicting-breast-cancer-tumor-malignancy/blob/master/nn-2l-raw.ipynb\" target=\"_blank\">Python Jupyter Notebook</a>**\n\n**This notebook holds the Python code connected to this 3 part article:**\n\n**<a href=\"https://towardsdatascience.com/the-keys-of-deep-learning-in-100-lines-of-code-907398c76504\" target=\"_blank\">Part 1</a> | <a href=\"https://towardsdatascience.com/coding-a-2-layer-neural-network-from-scratch-in-python-4dd022d19fd2\" target=\"_blank\">Part 2</a> | <a href=\"https://towardsdatascience.com/predict-malignancy-in-breast-cancer-tumors-with-your-own-neural-network-and-the-wisconsin-dataset-76271a05e941\" target=\"_blank\">Part 3</a>**<br>\n\n**With this code and the associated articles, you are going to:**\n- Create a neural network from scratch in Python. Train it using the gradient descent algorithm.\n- Apply that basic network to The Wisconsin Cancer Data-set. Predict if a tumor is benign or malignant, based on 9 different features.\n- Explore deeply how back-propagation and gradient descent work.\n- Review the basics and explore advanced concepts. \n\n**The data comes from The Wisconsin Cancer Data-set.**<br>\nThis data was gathered by the University of Wisconsin Hospitals, Madison and by Dr. William H. Wolberg.<br>\n**By request of the owners of the data**: we mention one of the studies linked to the data-set: O. L. Mangasarian and W. H. Wolberg: \"Cancer diagnosis via linear programming\", SIAM News, Volume 23, Number 5, September 1990, pp 1 & 18.\n\n\n\n"
    },
    {
        "repo": "/yala/OncoServe_Public",
        "language": "Python",
        "readme_contents": "# OncoServe: Deploying Deep Learning Models for Breast Cancer Risk Assessment, and Breast Density Assessment.\n\n## Introduction\nThis repository shares the models described in [Towards Robust Mammography-Based Models for Breast Cancer Risk](https://www.science.org/doi/10.1126/scitranslmed.aba4373) and [Mammographic Breast Density Assessment Using\nDeep Learning: Clinical Implementation](https://pubs.rsna.org/doi/10.1148/radiol.2018180694) as a (Flask) webserver.  You can send the webserver regular HTTP requests with a list of dicoms for a given mammogram, and a set of metadata keys (like MRN or Accession), and the webserver will return the model predictions along back with the same metadata. We note that we do not support all dicom formats, we assume presentation view mammograms, and have only tested this system with Hologic mammograms.\n\n## Structure:\nOncoServe spins up a webserver in a docker container encapsulating all the software requirments to convert dicoms, and run the deep learning models. It imports [Mirai](https://github.com/yala/Mirai) (and [OncoNet](https://github.com/yala/OncoNet_Public) for older models) and [OncoData](https://github.com/yala/OncoData_Public) as submodules.\n\nThe repositories perform the following functions:\n- [OncoData](https://github.com/yala/OncoData_Public): handles conversion from dicom to png\n- [Mirai](https://github.com/yala/Mirai) and [OncoNet](https://github.com/yala/OncoNet_Public) : used for model development and training.\n- [OncoServe](https://github.com/yala/OncoServe_Public): Wraps model in a webserver that allows it to return outputs in real time given an HTTP request. Used in clinical implementations.\n\n## System Requirements\n- [Docker](https://www.docker.com/)\n- 32 GB of RAM, 32GB of Disk.\n\n## How to run it?\n### Startup Steps:\n- First, download the correct docker image from the following links: [Mirai](https://www.dropbox.com/s/k0wq2z7xqr95y3b/oncoserve_mirai.0.5.0.tar?dl=0), [Density](https://www.dropbox.com/s/vncq542aapjtvc8/oncoserve_density_0.1.0.tar?dl=0).\n\n- Pull load the docker image from dockerhub.\n``` docker load < filename.tar```\n\n- Start the docker container following the instructions for the specific application (listed bellow).\n\n### Running the Density Application:\n```docker run -p 5000:5000 --shm-size 32G -e CONFIG_NAME=config.DensityConfig learn2cure/oncoserve_density:0.1.0```\n\n\n### Running the Mirai Application:\n```docker run -p 5000:5000 --shm-size 32G learn2cure/oncoserve_mirai:0.5.0```\n\n## How to use it?\n\n### Streaming mode (One mammogram at a time):\nOnce your webserver is setup, you can get model assessments by sending it HTTP requests.\nSee `tests/demo.py` for a usage example in python or `tests/demo.js` for a usage example in javascript. The python demo is organized as a python test case. Note, you'll need to update the paths in the setUp function in the demo to refer to real dicom paths (see comments in the file).\n\n### Batch mode:\nSee the [Mirai](https://github.com/yala/Mirai) github. This will require logging into the docker container with a shell and running our batch processing scripts.\n\nNote, batch processing is not supported under the density application.\n\n## Have questions?\nPlease email yala@berkeley.edu.\n\n## Usage\nThis tool and all associated code is provided for under MIT License.\n"
    },
    {
        "repo": "/bora-pajo/breast-cancer-prediction",
        "language": "Jupyter Notebook",
        "readme_contents": "### Predicting malignant versus benign breast cancer cases\nPredicting the probability that a diagnosed breast cancer case is malignant or benign based on Wisconsin dataset from UCI repository. \n\n### Project Overview\nAccording to the Centers for Disease Control and Prevention (CDC) breast cancer is the most common type of cancer for women regardless of race and ethnicity (CDC, 2016). Around 220,000 women are diagnosed with breast cancer each year in the United States (CDC, 2016). Although we may not be aware of all the factors contributing in developing breast cancer, certain attributes such as family history, age, obesity, alcohol and tobacco use have been identified from research studies on this topic (DeSantis, Ma, Bryan, & Jemal, 2014). Breast images procedures such as mammography have been found to be quite effective in early identification cases of breast cancer (Ball, 2012).  When breast images procedures are not utilized, patients can find out late about their diagnosis to be able to treat it.  Similar work on attempting to find the best way to predict the type of cancer based on images of mammograms has identified Support Vector Machine as the best predictor after tuning parameters.  \n \n\n#### Files in this repository\nMany files in the respository are examples and things to be saved as the work progresses. Those are names examples_01 and so on. \nThe file where you can find all the coding for different classifiers is called _breast_cancer_clean.ipynb_ \n\n#### Problem Statement\nThis project focuses in investigating the probability of predicting the type of breast cancer (malignant or benign) from the given characteristics of breast mass computed from digitized images.  The cases provided, are cases diagnosed with some type of tumor, but only some of them (approximately 37%) are malignant.  This project will examine the data available and attempt to predict the possibility that a breast cancer diagnosis is malignant or benign based on the attributes collected from the breast mass. To achieve this goal, the following steps are identified:\n\u2022\tDownload the breast cancer images data from UCI repository\n\u2022\tFamiliarize with the data by looking at its shape, the relations between variables, their possible correlations, and other attributes of the dataset. \n\u2022\tPreprocess data if needed\n\u2022\tSplit the data into testing and training samples\n\u2022\tEmploy various classifiers (K-neighbors, Decision trees, SVC, QDA, AdaBoost, Na\u00efve Bayes, Random Forest, and MLP classifier) to predict the data with different sets of training samples (100, 200, 300, and 400). \n\u2022\tOnce the best predicting model is identified, will reduce the training set in size to see what is the limit for this classifier to best predict these data.\n\u2022\tCompare the best identified classifier with evaluation metric stated at the beginning of the project.\n\u2022\tWrite conclusions. \n\n\n### Libraries used\n```python\nimport numpy as np #for linear algebra\nimport pandas as pd #for chopping, processing\nimport csv #for opening csv files\n%matplotlib inline \nimport matplotlib.pyplot as plt #for plotting the graphs\nfrom scipy import stats #for statistical info\nfrom time import time\n\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split # to split the data in train and test\nfrom sklearn.model_selection import KFold # for cross validation\nfrom sklearn.grid_search import GridSearchCV  # for tuning parameters\nfrom sklearn import metrics  # for checking the accuracy \n\n#Classifiers \n\nfrom sklearn import svm #for Support Vector Machines\nfrom sklearn.svm import SVC # for support vector classifier\nfrom sklearn.neighbors import NearestNeighbors #for nearest neighbor classifier\nfrom sklearn.neighbors import KNeighborsClassifier # for K neighbor classifier\nfrom sklearn.tree import DecisionTreeClassifier #for decision tree classifier\nfrom sklearn.naive_bayes import GaussianNB  #for naive bayes classifier\nfrom sklearn.ensemble import RandomForestClassifier #for Random Forest\nfrom sklearn.ensemble import AdaBoostClassifier # for Ada Boost\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis # for Quadratic Discriminant Analysis\nfrom sklearn.neural_network import MLPClassifier # for multi layer perceptron classifier\n```\n\n\n#### Dataset and Inputs\nThe characteristics of the cell nuclei have been captured in the images and a classification methods which uses linear programming to construct a decision line. The dataset is published by Kaggle and taken from the University of California Irvine (UCI) machine learning repository.  The data is taken from the Breast Cancer Wisconsin Center. It includes ten (10) attributes taken from each cell nucleus as well as ID and the diagnosis (M=malignant, B=benign).  The dataset has 570 cases and 31 variables.   \n\n#### Evaluation metric\n\nF1 score is a measure of accuracy or the ratio of the data that was accurately predicted. The closer the F1-score is to a value of 1, the best the prediction is, and the closer to a value of 0 it is, the worse the prediction. F1-score considers the true positives and the true negatives, and is best used when comparing various classifiers as I am proposing to do in this dataset.  From the literature, I reached the conclusion that F1-score is the best evaluation metric to be used for this type of classification problem.  \n\u2022\tThe formula for the F1 score from the sklearn documentation is F1 = 2* (precision * recall) / (precision + recall). \n\u2022\tPrecision is a measure for result relevancy (according to sklearn documentation). It is often referred to as specificity and shows the number of cases that are relevant.  For example, in this particular dataset the number of cases that are identified as malignant would be very relevant. The formula for precision is: P = Tp/(Tp+Fp) where Tp are the true positives (the number of cases that were identified correctly as true (in this case these will be the malignant cases), and Fp are false positives (or the number of cases that are not malignant but are incorrectly identified as such). \n\u2022\tRecall on the other hand is considered a measure of sensitivity and is measured as the number of True positives over the number of True positives plus the False negatives. R = Tp/(Tp+Fn)\n\u2022\tTrue positives are the correctly identified cases as 1 or malignant in our case\n\u2022\tTrue negatives are the correctly identified cases as 0 or benign in our case\n\u2022\tFalse positives are the cases that are identified as positive (1 or malignant in this case) but are in fact negative (0 or benign) \n\u2022\tFalse negatives are the cases that are identified as negative (0 or benign in this case) and are in fact positive (1 or malignant). These are the most dangerous in this dataset. \n\u2022\tReceiver operating characteristics (ROC) is another measurement metric used in this project.  It is a measurement of the classifier quality based on the true positives and the true negatives. ROC measured the proportion between true positives and true negatives.\n\u2022\tArea under the curve (AUC) is the last measurement metric used in this project.  It basically shows how well the classifier is performing.  If AUC is closer to a value of 1, the classifier is doing pretty well. If AUC is closer to 0 it is not doing a good job.  \n\n\n### Graphs\nSome examples from bokeh, matplolib plots, prettyplots, and ggplot for python\n\n#### Implementation\nFirst, the dataset was split into training and testing sets randomly. Training set included 400 cases and testing set included 169 cases. The rationale for this division was to see how many cases were the optimal number for training the data with each different classifier and how long it took in each case.  To implement different classifiers, I used training sets of 100, 200, 300, and 400 and got the results of training time, F-score for training, predicting time for the test data and the F-score for the testing set based on each case. The classifiers utilized were: 1) K-nearest neighbor, 2) Decision trees, 3) SVC, 4) Na\u00efve Bayes, 5) Random Forest, 6) AdaBoost, 7) QDA, and 8) MLP. I ran these classifiers two times: \n\n\n### Classifiers used to predict the breast cancer for a training size = 300 \n\nType of Classifiers | Training Time | Prediction Time| F1 score (training set) | **F1 Score (testing set)**\n:---:|:---:|:---:|:---:|:---:\n**K-nearest neighbor** | .0012 | .0022 | .9264 | .9081\n**Decision Trees** | .0033 | .0002 | 1.000 | .9239\n**SVC** | .0051 |.0029 | 1.000 |.0000\n**Naive Bayes** |.0006 | .0003 |.9058 | .9341\n**Random Forest** | .0381 | .0064 | 1.000 | .9101\n**AdaBoost** | .1866 | .0055 | 1.000 | .9451\n**QDA** |.0013 | .0005 | .9528 | .9688\n**MLP** |.0084 | .0005 | .6648 | .6267\n\n\n_The F-1 score measures the accuracy of the prediction. The closer it is to a value of 1, the better the prediction is._\nQDA seems to work perfectly in this case compared to other forms of classifiers and it is also quite fast to train the data.\n\nA second round of classifiers with an increase training size of 400 was attempted again.  Results are shown below:\n\n### Classifiers used to predict the breast cancer for a training size = 400 \n\nType of Classifiers | Training Time | Prediction Time| F1 score (training set) | **F1 Score (testing set)**\n:---:|:---:|:---:|:---:|:---:\n**K-nearest neighbor** | .0006 | .0029 | .9290 | .9038\n**Decision Trees** | .0049 | .0003 | 1.000 | .9074\n**SVC** | .0110 |.0068 | 1.000 |.0000\n**Naive Bayes** |.0007 | .0003 |.9164 |.9541\n**Random Forest** | .0403 | .0058 | .9936 | .9358\n**AdaBoost** | .1767 | .0057 | 1.000 | .9541\n**QDA** |.0013 | .0005 | .9585 | .9381\n**MLP** |.0102 | .0005 | .5663| .4843\n\n##### ROC and AUC\nThere are graphs for ROC and AUC provided for each classifier that was used.\n\n#### Improvements\nI think there are possible improvements to be done to the other classifiers that were used to predict this data.  Because they did not perform as great as the first three, I dismissed them and continued improving the ones that predicted the best from the very beginning.  This is a common approach humans take on many things, but it is possible to modify the other classifiers that did not perform well initially. Tuning them, or removing highly correlated variables (especially for SVC), could have improved these other classifiers significantly.  \n\nROC information added for the three best classifiers, Naive Bayes, QDA, and AdaBoost\n### License\n\nThis dataset was taken from UCI repository\n\n\n\n\n"
    },
    {
        "repo": "/aditisingh/Breast_cancer_detection",
        "language": "Python",
        "readme_contents": "# Breast_cancer_detection\nUsing pre-trained model to classify images to detect cancerous cells\n\nPre-requirements:\n1. Python2.7\n2. MATLAB (LIBSVM)\n3. Numpy, Scipy,Sklearn\n4. Tensorflow 1.0\n5. Tflearn\n\n\nBreakHis dataset can be found at: http://web.inf.ufpr.br/vri/breast-cancer-database\nThe VGG-16 weights are being used from Davi Frossard's webpage here: https://www.cs.toronto.edu/~frossard/vgg16/vgg16_weights.npz\nHere is a clean introductory tutorial by him: https://www.cs.toronto.edu/~frossard/post/vgg16/\n\nWe try pre-trained network and classification vs training from scratch.\nMethod1: Using pre-trained VGG-16 to get features.\n1. Run vgg16_cv.py to extract the features from each image of BreakHis dataset. It will create one feature file per image int he same folder\n2. Run generate_features.py to combine all individual feature files into one feature matrix (mat file). It also creates a separate target mat file.\n3. Run CV_balancing_code.m to treat the data imbalance. It outputs 4 files: training data, training data targets, test data and test data targets\n4. Use classifier_code.m and RandomForest_CV.m to classify the data using Linear SVM, Polynomial SVM and Random Forest.\n\nMethod 2:\nRun alexnet.py to get the trained AlexNet model and confusion matrix. \n\nAnalysis can be found at https://medium.com/womeninai/classifying-biological-images-using-pre-trained-cnns-102060de0687\n"
    },
    {
        "repo": "/bhklab/genefu",
        "language": "R",
        "readme_contents": "[![R-CMD-check](https://github.com/bhklab/genefu/workflows/R-CMD-check/badge.svg)](https://github.com/bhklab/genefu/actions)\n\n\n**Bioc-Release**: ![Bioconductor RELEASE](http://bioconductor.org/shields/build/release/bioc/genefu.svg) \n**Bioc-Devel**: ![Bioconductor DEVEL](http://bioconductor.org/shields/build/devel/bioc/genefu.svg)\n\ngeneFu: R package providing various functions relevant for gene expression analysis with emphasis on breast and ovarian cancers.\n\nVersion 2\nThis version includes major updates of gene expression signatures, subtype classification methods, and prognostic signature predictors. Some updates include the addition of recent gene signatures (now includes adding Crins and Yoshihara signatures for ovarian cancer), updates to PAM50 subtype probabilities classification and rorS functions, and addition of the IntClust and AIMS subtyping and Claudin-Low subtyping schemes for breast cancer. \n\n"
    },
    {
        "repo": "/IBM/MAX-Breast-Cancer-Mitosis-Detector",
        "language": "Python",
        "readme_contents": "[![Build Status](https://travis-ci.org/IBM/MAX-Breast-Cancer-Mitosis-Detector.svg?branch=master)](https://travis-ci.org/IBM/MAX-Breast-Cancer-Mitosis-Detector) [![Website Status](https://img.shields.io/website/http/max-breast-cancer-mitosis-detector.codait-prod-41208c73af8fca213512856c7a09db52-0000.us-east.containers.appdomain.cloud/swagger.json.svg?label=api+demo)](http://max-breast-cancer-mitosis-detector.codait-prod-41208c73af8fca213512856c7a09db52-0000.us-east.containers.appdomain.cloud)\n\n[<img src=\"docs/deploy-max-to-ibm-cloud-with-kubernetes-button.png\" width=\"400px\">](http://ibm.biz/max-to-ibm-cloud-tutorial)\n\n# IBM Code Model Asset Exchange: Breast Cancer Mitosis Detector\n\nThe [Tumor Proliferation Assessment Challenge 2016 (TUPAC16)](http://tupac.tue-image.nl/) was created to develop state-of-the-art algorithms for automatic prediction of tumor proliferation scores from whole-slide histopathology images of breast tumors. The [IBM CODAIT](http://codait.org) team trained a mitosis detection model (a modified ResNet-50 model) on the [TUPAC16 auxiliary mitosis dataset](http://tupac.tue-image.nl/node/3), and then applied it to the whole slide images for predicting the tumor proliferation scores.\n\nThis repository contains code to instantiate and deploy the mitosis detection model mentioned above. This model takes a 64 x 64 PNG image file extracted from the whole slide image as input, and outputs the predicted probability of the image containing mitosis. For more information and additional features, check out the [deep-histopath](https://github.com/CODAIT/deep-histopath) repository on GitHub.\n\nThe code in this repository deploys the model as a web service in a Docker container. This repository was developed as part of the [IBM Code Model Asset Exchange](https://developer.ibm.com/code/exchanges/models/).\n\n## Model Metadata\n| Domain | Application | Industry  | Framework | Training Data | Input Data Format |\n| ------------- | --------  | -------- | --------- | --------- | -------------- | \n| Vision | Cancer Classification | Health care | Keras | [TUPAC16](http://tupac.tue-image.nl/node/5) | 64x64 PNG Image|\n\n_Note:_ Although this model supports different input data formats, the inference results are sensitive to the input data. In order to keep the extracted images the same as the original datasets, PNG image format should be used.\n\n\n## References\n* _Dusenberry, Mike, and Hu, Fei_, [Deep Learning for Breast Cancer Mitosis Detection](https://github.com/CODAIT/deep-histopath/raw/master/docs/tupac16-paper/paper.pdf), 2018.\n\n## Licenses\n\n| Component | License | Link  |\n| ------------- | --------  | -------- |\n| This repository | [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0) | [LICENSE](LICENSE) |\n| Training Data | Custom License | [TUPAC16](http://tupac.tue-image.nl/node/5) |\n| Test Samples | Custom License | [Sample README](samples/README.md) |\n\n## Pre-requisites:\n\n* `docker`: The [Docker](https://www.docker.com/) command-line interface. Follow the\n[installation instructions](https://docs.docker.com/install/) for your system.\n* The minimum recommended resources for this model is 2GB Memory and 2 CPUs.\n* If you are on x86-64/AMD64, your CPU must support [AVX](https://en.wikipedia.org/wiki/Advanced_Vector_Extensions) at the minimum.\n\n# Deployment options\n\n* [Deploy from Quay](#deploy-from-quay)\n* [Deploy on Red Hat OpenShift](#deploy-on-red-hat-openshift)\n* [Deploy on Kubernetes](#deploy-on-kubernetes)\n* [Run Locally](#run-locally)\n\n## Deploy from Quay\n\nTo run the docker image, which automatically starts the model serving API, run:\n\n```\n$ docker run -it -p 5000:5000 quay.io/codait/max-breast-cancer-mitosis-detector\n```\n\nThis will pull a pre-built image from the Quay.io container registry  (or use an existing image if already cached locally) and run it.\nIf you'd rather checkout and build the model locally you can follow the [run locally](#run-locally) steps below.\n\n## Deploy on Red Hat OpenShift\n\nYou can deploy the model-serving microservice on Red Hat OpenShift by following the instructions for the OpenShift web console or the OpenShift Container Platform CLI [in this tutorial](https://developer.ibm.com/tutorials/deploy-a-model-asset-exchange-microservice-on-red-hat-openshift/), specifying `quay.io/codait/max-breast-cancer-mitosis-detector` as the image name.\n\n## Deploy on Kubernetes\n\nYou can also deploy the model on Kubernetes using the latest docker image on Quay.\n\nOn your Kubernetes cluster, run the following commands:\n\n```\n$ kubectl apply -f https://raw.githubusercontent.com/IBM/MAX-Breast-Cancer-Mitosis-Detector/master/max-breast-cancer-mitosis-detector.yaml\n```\n\nThe model will be available internally at port `5000`, but can also be accessed externally through the `NodePort`.\n\n## Run Locally\n\n1. [Build the Model](#1-build-the-model)\n2. [Deploy the Model](#2-deploy-the-model)\n3. [Use the Model](#3-use-the-model)\n4. [Development](#4-development)\n5. [Cleanup](#5-cleanup)\n\n### 1. Build the Model\n\nClone the `MAX-Breast-Cancer-Mitosis-Detector` repository locally. In a terminal, run the following command:\n\n```\n$ git clone https://github.com/IBM/MAX-Breast-Cancer-Mitosis-Detector.git\n```\n\nChange directory into the repository base folder:\n\n```\n$ cd MAX-Breast-Cancer-Mitosis-Detector\n```\n\nTo build the docker image locally, run: \n\n```\n$ docker build -t max-breast-cancer-mitosis-detector .\n```\n\nAll required model assets will be downloaded during the build process. _Note_ that currently this docker image is CPU\nonly (we will add support for GPU images later).\n\n### 2. Deploy the Model\n\nTo run the docker image, which automatically starts the model serving API, run:\n\n```\n$ docker run -it -p 5000:5000 max-breast-cancer-mitosis-detector\n```\n\n### 3. Use the Model\n\nThe API server automatically generates an interactive Swagger documentation page. Go to `http://localhost:5000` to load\nit. From there you can explore the API and also create test requests.\n\nUse the `model/predict` endpoint to load a test image (you can use one of the test images from the `samples` folder) and\nget predicted labels for the image from the API.\n\n![Swagger Doc Screenshot](docs/swagger-screenshot.png)\n\nYou can also test it on the command line, for example:\n\n```bash\n$ curl -F \"image=@samples/true.png\" -XPOST http://localhost:5000/model/predict\n```\n\nYou should see a JSON response like that below:\n\n```json\n{\"predictions\": [{\"probability\": 0.9884441494941711}], \"status\": \"ok\"}\n```\n\n### 4. Development\n\nTo run the Flask API app in debug mode, edit `config.py` to set `DEBUG = True` under the application settings. You will\nthen need to rebuild the docker image (see [step 1](#1-build-the-model)).\n\n### 5. Cleanup\n\nTo stop the docker container type `CTRL` + `C` in your terminal.\n\n## Links\n\n* [Transfer Learning in CNNs for Mitosis Detection](https://www.youtube.com/watch?v=E2Ne1JYLyp4): Interview on Transfer Learning in CNNs for Mitosis Detection at the OpenTech AI conference at IBM Finland, 2018\n* [Deep Learning for Breast Cancer Mitosis Detection](https://www.youtube.com/watch?v=vov4xyhs3jY&feature=youtu.be&t=1h4m57s): Presentation on SF Big Analytics Meetup, 2018\n\n## Resources and Contributions\n   \nIf you are interested in contributing to the Model Asset Exchange project or have any queries, please follow the instructions [here](https://github.com/CODAIT/max-central-repo).\n"
    },
    {
        "repo": "/vbookshelf/Breast-Cancer-Analyzer",
        "language": "Jupyter Notebook",
        "readme_contents": "\n## Breast-Cancer-Analyzer\n\nLive Web App: http://histo.test.woza.work/\n\n<br>\n\n<img src=\"http://histo.test.woza.work/assets/histo_sample.png\" width=\"350\"></img>\n\n<br>\n\nThis is a prototype for a freely available online tool that can detect two forms of Breast Cancer in histopathology image patches. The current histopathology process is time consuming and small malignant areas can be missed. This app can help speed up a pathologist's workflow and provide diagnosis support.\n\n<hr>\n\n<b>The app has two models. Each model can detect one of the following:</b>\n\n1. Metastatic Cancer - Cancer that has spread from the site of origin to other area/s of the body.\n2. Invasive Ductal Carcinoma - The most common subtype of breast cancer.\n\n\n<b>These are the papers that discuss the datasets that I used to train the two models:</b>\n\n1399 H&E-stained sentinel lymph node sections of breast cancer patients: the CAMELYON dataset<br>\nhttps://academic.oup.com/gigascience/article/7/6/giy065/5026175\n\nAutomatic detection of invasive ductal carcinoma in whole slide images with Convolutional Neural Networks<br>\nhttps://www.researchgate.net/publication/263052166_Automatic_detection_of_invasive_ductal_carcinoma_in_whole_slide_images_with_Convolutional_Neural_Networks\n\n<b>These are the links to the datasets:</b>\n\nhttps://www.kaggle.com/c/histopathologic-cancer-detection<br>\n(.tiff image format)\n\nhttps://www.kaggle.com/paultimothymooney/breast-histopathology-images<br>\n(.png image format)\n\n<b>The entire model building and training process is described in these two Kaggle kernels:</b>\n\nPart 1 : Breast Cancer Analyzer + Web App<br>\nhttps://www.kaggle.com/vbookshelf/part-1-breast-cancer-analyzer-web-app/notebook\n\nPart 2 : Breast Cancer Analyzer + Web App<br>\nhttps://www.kaggle.com/vbookshelf/part-2-breast-cancer-analyzer-web-app/notebook\n\n\n\n\nThe python code to build and train the models is included in the Jupyter notebook. All the javascript, css and html files are also freely available here. The trained models are also available.\n\n<hr>\n\nBugs & Lessons Learned:\n\n1. A large part of the web, including the web app for this project uses the Javascript language. I used Javascript to feed the images to the model. The challenge lies in the fact that Javascript is very fast whereas the model is not. This difference in speed can lead to incorrect predictions.<br>\nFor example, say we are using a loop to feed ten images to the model for prediction. Because of the realtively slower prediction speed, the model may end up making all of it's ten predictions using only image1 or image10. To solve this problem, on each iteration of the loop, we must make the code wait for the model to finish a prediction before the next loop begins i.e. before feeding it the next image.<br>\nIn the app I used async/await to achieve this. Because my Javascript knowledge is very basic, you should check the Javascript code thoroughly before using it in one of your own projects.\n\n\n2. I think there is one other potential source of prediction errors. In order to read an image Tensorflowjs needs the image to first be made part of an html image element. It then reads this image element and converts it into a tensor. When using multiple images, delays relating to putting an image into an html element (setting the src attribute) could also cause prediction errors. The programmer needs to keep this in mind especially if the image sizes are large.\n\n\n3. Most web browsers don't support the tiff image format. This needs to be kept in mind when pre-processing training data if the intention is to build a web app.\n\n\n4. Because Tensorflowjs is a new technology, web apps bulit using it may not work in some browsers. The user will see a message saying the \"Ai is loading...\" but that message will never go away because the app is actually frozen. It's better to advise users to use the latest version of Chrome.\n"
    },
    {
        "repo": "/rezacsedu/Multimodal-autoencoder-for-breast-cancer",
        "language": "Python",
        "readme_contents": "## Multimodal autoencoders for subtypes and survival prediction of breast cancer\nImplementation of our paper titled \"Prognostically Relevant Subtypes and Survival Prediction for Breast Cancer Based on Multimodal Genomics Data\" submitted to IEEE Access journal, August 2019. In this implementation, a multimodal autoencoders(MAE) is used to predict different clinical status of breast cancer patients based on multiplatformic genomics data. The MAE is trained with genomics data such as DNA methylation, gene expression, miRNA expressionfrom, and clinical outcomes from The Cancer Genome Atlas(TCGA). \n\n### Predicted clinical status\n1. Breast cancer subtypes which is determined by the estrogen receptor (ER), progesterone receptor (PGR), and HER2/neu status\n2. Survival rate (0-1, with 1 being the best chance of survival).\n\n### Requirements\n* Python 3\n* TensorFlow\n* Keras. \n\n### Download and create the dataset\n* Clone the repo using `git clone https://github.com/rezacsedu/MultimodalAE-BreastCancer.git`\n* Run the dataset creation program `python3 main_download.py -d DATASET_IDX`.\n\n| DATASET_IDX |                      Data Types                      |Data size(GB) |\n|------------:|:-----------------------------------------------------|:-----------------------:|\n|           1 | DNA Methylation                                      |          148            |\n|           2 | Gene Expression                                      |          9              |\n|           3 | miRNA Expression                                     |          0.24           |\n|           4 | Gene Expression + miRNA Expression                   |          10             |\n|           5 | DNA Methylation + Gene Expression + miRNA Expression |          162            |\n\n### Train the neural networks\n* Run the neural networks program `python3 main_run.py <options>`, with the below supported options: \n\n|               Option               |   Values   |                                                                                                                                                                                                                                                                                                                                                                                                              Details                                                                                                                                                                                            | Required |\n|-----------------------------------:|:-----------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------:|\n| -p PLATFORM<br>--platform PLATFORM | int [1-2]  | [1] Tensorflow, [2] Theano                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |    yes   |\n|             -t TYPE<br>--type TYPE | int [1-2]  | [1] Breast cancer type classification<br>[2] Survival rate regression                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |    yes   |\n|    -d DATASET<br>--dataset DATASET | int [1-15] | [1] DNA Methylation GPL8490<br>[2] DNA Methylation GPL16304<br>[3] Gene Expression Count<br>[4] Gene Expression FPKM<br>[5] Gene Expression FPKM-UQ<br>[6] miRNA Expression<br>[7] Gene Expression Count + miRNA Expression<br>[8] Gene Expression FPKM + miRNA Expression<br>[9] Gene Expression FPKM-UQ + miRNA Expression<br>[10] DNA Met GPL8490 + Gene Count + miRNA<br>[11] DNA Met GPL16304 + Gene Count + miRNA<br>[12] DNA Met GPL8490 + Gene FPKM + miRNA<br>[13] DNA Met GPL16304 + Gene FPKM + miRNA<br>[14] DNA Met GPL8490 + Gene FPKM-UQ + miRNA<br>[15] DNA Met GPL16304 + Gene FPKM-UQ + miRNA |    yes   |\n|              --pretrain_epoch PRE_EPOCH | int        | Pre-training epoch. Default = 100                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |    no    |\n|          --train_epoch TRAIN_EPOCH | int        | Training epoch. Default = 100                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |    no    |\n|                      --batch BATCH | int        | Batch size for pre-training and training. Default = 10                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |    no    |\n|                    --pre_lr PRE_LR | int        | Pre-training learning rate. Default = 0.01                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |    no    |\n|                --train_lr TRAIN_LR | int        | Training learning rate. Default = 0.1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |    no    |\n|                  --dropout DROPOUT | int        | Dropout rate. Default = 0.2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |    no    |\n|                          --pca PCA | int [1-2]  | [1] Use PCA<br>[2] Don't use PCA<br>Default = [2] Don't use                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |    no    |\n|              --optimizer OPTIMIZER | int [1-3]  | [1] Stochastic gradient descent<br>[2] RMSProp<br>[3] Adam<br>Default = [1] Stochastic gradient descent                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |    no    |\n\n### Example\nIf we want to perform breast cancer subtype classification based on the dime sion reduced DNA methylation dataset using PCA on TensorFlow platform, one can issue the following command from the terminal: \n`python3 main_run.py --platform 1 --type 1 --dataset 1 --batch 10 --pretrain_epoch 5 --train_epoch 5 --pca 1 --optimizer 3`\n\nIn the preceding command, we define:\n-- 10 as the batch size\n-- 5 as the number of pretraining epoch\n-- 5 is the fine tuning epoch\n-- 3 is the idx for the Adam optimizer. \n\n### #Sample execution: \nCancer type classification with DNA methylation platform GPL8490 with TensorFlow\n\n    ER status prediction\n    -----------------------------\n    [START] Pre-training step:\n    >> Epoch 1 finished     AE Reconstruction error 522.190925\n    >> Epoch 2 finished     AE Reconstruction error 497.765570\n    >> Epoch 3 finished     AE Reconstruction error 492.680869\n    >> Epoch 4 finished     AE Reconstruction error 494.515497\n    >> Epoch 5 finished     AE Reconstruction error 468.050771\n    >> Epoch 1 finished     AE Reconstruction error 2680.144531\n    >> Epoch 2 finished     AE Reconstruction error 2672.767578\n    >> Epoch 3 finished     AE Reconstruction error 2691.162842\n    >> Epoch 4 finished     AE Reconstruction error 2597.989502\n    >> Epoch 5 finished     AE Reconstruction error 2758.419678\n    [END] Pre-training step\n    \n    [START] Fine tuning step:\n    >> Epoch 0 finished     Training loss 0.610027\n    >> Epoch 1 finished     Training loss 0.594821\n    >> Epoch 2 finished     Training loss 0.568818\n    >> Epoch 3 finished     Training loss 0.564796\n    >> Epoch 4 finished     Training loss 0.558171\n    [END] Fine tuning step\n    \n    Accuracy: 0.8786260\n    Precision: 0.861820\n    Recall: 0.878625954\n    F1-score: 0.8692177\n\n    PGR status prediction\n    ---------------------------------\n    [START] Pre-training step:\n    >> Epoch 1 finished     AE Reconstruction error 422.876587\n    >> Epoch 2 finished     AE Reconstruction error 393.641800\n    >> Epoch 3 finished     AE Reconstruction error 377.866021\n    >> Epoch 4 finished     AE Reconstruction error 368.311999\n    >> Epoch 5 finished     AE Reconstruction error 380.356941\n    >> Epoch 1 finished     AE Reconstruction error 2793.383789\n    >> Epoch 2 finished     AE Reconstruction error 2742.516602\n    >> Epoch 3 finished     AE Reconstruction error 2704.654785\n    >> Epoch 4 finished     AE Reconstruction error 2839.105469\n    >> Epoch 5 finished     AE Reconstruction error 2749.048584\n    [END] Pre-training step\n    \n    [START] Fine tuning step:\n    >> Epoch 0 finished     Training loss 0.921267\n    >> Epoch 1 finished     Training loss 0.662474\n    >> Epoch 2 finished     Training loss 0.674687\n    >> Epoch 3 finished     Training loss 0.669110\n    >> Epoch 4 finished     Training loss 0.739354\n    [END] Fine tuning step\n    \n    Accuracy: 0.8694656\n    Precision: 0.848254\n    Recall: 0.869465648\n    F1-score: 0.8569493\n\n    HER2 status prediction\n    ------------------------------\n    [START] Pre-training step:\n    >> Epoch 1 finished     AE Reconstruction error 309.675462\n    >> Epoch 2 finished     AE Reconstruction error 302.142036\n    >> Epoch 3 finished     AE Reconstruction error 294.692107\n    >> Epoch 4 finished     AE Reconstruction error 290.237393\n    >> Epoch 5 finished     AE Reconstruction error 289.501104\n    >> Epoch 1 finished     AE Reconstruction error 1846.207275\n    >> Epoch 2 finished     AE Reconstruction error 1806.483032\n    >> Epoch 3 finished     AE Reconstruction error 1898.162720\n    >> Epoch 4 finished     AE Reconstruction error 1902.564453\n    >> Epoch 5 finished     AE Reconstruction error 1867.702637\n    [END] Pre-training step\n    \n    [START] Fine tuning step:\n    >> Epoch 0 finished     Training loss 1.010514\n    >> Epoch 1 finished     Training loss 0.988286\n    >> Epoch 2 finished     Training loss 0.995581\n    >> Epoch 3 finished     Training loss 0.987776\n    >> Epoch 4 finished     Training loss 0.986907\n    [END] Fine tuning step\n    \n    Accuracy: 0.8613043\n    Accuracy: 0.8612809\n    Precision: 0.875822\n    Recall: 0.861304347\n\n#### Special note ###\nIf you already have the processed datasets without running the `main_download.py`, please add `MAIN_MDBN_TCGA_BRCA = \"main_datasets_folder\"` on the first line of these two files:\n\n* /mdbn_tcga_brca/Tensorflow/dataset_location.py\n* /mdbn_tcga_brca/Theano/dataset_location.py\n\nwith `main_datasets_folder` being the main folder of your datasets.\n\n### Citation request\nIf you use the code of this repository in your research, please consider citing the folowing papers:\n\n    @inproceedings{karim2019MAE,\n        title={Prognostically Relevant Subtypes and Survival Prediction for Breast Cancer Based on Multimodal Genomics Data},\n        author={Karim, Md Rezaul and Beyan Deniz and Decker, Stefan},\n        booktitle={submitted to IEEE Access journal},\n        year={2019}\n    }\n\n### Contributing\nFor any questions, feel free to open an issue or contact at rezaul.karim@rwth-aachen.de\n"
    },
    {
        "repo": "/Jonas1312/PFA-ScanNet",
        "language": "Python",
        "readme_contents": "# PFA-ScanNet\n\nPFA-ScanNet: *Pyramidal Feature Aggregation with Synergistic Learning for Breast Cancer Metastasis Analysis.* [[paper]](https://arxiv.org/abs/1905.01040)\n\n## Abstract\n\nAutomatic detection of cancer metastasis from whole slide images (WSIs) is a crucial step for following patient staging and prognosis. Recent convolutional neural network based approaches are struggling with the trade-off between accuracy and computational efficiency due to the difficulty in processing large-scale gigapixel WSIs.\n\nTo meet this challenge, we propose a novel Pyramidal Feature Aggregation ScanNet (PFA-ScanNet) for robust and fast analysis of breast cancer metastasis. Our method mainly benefits from the aggregation of extracted local-to-global features with diverse receptive fields, as well as the proposed synergistic learning for training the main detector and extra decoder with semantic guidance. Furthermore, a high-efficiency inference mechanism is designed with dense pooling layers, which allows dense and fast scanning for gigapixel WSI analysis.\n\nAs a result, the proposed PFA-ScanNet achieved the state-of-the-art FROC of 90.2% on the Camelyon16 dataset, as well as competitive kappa score of 0.905 on the Camelyon17 leaderboard. In addition, our method shows leading speed advantage over other methods, about 7.2 min per WSI with a single GPU, making automatic analysis of breast cancer metastasis more applicable in the clinical usage.\n\n![Architecture](./figures/net-1.png)\n![PFE and BM](./figures/module-1.png)\n"
    },
    {
        "repo": "/hrsht-13/Breast-Cancer-Detection",
        "language": "Jupyter Notebook",
        "readme_contents": "# Breast-Cancer-Detection\n# Overview\nAmong many cancers, breast cancer is the second most common cause of death in women. Early detection and early treatment reduce breast cancer mortality. Mammography plays an important role in breast cancer screening because it can detect early breast masses or calcification regions.\n## Signs of Breast Cancer:\n#### 1. Weight loss\n#### 2. Skin changes\n#### 3. Pain\n#### 4. Breast changes\n\nThis project uses mammograms for breast cancer detection using deep learning techniques.\n\nFor the diagnosis of breast cancer doctors often use additional tests to find or diagnose breast cancer. ``A mammogram is an X-ray picture of the breast``. Doctors use a mammogram to look for ``early signs of breast cancer``. Regular mammograms are the best tests doctors have to find breast cancer early, sometimes up to three years before it can be felt.\nA mammogram shows ``how dense the breasts are``. Women with dense breasts have a higher risk of getting breast cancer.\n\n# Dataset\nThe dataset contains breast mammography images(224,224,3). With labels of:\n#### 1. Density \nThe levels of density are:\nA: ``(1)`` Almost entirely fatty indicates that the breasts are almost entirely composed of fat. About 1 in 10 women has this result.\nB: ``(2)`` Scattered areas of fibroglandular density indicates there are some scattered areas of density, but the majority of the breast tissue is nondense. About 4 in 10 women have this result.\nC: ``(3)`` Heterogeneously dense indicates that there are some areas of nondense tissue, but that the majority of the breast tissue is dense. About 4 in 10 women have this result.\nD: ``(4)`` Extremely dense indicates that nearly all of the breast tissue is dense. About 1 in 10 women has this result.\n#### 2. Tumour \nA: ``Benign`` (noncancerous)\nB: ``Malignant`` (cancerous)\n\n# Training Images for each Class\n#### Benign \n![alt text](https://github.com/hrsht-13/Breast-Cancer-Detection/blob/main/image/Begign.png)\n#### Malignant\n![alt text](https://github.com/hrsht-13/Breast-Cancer-Detection/blob/main/image/malignant.png)\n# Image Processing\nSince the mammograms looks blury and dull, image preprocessing has been done to increase the sharpness and contrast of the image.\n![atl text](https://github.com/hrsht-13/Breast-Cancer-Detection/blob/main/image/processing.png)\n\n# Why Deep-Learning?\nAI system studying X-ray mammograms was shown to be better than human experts when it came to predicting whether or not a patient has breast cancer. More specifically, the model was found to be as good as two doctors looking at the images, and better at spotting cancer than a single doctor, while also reducing the number of \u201cfalse-negative\u201d results. Such systems will never replace medical staff, but would serve as an extra set of eyes, while also being able to work 24/7 without getting tired or making mistakes.\n\n# Model Deployment using Gradio\n>!git clone https://github.com/hrsht-13/Breast-Cancer-Detection.git\n\n>%cd Breast-Cancer-Detection/\n\n>pip install -r requirements.txt\n\n>!python app.py \n\n###### Open the link to use the app\n\n# App Display\n### 1. Web Page\n![atl text](https://github.com/hrsht-13/Breast-Cancer-Detection/blob/main/image/webpage.png)\n### 2. Prediction\n![atl text](https://github.com/hrsht-13/Breast-Cancer-Detection/blob/main/image/prediction.png)\n### 3. Prediction after cropping image (in the app itself)\n![atl text](https://github.com/hrsht-13/Breast-Cancer-Detection/blob/main/image/after%20cropping.png)\n# Author \n### https://dphi.tech/challenges/data-sprint-31-breast-cancer-detection/75/overview/about\n"
    },
    {
        "repo": "/sirCamp/kaggle-breast-cancer-prediction",
        "language": "Python",
        "readme_contents": "# Different Approaches to predict malignous breast cancers based on Kaggle dataset\nThis project is started with the goal use machine learning algorithms and learn how to optimize the tuning params and also and hopefully to help some diagnoses.\n\nThese are different approaches like:\n + ANN\n + DecisionTree\n + Bayes \n + KNeighbors\n \nMentioned as the goal of the project is to predict the right way if there are a breast cancer or not.\n\nThe whole project is written in Python.\nAll the parameters of algorithms are tuned at best possible and the Reached accuracy is around ~ 94%.\nTo be precise the lower Obtained result is around 90% and the best is over 97% with 94% as mean.\n\nThis different results are caused by the shuffling of the elements. That is Necessary to make the data more \"reals\".\n\nEach algorithm work trainset on 70% of initial dataset and it is tested with the 30%.\n\n\n###How to\nTo run the scripts you just type:\n```python\npython script_name.py\n```\nAs result of execution the reached accuracy will print\n\n* the dataset can be found [here](https://www.kaggle.com/uciml/breast-cancer-wisconsin-data)"
    },
    {
        "repo": "/theArjun/disease-predictor",
        "language": "Python",
        "readme_contents": "# Disease Predictor\n\nA web app for heart disease prediction, diabetes prediction and breast cancer prediciton using Machine Learning based on the Kaggle Datasets. \n\n## Getting Started\n\nThese instructions will get you a copy of the project up and running on your local machine for development and testing purposes.\n\n### Installation\n\nFollow the instructions to run the system on your local machine :\n\nFirst, clone the project and \n```\nhttps://github.com/theArjun/disease-predictor.git\n```\n\nInstall the virtual environment.\n```\npip install virtualenv\n```\nThen,\n```\nvirtualenv myenv\n```\nAfter that, activate the virtual environment\n```\nsource myenv/bin/activate\n```\nSubsequently, install the dependencies on your local machine.\n```\npip install -r requirement.txt\n```\nFinally, run the application.\n```\npython manage.py runserver\n```\n\n## Built With\n\n* [Django](https://www.djangoproject.com/) - The web framework used\n* [PIP](https://pip.pypa.io/en/stable//) - Dependency Management\n\n## Contributing\n\nPlease fork this project and send us a pull request.\n\n## Authors\n\n* **Arjun Adhikari**\n\n\n"
    },
    {
        "repo": "/datasets/breast-cancer",
        "language": "Python",
        "readme_contents": "This is a dataset about breast cancer occurrences.\n\n## Data\n\nThis dataset is taken from [OpenML - breast-cancer](https://www.openml.org/d/13)\n\nThis breast cancer domain was obtained from the University Medical Centre, Institute of Oncology, Ljubljana, Yugoslavia. Thanks go to M. Zwitter and M. Soklic for providing the data. \nPlease include this citation if you plan to use this database.\n\nMatjaz Zwitter & Milan Soklic (physicians) Institute of Oncology University Medical Center Ljubljana, Yugoslavia -- Donors: Ming Tan and Jeff Schlimmer (Jeffrey.Schlimmer@a.gp.cs.cmu.edu) -- Date: 11 July 1988.\n\n* 286 instances\n* 10 attributes\n* Missing values: yes\n\nClass Distribution:\n* no-recurrence-events: 201 instances\n* recurrence-events: 85 instances\n\n### Output data\nOutput data is located in directory `data`\n\n`data/breast-cancer.csv`\n\n## Scripts\n\nScripts for dataset are located in directory `scripts`\n\n`scripts/main.py`\n\n## Licence\nLicensed under the [Public Domain Dedication and License][pddl] (assuming\neither no rights or public domain license in source data).\n\n[pddl]: http://opendatacommons.org/licenses/pddl/1.0/"
    },
    {
        "repo": "/rajat1994/WebApp-for-breast-cancer-detection",
        "language": "Python",
        "readme_contents": "# WebApp-for-breast-cancer-detection\n* A flask application for breast cancer detection (Under Development). The app can tell whether the breast mass is benign or     malignant. It uses the deep neural net classifier to find the pattern in the data.\n* Link of the dataset : http://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Original%29\n* The attributes in the sample are in the range (1-10). So,doctors will enter the values of attributes and clicking on the \n  compute button can tell whether it's benign or malignant.\n* Video : https://www.youtube.com/watch?v=ulB_VYAcY5U  \n\n### Requirements ###\n\n* Python >=2.7 or >=3.4\n* TensorFlow >= 1.0.0\n* Flask\n* Heroku\n\n\n### How to run ###\n\n* $ pip install -r requirements.txt\n* $ gunicorn controller:app --log-file=-\n   \n\n### Deploy to Heroku ###\n\n* $ heroku apps:create [NAME]\n* $ git add .\n* $ git commit -m \"first commit\"\n* $ git push heroku master\n\n\nor Heroku Button\n\n[![Deploy](https://www.herokucdn.com/deploy/button.svg)](https://heroku.com/deploy)\n"
    },
    {
        "repo": "/BodenmillerGroup/SCPathology_publication",
        "language": "MATLAB",
        "readme_contents": "[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3518284.svg)](https://doi.org/10.5281/zenodo.3518284)\n# The Single-Cell Pathology Landscape of Breast Cancer\n\nThis repository contains all code used to produce the results and figures of the publication \"The Single-Cell Pathology Landscape of Breast Cancer\". All data, including tiff images, masks, single-cell and patient data are available on Zenodo (10.5281/zenodo.3518284).\n\n## Matlab scripts:\nImage and other early analysis steps were performed using Matlab. Since the single-cell data was extracted using histoCAT, the Matlab scripts assume a data structure as in a loaded histoCAT session. Saved histoCAT sessions can be downloaded from Zenodo (10.5281/zenodo.3518284).\n\n## R scripts:\nDownstream analysis was performed using R pipelines. The R analysis is divided into one notebook for the analysis of the first TMA of 281 patients from University Hospital Basel and a second one for comparison and analysis of the second multi-core cohort from Univerity Hospital Zurich. All input data required to reproduce the figures of this publication are available on Zenodo(10.5281/zenodo.3518284). The BaselTMA and ZurichTMA folders contain the input data for the respective R pipelines.\n\n## Data organization on Zenodo:\nOMEandSingleCellMasks.zip contains the ome-tiff stacks and the single-cell masks.\nTumorStroma_masks.zip contains masks for tumor and stromal regions.\nSingleCell_and_Metadata.zip contains the single-cell and patient data as well as all other input data for the R pipelines provided here.\n\n| Where to find:                            | Subpath                                                     |\n| ----------------------------------------- | ----------------------------------------------------------- |\n| Patient and core metadata BaselTMA        | SingleCell_and_Metadata/BaselTMA/Basel_PatientMetadata.csv  |\n| Patient and core metadata ZurichTMA       | SingleCell_and_Metadata/ZurichTMA/Zuri_PatientMetadata.csv  |\n| Single-cell data BaselTMA                 | SingleCell_and_Metadata/BaselTMA/SC_dat.csv                 |\n| Single-cell data ZurichTMA                | SingleCell_and_Metadata/ZurichTMA/SC_dat.csv                |\n| Single-cell segmentation masks both TMAs  | OMEandSingleCellMasks/Basel_Zuri_masks/                     |\n| Image tiffs both TMAs                     | OMEandSingleCellMasks/ome/                                  |\n| Antibody panel                            | SingleCell_and_Metadata/Basel_Zuri_StainingPanel.csv        |\n\n### Important notes when working with the data provided on Zenodo: \n- The single-cell data provided for downstream R analysis is already spillover corrected.\n-  The single-cell masks that were generated using CellProfiler do not always contain strictly sequential single-cell labels. Every now and then an ID is skipped due to excluded edge cells. This can cause issues in histoCAT and therefore the single cells are automatically relabelled sequentially during loading into histoCAT. We exported the single-cell data from histoCAT for downstream R analysis and therefore the single-cell labels are the newly assigned sequential ones and match the labels in the histoCAT sessions. However, the original mask files that are also provided here still contain the original labels from CellProfiler. For matching the single-cell data provided here directly to the masks (e.g. for visualization of single-cell features on the image outside of histoCAT), the single-cell labels in the mask need to be relabelled as well or matched based on the rank.\n"
    },
    {
        "repo": "/analokmaus/kaggle-rsna-breast-cancer",
        "language": "Python",
        "readme_contents": "# RSNA Screening Mammography Breast Cancer Detection\n**6th place solution: Team Chiral Mistrals**\n\nRabotniKuma (Hiroshi Yoshihara) part\n\n## Environment\nWe recommend you to use [Kaggle GPU docker v128](https://console.cloud.google.com/gcr/images/kaggle-gpu-images/GLOBAL/python).\n\nConda environment yaml file can be found at `./environment.yaml`.\n\n\n## Data preparation\n1. Download competition dataset and place them at `./input/rsna-breast-cancer-detection/`.\n2. Run image conversion script: `python convert_image.py`\n\n\n## Experiments\nExpriment configs are stored in `./configs.py`. \n### Expriment lists\n| Config name | Description                       | CV    | Public LB | Private LB |\n|-------------|-----------------------------------|-------|-----------|------------|\n| Aug07lr0    | Multi-view model, 1024x512        | 0.493 | 0.64      | 0.46       |\n| Res02lr0    | Multi-view model, 1536x768        | 0.488 | 0.59      | 0.46       |\n| Res02mod2   | Multi-view fusion model, 1536x768 | 0.516 | -         | -          |\n| Res02mod3   | Multi-view fusion model, 1536x768 | 0.525 | 0.63      | 0.48       |\n\n### Run experiments\nMake sure your hardware has at least a total of 48 GB of GPU RAM and run the following: \n```bash\npython train.py --config {config name} --num_works {number of cpu cores to be used}\n```\nPlease modify batch size and learning rate in config file(`./configs.py` ) if your hardware has less GPU RAM.\n\nResults (weights, predictions, training logs) will be export to `./results/{config name}/`.\n"
    },
    {
        "repo": "/NajiAboo/BPSO_BreastCancer",
        "language": "Jupyter Notebook",
        "readme_contents": "# BPSO_BreastCancer\n#Feature optimization using Binary Particle Swarm Optimization\n\nBreast cancer is currently one of the leading causes of cancer-related deaths among women around the world. Although the severity of the disease is undeniable, an efficient early diagnosis of the disease can lead to a much higher chance of survival for the patients. Effective clinical decision support systems could potentially be of very high utility for medical practitioners, in this regard. Here a binary Particle Swarm Optimization (BPSO) based feature selection approach is presented, which can be used to improve the performance of automatic breast cancer prediction. The key idea is to formulate the problem of feature selection in terms of a discrete optimization problem, with appropriate data-driven objective function.\n\n#BINARY PARTICLE SWARM OPTIMIZATION \nPSO is an evolutionary computation technique proposed by Kennedy and Eberhart in 1995. PSO is motivated by social  behaviours  such  as  birds  flocking  and  fish  schooling. The underlying phenomenon of PSO is that knowledge is optimised by social interaction in the population where thinking is not only personal but also social. \n"
    },
    {
        "repo": "/sebastianbk/BreastCancerNeuralNetwork",
        "language": "C#",
        "readme_contents": "Breast Cancer Neural Network in .NET\n====================================\n\nThis project is an example of how to implement a neural network (using back-propagation) in C# (.NET). It is based on James McCaffrey's demo at Build 2014 in San Francisco but, rather than using the Iris sample data set, it uses the Diagnostic Wisconsin Breast Cancer Database.\n\nAll credit goes to James McCaffrey for implementing the back-propagation algorithm. I simply refactored parts of the original code to make it a little tidier and easier to navigate in for other developers.\n\nYou can find the original source code here: http://quaetrix.com/Build2014.html\n\nSample Data Source\n==================\nThe sample data used in this example comes the University of Wisconsin. The features in the data set are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image.\n\nIn this example, the data set has been slightly re-formatted. The CSV file in the project does not include the ID number of each observation. Moreover, the original class variable has been re-encoded in the following manner:\n\n\t2 => 1,0\n\t4 => 0,1\n\nThe original data set can be found here: http://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29\n\na) Creators: \n\n\tDr. William H. Wolberg, General Surgery Dept., University of\n\tWisconsin,  Clinical Sciences Center, Madison, WI 53792\n\twolberg@eagle.surgery.wisc.edu\n\n\tW. Nick Street, Computer Sciences Dept., University of\n\tWisconsin, 1210 West Dayton St., Madison, WI 53706\n\tstreet@cs.wisc.edu  608-262-6619\n\n\tOlvi L. Mangasarian, Computer Sciences Dept., University of\n\tWisconsin, 1210 West Dayton St., Madison, WI 53706\n\tolvi@cs.wisc.edu \n\nb) Donor: Nick Street\n\nc) Date: November 1995\n"
    },
    {
        "repo": "/akshaybahadur21/Breast-Cancer-Neural-Networks",
        "language": "MATLAB",
        "readme_contents": "# Breast Cancer Classifier (Shallow Network) \ud83d\udd2c\n\n[![](https://img.shields.io/github/license/sourcerer-io/hall-of-fame.svg?colorB=ff0000)](https://github.com/akshaybahadur21/Breast-Cancer-Neural-Networks/blob/master/LICENSE.txt)  [![](https://img.shields.io/badge/Akshay-Bahadur-brightgreen.svg?colorB=ff0000)](https://akshaybahadur.com)\n\nThis code helps you classify malignant and benign tumors using Neural Networks.\n\n## Code Requirements \ud83e\udd84\nThe example code is in Matlab ([R2016](https://in.mathworks.com/help/matlab/) or higher will work). \n\n\n## Description \ud83e\uddea\nAn ANN is based on a collection of connected units or nodes called artificial neurons (analogous to biological neurons in an animal brain). Each connection (synapse) between neurons can transmit a signal from one to another. The receiving (postsynaptic) neuron can process the signal(s) and then signal downstream neurons connected to it. In common ANN implementations, the synapse signal is a real number, and the output of each neuron is calculated by a non-linear function of the sum of its input. Neurons and synapses may also have a weight that varies as learning proceeds, which can increase or decrease the strength of the signal that it sends downstream. Further, they may have a threshold such that only if the aggregate signal is below (or above) that level is the downstream signal sent.\n\n<img src=\"https://github.com/akshaybahadur21/Breast-Cancer-Neural-Networks/blob/master/neural.png\">\n\nFor more information, [see](https://en.wikipedia.org/wiki/Artificial_neural_network)\n\n## Notes \ud83d\uddd2\ufe0f\n1) Dataset- UCI-ML\n2) I have used 30 features to classify\n3) Instead of 0=benign and 1=malignant, I have used 1=benign and 2=malignant\n\n## Results \ud83d\udcca\n<img src=\"https://github.com/akshaybahadur21/Breast-Cancer-Neural-Networks/blob/master/cancer_neural.gif\">\n\n## Execution \ud83d\udc09\nTo run the code, type `run cancer.m`\n\n```\nrun cancer.m\n```\n\n## Python  Implementation \ud83d\udc68\u200d\ud83d\udd2c\n- Used a shallow neural net with one hidden layer and 20 units.\n- I have used a linear learning rate decay for decreasing cost without overshooting the minima.\n\n1) Dataset- UCI-ML\n2) I have used 30 features to classify\n3) Instead of 0=benign and 1=malignant, I have used 1=benign and 2=malignant\n\n## Results \ud83d\udcca\n\n<img src=\"https://github.com/akshaybahadur21/Breast-Cancer-Neural-Networks/blob/master/bc_nn.gif\">\n\n## Execution \ud83d\udc09\nTo run the code, type `python B_Cancer_nn.py`\n\n```\npython B_Cancer_nn.py\n```\n\n"
    },
    {
        "repo": "/NeuroSyd/breast-cancer-sub-types",
        "language": "Python",
        "readme_contents": "# AFExNet: An Adversarial Autoencoder for Differentiating Breast Cancer Sub-types and Extracting Biologically Relevant Genes [[Paper]](https://ieeexplore.ieee.org/document/9378938)\n[![License: CC BY 4.0](https://img.shields.io/badge/License-CC%20BY%204.0-lightgrey.svg)](https://creativecommons.org/licenses/by/4.0/)\n[![contribution](https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat)](https://github.com/raktimmondol/breast-cancer-sub-types-classification/pulls)\n![python version](https://img.shields.io/badge/python-2.7%20%7C%203.5%20-green.svg)\n![keras version](https://img.shields.io/badge/keras-2.0.6-brightgreen.svg)\n![tensorflow version](https://img.shields.io/badge/tensorflow-1.13.1-orange.svg)\n![imblearn version](https://img.shields.io/badge/imbalanced--learn-0.4.3-blue.svg)\n\nIn this project, we demonstrate how adversarial auto-encoder (AAE) model can be used to extract the features from high dimensional genetic (omics) data. We evaluated the performance of the model through twelve di\ufb00erent supervised classi\ufb01ers to verify the usefulness of the new features in breast cancer subtypes prediction.\n\n+ For biological insight please follow this link: https://github.com/NeuroSyd/latent-space-discovery\n\n![project_logo_transparent](https://user-images.githubusercontent.com/28592095/56498063-8039da00-6543-11e9-8b4a-a551bad3ed0f.png)\n\n\n\n## Getting Started\n\nThe following instructions will get you a copy of the project up and running on your local machine for development and testing purposes. See the instruction below:\n\n### Prerequisites\n\nThe following libraries are required to reproduce this project:\n\n1) Keras 2.0.6 is recommended but works up to 2.1.2 \n\n2) Keras-adverserial (0.0.3)\nDownload Link: https://github.com/bstriner/keras-adversarial\ninstall by using \"python setup.py install\" \n\n3) Tensorflow (1.13.1)\n\n4) Scikit-Learn (0.20.3)\n\n5) Numpy (1.16.3)\n\n6) Imbalanced-Learn (0.4.3)\n\nSupports both Python 2.7.0 and Python 3.5.6\n\nHyperparameters of classifiers are optimized using TPOT https://github.com/EpistasisLab/tpot\n\n### Directory Layout\n```bash\n\u251c\u2500\u2500 results\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 figures\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 Here figures will be stored\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 result.tsv\n\u251c\u2500\u2500 data\n\u2502\u00a0  \u251c\u2500\u2500 data will be stored here\n\u251c\u2500\u2500 feature_extraction\n\u2502\u00a0  \u251c\u2500\u2500 AAE\n\u2502\u00a0  \u2502\u00a0   \u251c\u2500\u2500 1\n\u2502\u00a0  \u2502\u00a0   \u251c\u2500\u2500 2\n\u2502\u00a0  \u2502\u00a0   \u251c\u2500\u2500 3\n\u2502\u00a0  \u2502\u00a0   \u251c\u2500\u2500 4\n\u2502\u00a0  \u2502\u00a0   \u251c\u2500\u2500 5          # five folder for five fold cross validation\n\u2502\u00a0  \u2502\u00a0   \u251c\u2500\u2500 fine_tuned\n\u2502\u00a0  \u2502\u00a0   \u2502\u00a0   \u251c\u2500\u2500 1\n\u2502\u00a0  \u2502\u00a0   \u2502\u00a0   \u251c\u2500\u2500 2\n\u2502\u00a0  \u2502\u00a0   \u2502\u00a0   \u251c\u2500\u2500 3\n\u2502\u00a0  \u2502\u00a0   \u2502\u00a0   \u251c\u2500\u2500 4\n\u2502\u00a0  \u2502\u00a0   \u2502\u00a0   \u251c\u2500\u2500 5\n\u2502\u00a0  \u251c\u2500\u2500deepAE\n\u2502\u00a0  \u251c\u2500\u2500denoisingAE\n\u2502\u00a0  \u251c\u2500\u2500shallowAE\n\u2502\u00a0  \u251c\u2500\u2500VAE\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 notes.txt\n\u2514\u2500\u2500 .gitignore\n```\nMake sure to keep the directory as following for other feature extraction methods:\n```\n.\n\u251c\u2500\u2500 ...\n\u251c\u2500\u2500 feature_extraction                   \n\u2502\u00a0  \u251c\u2500\u2500 denoisingAE       # same for deepAE, shallowAE and VAE\n\u2502\u00a0  \u2502\u00a0   \u251c\u2500\u2500 1\n\u2502\u00a0  \u2502\u00a0   \u251c\u2500\u2500 2\n\u2502\u00a0  \u2502\u00a0   \u251c\u2500\u2500 3\n\u2502\u00a0  \u2502\u00a0   \u251c\u2500\u2500 4\n\u2502\u00a0  \u2502\u00a0   \u251c\u2500\u2500 5            \n\u2514\u2500\u2500 ...\n````\n### Usage\n\nRun the following to train and fine tune the autoencoder\n\n```\nmain.py\n```\n\nAnd run the following when model already fine tuned\n\n```\nwithout_fine_tuning.py\n```\n\n### Benchmarking Code\n\n```\nimport timeit\nstart_time = timeit.default_timer()\nimport psutil\nimport os\n....................\n....................\n....................\n....... code .......\n....................\n....................\n....................\nstart_time = timeit.default_timer()\n\n\n###### COMPUTATION TIME ########\nprint('Wall Clock Time')\nprint ((end_time - start_time), 'Sec')\ntime=(end_time - start_time)\nminutes = time // 60\ntime %= 60\nseconds = time\nprint(minutes, 'Minutes', seconds,'Seconds')\n\n########  CPU USAGE #######\nprint('CPU Usage') \nprint(psutil.cpu_percent(), '%')\nprint('THE END')\n```\nTo know the MEMORY USAGE please follow the instruction below:\n\nInstall memory profiler library: https://pypi.org/project/memory-profiler/ then run the following command.\n\n```\nmprof run main.py\n```\nFinally see the memory usage by running:\n```\nmprof plot\n```\n\n## Proposed Architecture\n\n![AFExNET](https://user-images.githubusercontent.com/28592095/115665054-821e1a00-a364-11eb-9774-6f72ef5bd589.png)\n\n\n## Datasets\n\n* [cBioPortal](https://www.cbioportal.org/) - Cancer Genomics Datasets\n* [Breast Invasive Carcinoma (TCGA, Cell 2015)](http://www.cbioportal.org/study?id=brca_tcga_pub2015) - Clinical information is used to label various molecular subtypes\n\n``` Breast Invasive Carcinoma (BRCA) ```\n\n| Molecular Subtypes | Number of Patients | Label |\n| ------------------ | ------------------ | ------------ |\n| Luminal A | 304 | 0 |\n| Luminal B | 121 | 1 |\n| Basal & Triple Negetive | 137 | 2 |\n| Her 2 Positive | 43 | 3 |\n\n| Total Number of Samples (Patients) | Total Number of Features (Genes) |\n| :------------------: | :------------------: |\n| 605 | 20439 |\n\n* [Details about Molecular Subtypes of Breast Cancer](https://www.breastcancer.org/symptoms/types/molecular-subtypes)\n\n## Contribution\n\nIf you want to contribute to this project and make it better, your help is very welcome. When contributing to this repository please make a clean pull request.\n\n\n## Acknowledgments\n\n* The proposed architecture is inspired by https://github.com/bstriner/keras-adversarial\n\n## Tech Stack\n![tech_stack_banner](https://user-images.githubusercontent.com/28592095/115676246-358d0b80-a371-11eb-9482-0752d5a27d3f.png)\n\n## Cite Us: \n![alt text](https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png \"Logo Title Text 1\")\nIf you find this code useful in your research, please consider citing:\n```\n@ARTICLE{9378938,\n  author={R. K. {Mondol} and N. D. {Truong} and M. {Reza} and S. {Ippolito} and E. {Ebrahimie} and O. {Kavehei}},\n  journal={IEEE/ACM Transactions on Computational Biology and Bioinformatics}, \n  title={AFExNet: An Adversarial Autoencoder for Differentiating Breast Cancer Sub-types and Extracting Biologically Relevant Genes}, \n  year={2021},\n  volume={},\n  number={},\n  pages={1-1},\n  doi={10.1109/TCBB.2021.3066086}}\n```\n"
    },
    {
        "repo": "/jbustospelegri/breast_cancer_diagnosis",
        "language": "Python",
        "readme_contents": "# Algoritmo de clasificaci\u00f3n de cancer de lesiones en ex\u00e1menes mamogr\u00e1ficos.\n\nEste proyecto persigue el objetivo de crear una herramienta que permita clasificar las lesiones \npresentes en im\u00e1genes mamogr\u00e1ficas como benignas o malignas.\n\nPara la realizaci\u00f3n de esta aplicaci\u00f3n se han utilizado 4 arquitecturas que componen el estado \ndel arte en tareas de clasificaci\u00f3n de imagen como son: _VGG16_, _ResNet50_, _DenseNet121_ y _InceptionV3_.\nLas predicciones realizadas por cada arquitectura son combinadas en un _Random Forest_ para obtener la\npredicci\u00f3n final de cada instancia (probabilidad de que un c\u00e1ncer sea maligno)\n\nLas bases de datos utilizadas para entrenar cada modelo son:\n- CBIS-DDDSM: Disponible en https://wiki.cancerimagingarchive.net/display/Public/CBIS-DDSM\n- INBreast: Disponible en https://www.kaggle.com/martholi/inbreast?select=inbreast.tgz\n- MIAS: Disponible en https://www.kaggle.com/kmader/mias-mammography/version/3?select=Info.txt\n\n**Este repositorio pretende servir de base para que otras personas puedan aprovechar el trabajo realizado\ny unirse en la causa para la lucha contra el c\u00e1ncer de seno.** De este modo, a continuaci\u00f3n se detallar\u00e1 la estructura\ndel repositorio as\u00ed como los objetivos de cada m\u00f3dulo o paquete.\n\n- `bin`: Contiene los archivos necesarios para crear una interfaz gr\u00e1fica a partir de la librer\u00eda `Pyqt5`. Entre estos\ndestaca la carpeta `hooks` con las dependencias necesarias a instalar en la aplicaci\u00f3n.\n\n- `notebooks`: Contiene algunos an\u00e1lisis _adhoc_ para la realizaci\u00f3n de la herramienta (procesado de im\u00e1genes o creaci\u00f3n\nde la combinaci\u00f3n secuencial de clasificadores).\n\n- `src`: Contiene los paquetes y los _scripts_ principales de ejecuci\u00f3n de c\u00f3digo. Este paquete se divide en:\n    - `algoriths`: M\u00f3dulo utilizado para crear las redes neuronales de clasificaci\u00f3n y de segmentaci\u00f3n (_on going_).\n    **En este m\u00f3dulo se deber\u00edan de a\u00f1adir todas aquellas arquitecturas de red nuevas a incluir**. Por otra parte,\n    tambi\u00e9n existen scripts para la creaci\u00f3n secuencial de clasificadores a partir de un _Random Forest_. La generaci\u00f3n\n    de nuevos algor\u00edtmos podr\u00eda introducirse en este script. \n    \n    - `breast_cancer_dataset`: M\u00f3dulo que contiene los scripts utilizados para realizar el procesado de datos de \n    cada set de datos individual (CBIS, MIAS e INBreast). Estos scripts se encuentran en el paquete `databases` del \n    m\u00f3dulo, de modo que **para cualquier base de datos que se desee a\u00f1adir, ser\u00e1 necesario introducir su procesado en este\n    paquete**. Por otra parte, el script _database_generator.py_ crea el set de datos utilizado por los algor\u00edtmos de \n    _deep learning_ utilizados uniendo cada base de datos individual contenida en el paquete `databases`. \n    Asimismo, se aplican t\u00e9cnicas de _data augmentation_ y se realiza el split de datos en entrenamiento y validaci\u00f3n.\n    \n     - `data_viz`: m\u00f3dulo utilizado para generar visualizaciones de los resultados obtenidos por las redes.\n     \n     - `preprocessing`: m\u00f3dulo que contiene las funciones de preprocesado gen\u00e9ricas aplicadas a todos los conjuntos de \n     datos. Adem\u00e1s, contiene las funcionalidades necesarias para estandarizar las im\u00e1genes a formato _png_ o _jpg_.\n     **Cualquier procesado nuevo a a\u00f1adir, deber\u00e1 hacerse en este m\u00f3dulo**.\n     \n     - `static`: m\u00f3dulo que contiene los archivos est\u00e1ticos utilizados para la creaci\u00f3n de la interfaz gr\u00e1fica del \n     programa como ficheros _.css_, _.html_ e im\u00e1genes.\n     \n     - `user_interace`:  m\u00f3dulo utilizado para crear la aplicaci\u00f3n `Pyqt5` de clasificaci\u00f3n de im\u00e1genes de seno.\n     \n     - `utils`: m\u00f3dulo gen\u00e9rico en el cual configurar las rutas de las bases de datos dentro del entorno local desde \n     d\u00f3nde se est\u00e9 ejecutando el aplicativo, as\u00ed como la configuraci\u00f3n de los hiperpar\u00e1metros de las redes neuronales. \n     \n     - `main_train.py`: script utilizado para realizar generar el pipeline de entrenamiento, desde la obtenci\u00f3n de datos\n     hasta la creaci\u00f3n y el entrenamiento de cada modelo.\n     \n     - `main.py`: script utilizado para lanzar la aplicaci\u00f3n final realizada.\n     \nJuntamente con los m\u00f3dulos contenidos en esta descripci\u00f3n, se crear\u00e1n un conjunto de carpetas adicionales. Estas carpetas\nno est\u00e1n contenidas en el repositorio por motivos de capacidad de almacenaje. A continuaci\u00f3n se detallan los m\u00f3dulos y \nsus objetivos:\n\n- `logging`: Carpeta que contendr\u00e1 los logs de ejecuciones del programa, como por ejemplo los errores producidos durante\nel procesado de las imagenes.\n\n- `models`: Carpeta que contendr\u00e1 los modelos almacenados juntamente con las predicciones realizadas durante el entrenamiento. \n\n- `data`: Carpeta que contendr\u00e1 las imagenes de cada set de datos convertidas (sub-directorio _01_CONVERTED_) y \nprocesadas (sub-directorio _02_PROCESED_). Esta carpeta tiene el objetivo de reiterar el proceso de procesado de imagenes\nuna vez realizado."
    },
    {
        "repo": "/jordanvaneetveldt/breast_mass_detection",
        "language": "Python",
        "readme_contents": "# Breast cancer detection in Python \n\nMore info: https://medium.com/towards-data-science/end-to-end-breast-cancer-detection-in-python-part-1-13a1695d455 (1)\n\nTo execute 'create_dataset.py' you should have access to the INbreast dataset that can be requested here: http://medicalresearch.inescporto.pt/breastresearch/index.php/Get_INbreast_Database.\n\nThen you should have the following structure to properly create the dataset:\n\n```.\n\u251c\u2500\u2500 INbreast Release 1.0\n\u2502   \u251c\u2500\u2500 AllDICOMs\n\u2502   \u2502   \u251c\u2500\u2500 20586908_6c613a14b80a8591_MG_R_CC_ANON.dcm\n\u2502   \u2502   \u251c\u2500\u2500 ...\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 AllXML\n\u2502   \u2502   \u251c\u2500\u2500 20586908.xml\n\u2502   \u2502   \u251c\u2500\u2500 ...\n\u2514\u2500\u2500 \u2514\u2500\u2500 \u2514\u2500\u2500 ...\n```\n\nIf you want to use another folder, you just need to modify DCM_PATH and XML_PATH in line 14 and 15 from ```create_dataset.py```. This will create a train, validation and test folder with the same procedure as described in my article (1). You can change the seed in line 19 to create a different dataset. \n\nFinally to reproduce my results, you can train YOLOv4 with my config file: ```yolov4-breast.cfg```.\n"
    },
    {
        "repo": "/AFAgarap/support-vector-machine",
        "language": "Python",
        "readme_contents": "Linear Support Vector Machine (SVM)\n==\n\n![](https://img.shields.io/badge/license-Apache--2.0-blue.svg)\n[![PyPI](https://img.shields.io/pypi/pyversions/Django.svg)]()\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1045859.svg)](https://doi.org/10.5281/zenodo.1045859)\n\n*This is a part of a recently-concluded research: [On Breast Cancer Detection: An Application of Machine Learning Algorithms on the Wisconsin Diagnostic Dataset](http://arxiv.org/abs/1711.07831) (September 2017 - November 2017)* [[code](https://github.com/AFAgarap/wisconsin-breast-cancer)].\n\nSupport vector machine (SVM) was developed by Vapnik, and it has been used in many real-world applications, especially in cases of binary classification.\nIts main objective is to find the optimal hyperplane that separates two classes in a given data _D_. The classification of data is accomplished using the decision function _f(x)_:\n\n![](assets/input.png)\n\n![](assets/decision_function.png)\n\nwhere `{-1,+1}` are the classes of given data. The learning parameters (weights `w`, and biases `b`) are obtained as the solution of the following optimization problem:\n\n![](assets/constrained-svm.png)\n\n![](assets/euclidean-norm.png)\n\n![](assets/constraint-1.png)\n\n![](assets/constraint-2.png)\n\nwhere `||w||_{2}` is the Euclidean norm (also known as the L2-norm), `\\xi` is the cost function, and `C` is the penalty parameter (which may be an arbitrary value or a value obtained through hyper-parameter tuning). The corresponding unconstrained optimization problem is the following:\n\n![](assets/l1-svm.png)\n\nwhere `wx + b` is the function that returns the vector containing the scores for each classes (i.e. the predicted classes). The objective of the equation above is known as the primal form of L1-SVM, with the standard hinge loss. In this project, the L2-SVM variant of SVM was used as it is differentiable, and it provides a more stable result than the L1-SVM.\n\n![](assets/l2-svm.png)\n\nFor this implementation, the SVM was written using Python and TensorFlow (as the machine intelligence library), and the problem tackled is a binary classification of breast cancer using the [Wisconsin diagnostic dataset](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)).\n\nThe official dataset information states the following:\n\n```\nFeatures are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass.\nThey describe the characteristics of the cell nuclei present in the image.\n```\n\nThe following is the class distribution of the dataset:\n\n|Class|Number of instances|\n|-----|-------------------|\n|benign|357|\n|malignant|212|\n\nA total of 569 instances. In this implementation, the classes were `{-1, +1}`, representing the benign class and malignant class respectively.\n\nThe features included in the dataset are the following:\n\n* radius\n* texture\n* perimeter\n* area\n* smoothness\n* compactness\n* concavity\n* concave points\n* symmetry\n* fractal dimension\n\nEach feature had its (1) mean, (2) standard error, and (3) \"worst\" or largest (mean of the three largest values) computed. Hence, the dataset having 30 features.\n\n|Variable|Instances|Shape|\n|--------|---------|-----|\n|x (feature)|569|(569, 30)|\n|y (label)|569|(569)|\n\n## Pre-requisite\n\nIt is recommended that you have Python 3.x (specifically 3.5 or 3.6) installed in your system. Install the Python libraries specified in the following command to run the program.\n\n```buildoutcfg\n$ sudo pip3 install matplotlib numpy sklearn tensorflow\n```\n\nYou may opt to use `tensorflow-gpu` instead of `tensorflow`, it's entirely your choice.\n\n## Usage\n\nFirst, clone the project.\n```\n~$ git clone https://github.com/afagarap/support-vector-machine.git/\n```\n\nProgram parameters.\n\n```buildoutcfg\nusage: main.py [-h] -c SVM_C -n NUM_EPOCHS -l LOG_PATH\n\nSVM built using TensorFlow, for Wisconsin Breast Cancer Diagnostic Dataset\n\noptional arguments:\n  -h, --help            show this help message and exit\n\nArguments:\n  -c SVM_C, --svm_c SVM_C\n                        Penalty parameter C of the SVM\n  -n NUM_EPOCHS, --num_epochs NUM_EPOCHS\n                        number of epochs\n  -l LOG_PATH, --log_path LOG_PATH\n                        path where to save the TensorBoard logs\n```\n\nThen, go to its directory by using `cd`, and run the main program according to your desired parameters.\n```\n~$ cd support-vector-machine\n~/support-vector-machine$ python3 main.py --svm_c 1 --num_epochs 1000 --log_path ./logs\n```\n\n## Sample Result\n\nThe hyper-parameters used in the experiment were assigned by hand, and not through optimization/tuning.\n\n#### Hyper-parameters used for the SVM\n|Hyperparameters|SVM|\n|--------------|------|\n|BATCH_SIZE|4\n|EPOCHS|1000|\n|LEARNING RATE|1e-3|\n|SVM_C|1|\n\n\nTraining accuracy (graph above), and training loss (graph below).\n\n![](assets/loss_and_accuracy.png)\n\nTruncated training loss and training accuracy, with counts of true negative, false negative, true positive, and false positive.\n\n```\nstep[0] train -- loss : 1310.61669921875, accuracy : 0.32500001788139343\nstep[100] train -- loss : 754.3006591796875, accuracy : 0.32500001788139343\nstep[200] train -- loss : 580.3919677734375, accuracy : 0.3499999940395355\n...\nstep[10800] train -- loss : 5.456733226776123, accuracy : 1.0\nstep[10900] train -- loss : 6.086201190948486, accuracy : 0.9749999642372131\nEOF -- training done at step 10999\nValidation accuracy : 0.949999988079071\nTrue negative : 12\nFalse negative : 2\nTrue positive : 26\nFalse positive : 0\n```\n\nConfusion matrix on test data.\n\n![](assets/confusion_matrix.png)\n\n\n#### Standardized Dataset\nThe results above are based on a raw dataset from `sklearn`, i.e. `sklearn.datasets.load_breast_cancer().data`. Now, the following is a sample output based on a standardized dataset (using `sklearn.preprocessing.StandardScaler`):\n\n![](assets/loss_and_accuracy_based_on_standardized_data.png)\n\nTruncated training loss and training accuracy, with counts of true negative, false negative, true positive, and false positive.\n\n```buildoutcfg\nstep[0] train -- loss : 86.02317810058594, accuracy : 0.44999998807907104\nstep[100] train -- loss : 49.41931915283203, accuracy : 0.6250000596046448\nstep[200] train -- loss : 41.406898498535156, accuracy : 0.925000011920929\n...\nstep[10800] train -- loss : 2.045114040374756, accuracy : 1.0\nstep[10900] train -- loss : 6.896279335021973, accuracy : 0.9749999642372131\nEOF -- training done at step 10999\nValidation accuracy : 0.9750000238418579\nTrue negative : 16\nFalse negative : 0\nTrue positive : 23\nFalse positive : 1\n```\n\nConfusion matrix on the standardized test data.\n\n![](assets/confusion_matrix_based_on_standardized_data.png)\n\n## Citation\n\nTo cite the repository/software, kindly use the following BibTex entry:\n```\n@misc{abien_fred_agarap_2017_1045859,\n  author       = {Abien Fred Agarap},\n  title        = {AFAgarap/support-vector-machine v0.1.5-alpha},\n  month        = nov,\n  year         = 2017,\n  doi          = {10.5281/zenodo.1045859},\n  url          = {https://doi.org/10.5281/zenodo.1045859}\n}\n```\n\n## License\n\n```buildoutcfg\nCopyright 2017 Abien Fred Agarap\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n```\n"
    },
    {
        "repo": "/AhmedEnnaime/Salamti",
        "language": "Dart",
        "readme_contents": "\r\n# Salamti\r\n\r\nSalamti is a mobile application that has 2 versions (Patient version , Doctor version). The goal of this application is to facilitate the process of chemotherapy sessions and give the doctors a good management to their appointments and a general idea of the state of his patients. The application has multiple features to help patients to control themeselves without the need to moove to the hospital. The application provides patients with quizzes to know their state and based on this latter the application generates a medical prescription.\r\n\r\n\r\n## Usage/Examples\r\nTo use the application you can use the fake api in the master branch you can run it with this command\r\n\r\n```javascript\r\nnpm run json:server\r\n```\r\nYou can choose your simulator and run the application in your favorite ide (android studio or vs code), i recommend android studion for this application.\r\n\r\nYou can get authentified with one of the Ips and passwords listed in the db.json file because the account creation for patients is available only for doctors or admins.\r\n\r\n## Tech Stack\r\n\r\n**Client:** DART, FLUTTER\r\n\r\n**Server:** NodeJs\r\n\r\n## Authors\r\n\r\n- [@Ahmed Ennaime](https://www.linkedin.com/in/ahmed-ennaime-731171225/)\r\n\r\n\r\n## Feedback\r\n\r\nIf you have any feedback, please reach out to me at ahmedennaime20@gmail.com\r\n\r\n## Support\r\n\r\nFor support, email ahmedennaime20@gmail.com or contact me on Linkedin Ahmed Ennaime.\r\n\r\n"
    },
    {
        "repo": "/vishabh123/vishabh",
        "language": "Python",
        "readme_contents": "# vishabh\nBreast Cancer data analysis\nhttp://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+%28diagnostic%29\n"
    },
    {
        "repo": "/gayathri1462/Breast-Cancer-Detection-Web-App",
        "language": "Jupyter Notebook",
        "readme_contents": "# Breast Cancer Detection Web Application\n\n## A Web Application to predict Breast Cancer using SVM  (Deployed on Heroku)\n\n![alt text](https://github.com/gayathri1462/Breast-Cancer-Detection-Web-App/blob/main/output.jpeg?raw=true)\n\n#### YouTube link to see the working: https://youtu.be/ntSBQy5JriQ\n\n#### Working link: (Deployed on Heroku) https://cancer-flask-app.herokuapp.com/\n\n### 1.Project Requirements or Dependencies\n* Anaconda Python (to get ML Libraries)\n* Pip install flask (For Front-end)\n\n### 2. Load Dataset\nBreast Cancer Wisconsin (Diagnostic) Original Data Set\n\nAttribute Information:\n1.\tSample code number: ID number\n2.\tClump Thickness:1-10\n3.\tUniformity of Cell size:1-10\n4.\tUniformity of Cell shape:1-10\n5.\tMarginal Adhesion:1-10\n6.\tSingle Epithelial Cell Size:1-10\n7.\tBare Nuclei:1-10\n8.\tBland Chromatin:1-10\n9.\tNormal Nucleoli:1-10\n10.\tMitoses:1-10\n11.\tClass: (2 for Benign, 4 for Malignant)\n### 3.Build and Train the model using SVM\nUsing SVM (Support Vector Machines) we build and train a model using human cell records, and classify cells to predict whether the samples are benign or malignant.\n### 4.Flask Creation\nPython app.py\nhttp://127.0.0.1:5000/\n\n1.\tBreast_Cancer_Detection.ipynb \u2014 This contains code for the machine learning model to predict cancer based on the class.\n2.\tapp.py \u2014 This contains Flask APIs that receives cells details through GUI or API calls, computes the predicted value based on our model and returns it\n3.\ttemplates & static  \u2014 This folders contains the HTML template and CSS styling to allow user to enter cells details and displays the predicted output.\n\n### 5.Backend creation using model.pkl file\n\nUse this pretrained model and connect it with our Flask application.\nUse this for prediction for model and to show the output\n\n### 6. Adding form to flask app\n \n### 7.Integrating web application with machine learning backend.\n\n### 8. Deployment on Heroku\n\n"
    },
    {
        "repo": "/Elysian01/Impulse-LifeSaviour",
        "language": "HTML",
        "readme_contents": "# Impulse-LifeSaviour\n\n<ul>\n  <li type = \"square\">The Project focuses on saving life of peoples , by saving time , human errors and by early prediction of diseases or infection</li>\n<br>\n<li type = \"square\">Through this application you can check whether you are experencing from Coronavirus or not , this can widely help the government to identify patients</li>\n<br>\n<li type = \"square\">There are many other feature supported by this application like DISEASE PREDICTION which predicts over 10+ Main diseases based on symptoms</li>\n   \n   <br>\n  <table>\n  <tr>\n    <td>Malaria</td>\n    <td>Hypertension</td>\n    <td>Paralysis</td>\n    <td>Pneumonia</td>\n  </tr>\n   <tr>\n    <td>Dengue</td>\n    <td>Migraine</td>\n    <td>Drug Reaction</td>\n    <td rowspan = 2> Dimorphic hemmorhoids(piles)</td>\n  </tr>\n  \n  <tr>\n    <td>Heart Attack</td>\n    <td>Cervical spondylosis</td>\n    <td>Alcoholic hepatitis</td>\n  </tr>\n</table>\n  <br>\n<li type = \"square\">It also predicts </li>\n<li><ul>\n<li type = \"disc\" >CHRONIC KIDNEY DISEASE</li>\n<li type = \"disc\">BREAST CANCER EARLY PREDICTION</li>\n<li type = \"disc\">HEART DISEASE</li>\n</ul></li>\n\n</ul>\n\n<h3>For BREAST CANCER CODE <a href = \"https://github.com/Elysian01/Breast-Cancer-Early-Prediction\">CLICK HERE</a> </h3>\n\n### [Click here](https://github.com/Elysian01/ML-Projects) to view all of ML code required to build this application.\n\n<h3>Website <a href = \"https://impulse101.herokuapp.com/\">Link</a></h3>\n"
    },
    {
        "repo": "/jnarhan/Breast_Cancer",
        "language": "Jupyter Notebook",
        "readme_contents": "# Detection and Diagnosis of Breast Cancer Using Deep Learning\n\nRepository for CUNY M.Sc. Data Analytics Capstone Project on Breast Cancer detection and diagnosis.\n\n## Overview\n\nThis repository contains source code and results related to our research project investigating breast cancer detection and diagnosis using whole image mammograms.\n\nClassification Metric      | Detection Score | Diagnosis Score\n-------------------------- | --------------- | ---------------\nAccuracy                   | 88.99%          | 78.15%\nSensitivity                | 90.76%          | 75.46%\nSpecificity                | 87.23%          | 80.86%\nPositive Predictive Value  | 87.66%          | 79.87%\nNegative Predictive Value  | 90.42%          | 76.61%\nF1-Score                   | 0.89            | 0.78\n\n\n## Abstract\n\nMammograms are arguably the gold standard in visually screening for breast cancer. However  variability in human tissue and the subtlety of abnormalities can challenge lesion identification. Motivated by the ability of convolutional neural networks to classify images in a wide spectrum of fields, this study presents a pre-processing treatment and neural network architecture for the automated detection of abnormalities (such as masses or micro-calcifications) and the pathology classification of identified lesions (i.e.using diagnosis as either benign or malignant). These task are performed using full mammogram images. The process leverages thresholding, image registration and differencing to improve on detection and diagnosis performance. For both classification objectives, we expose the differenced images to a pre-trained neural network for feature extraction and then train a smaller network on these features for classification as either normal or abnormal and where abnormalities exist, as either benign or malignant. Given the limited mammography data we use a variety of regularization techniques including dropout and kernel normalization to control overfitting.  Experimental results on two public dataset, DDSM and MIAS achieved state-of-the-art results in detection of lesions.\n\n\n## Data Sets\n\nWe used the following publicly available data sets with our research:\n\n[Digital Database of Screening Mammography (DDSM)](http://marathon.csee.usf.edu/Mammography/Database.html)\n\n[Mammographic Image Analysis Society (MIAS)](http://peipa.essex.ac.uk/info/mias.html)\n\n## References\n\n[DDSM Mammography Software](http://marathon.csee.usf.edu/Mammography/software/heathusf_v1.1.0.html)\n\n[Image Registration for Breast Imaging: A Review](https://www.ncbi.nlm.nih.gov/pubmed/17280947)\n\n[Discrimination of Breast Cancer with Microcalcifications on Mammography by Deep Learning](http://www.nature.com/articles/srep27327)\n\n[Representation learning for mammography mass lesion classification with convolutional neural networks](http://www.sciencedirect.com/science/article/pii/S0169260715300110)\n\n...More to come..."
    },
    {
        "repo": "/vasudev-sharma/Breast-Cancer-Detection-using-Artificial-Neural-Networks",
        "language": "MATLAB",
        "readme_contents": "# Detecting Breast Cancer using Neural Nets\n\n## What is the Project all about?\nIn India and over the world, Cancer has become a deadly disease and more and more people are suffering from Cancer and a survey says one in every 30 women suffer from this disease in their lifetime and so basically the project was first thought of because of the increase in cases of breast cancer and one thing which is very important that if we can detect the Cancer at an early stage then there is an increased chances of it getting cured.So this project lays a foundation in making the detection of the cancer automated so that more and more people can get it diagonised early so as get cured.\n\n## How it is implemented?\n\nThe signs of detection are Masses and micro calcification clusters which are important in early detection of breast cancer.\n\nMicro calcification are nothing but tiny mineral deposits within the breast tissue. They look similar to small white colored spots. They may or may not be caused by cancer.\n\nMasses can be many things, including cysts (fluid-filled sacs) and non-cancerous solid tumors, but they could also be cancerous.\n\nThe difficulty in cancer detection is that the abnormalities from normal breast tissues are hard to read because of their subtle appearance and ambiguous margins.Automated tools which can help radiologist in early detection of breast cancer.\n\nFurther we have classified the cancer into three categories after its detection- **Normal, Malignant, Benign**.\n\n## Methodology\n\nWe\u200b \u200bhave\u200b \u200bused\u200b \u200badaptive\u200b \u200bmean\u200b \u200bfilter\u200b \u200bto\u200b \u200bremove\u200b \u200bnoise\u200b \u200bfrom\u200b \u200bimage.\u200b \u200bsince\u200b \u200bit\u200b \u200bis\u200b \u200bbetter among\u200b \u200ball\u200b \u200bthe\u200b \u200bspatial\u200b \u200bfilters\u200b \u200band\u200b \u200bdistinguish\u200b \u200bfine\u200b \u200bdetails\u200b \u200bfrom\u200b \u200bnoise.\u200b\n\u200bThe\u200b \u200bAdaptive Median\u200b \u200bFilter\u200b \u200bperforms\u200b \u200bspatial\u200b \u200bprocessing\u200b \u200bto\u200b \u200bdetermine\u200b \u200bwhich\u200b \u200bpixels\u200b \u200bin\u200b \u200ban\u200b \u200bimage have\u200b \u200bbeen\u200b \u200baffected\u200b \u200bby\u200b \u200bimpulse\u200b \u200bnoise.\u200b \u200bThe\u200b \u200bAdaptive\u200b \u200bMedian\u200b \u200bFilter\u200b \u200bclassifies\u200b \u200bpixels as\u200b \u200bnoise\u200b \u200bby\u200b \u200bcomparing\u200b \u200beach\u200b \u200bpixel\u200b \u200bin\u200b \u200bthe\u200b \u200bimage\u200b \u200bto\u200b \u200bits\u200b \u200bsurrounding\u200b \u200bneighbor\u200b \u200bpixels. \n\nThe\u200b \u200bsize\u200b \u200bof\u200b \u200bthe\u200b \u200bneighborhood\u200b \u200bis\u200b \u200badjustable,\u200b \u200bas\u200b \u200bwell\u200b \u200bas\u200b \u200bthe\u200b \u200bthreshold\u200b \u200bfor\u200b \u200bthe comparison.\u200b \u200bA\u200b \u200bpixel\u200b \u200bthat\u200b \u200bis\u200b \u200bdifferent\u200b \u200bfrom\u200b \u200ba\u200b \u200bmajority\u200b \u200bof\u200b \u200bits\u200b \u200bneighbors,\u200b \u200bas\u200b \u200bwell\u200b \u200bas being\u200b \u200bnot\u200b \u200bstructurally\u200b \u200baligned\u200b \u200bwith\u200b \u200bthose\u200b \u200bpixels\u200b \u200bto\u200b \u200bwhich\u200b \u200bit\u200b \u200bis\u200b \u200bsimilar,\u200b \u200bis\u200b \u200blabeled\u200b \u200bas impulse\u200b \u200bnoise.\n\n\u200bThese\u200b \u200bnoise\u200b \u200bpixels\u200b \u200bare\u200b \u200bthen\u200b \u200breplaced\u200b \u200bby\u200b \u200bthe\u200b \u200bmedian\u200b \u200bpixel\u200b \u200bvalue\u200b \u200bof the\u200b \u200bpixels\u200b \u200bin\u200b \u200bthe\u200b \u200bneighborhood\u200b \u200bthat\u200b \u200bhave\u200b \u200bpassed\u200b \u200bthe\u200b \u200bnoise\u200b \u200blabeling\u200b \u200btest.we\u200b \u200bare initially\u200b \u200bconverting\u200b \u200bthe\u200b \u200bimage\u200b \u200binto\u200b \u200bgrayscale\u200b \u200bimage\u200b \u200busing\u200b \u200brgb2gray()\u200b \u200bfunction\u200b \u200bthen \u200b\u200bapplying\u200b \u200badaptive\u200b \u200bmean\u200b \u200bfiltering\u200b \u200bto\u200b \u200bthe\u200b \u200bresulting\u200b \u200bimage\u200b \u200band\u200b \u200bthen\u200b \u200bconverted\u200b \u200bthe image\u200b \u200binto\u200b \u200bunsigned\u200b \u200binteger\u200b \u200b8\u200b \u200busing\u200b \u200bunit8()\u200b \u200bfunction.\n\n\u200bIn\u200b \u200bthis\u200b \u200bway\u200b \u200bwe\u200b \u200bpreprocessed image.then\u200b \u200bwe\u200b \u200bperformed\u200b \u200bGMM\u200b \u200bsegmentation(Gaussian\u200b \u200bMixture\u200b \u200bModel)\u200b \u200bon\u200b \u200bthe preprocessed\u200b \u200bimage\u200b \u200bwith\u200b \u200bnumber\u200b \u200bof\u200b \u200bregions\u200b \u200b2\u200b \u200band\u200b \u200bnumber\u200b \u200bof\u200b \u200bGMM\u200b \u200bcomponents 2\u200b \u200band\u200b \u200bmaximum\u200b \u200bnumber\u200b \u200biterations\u200b \u200b10.\u200b \u200bwe\u200b \u200bperformed\u200b \u200bk-means\u200b \u200bsegmentation\u200b \u200bwith k=2.\u200b \u200bthen\u200b \u200bwe\u200b \u200bImplemented\u200b \u200bHMRF-EM\u200b \u200b(Hidden\u200b \u200bMarkov\u200b \u200bRandom\u200b \u200bField\u200b \u200bModel)\u200b \u200band its\u200b \u200bExpectation-Maximization\u200b \u200bAlgorithm. \n \n\n**The picture decribes the difference between Malignant and Benign tissues in Breast**\n\n![Difference between Malignant and Benign tissues in Breast](https://raw.githubusercontent.com/st186/Breast-cancer-detection-using-Neural-networks/fca5059fa43d76a5cce9a39968f6b0d5e1051cfd/cancer2.PNG)\n\n## Block Diagram of the Project.\n\n![Preview](https://raw.githubusercontent.com/st186/Breast-cancer-detection-using-Neural-networks/7cccc44b3ce4a51219e97df4f18da6147367996a/cancer.PNG)\n\n## How to make the project work?\n\nOpen the project in matlab and then run **`guidemo.m`** and then a gui mode window will open and then just follow the steps there.For further information check the screenshots.\n\n## NOTE--> To get this project working, kindly install MATLAB's [Wavelet Toolbox](https://in.mathworks.com/products/wavelet.html)\n\n## Screenshots\n\n### After running `guidemo.m` script, follow these screenshots to segment a mammogram image\n\n- **STEP 1: Now you have to browse the image of the mammograms and give it as an input**\n<br><img src = \"https://github.com/vs74/Breast-Cancer-Detection-using-Artificial-Neural-Networks/blob/master/static_files/step_1.png\" hieght=\"900\" align=\"center\" width =\"900\">\n\n- **STEP 2: In this step adaptive mean filtering is done**\n<br><img src = \"https://github.com/vs74/Breast-Cancer-Detection-using-Artificial-Neural-Networks/blob/master/static_files/step_2.png\" hieght=\"900\" align=\"center\" width =\"900\">\n\n- **STEP 3: GMM Segmentation is done**\n<br><img src = \"https://github.com/vs74/Breast-Cancer-Detection-using-Artificial-Neural-Networks/blob/master/static_files/step_3.png\" hieght=\"900\" align=\"center\" width =\"900\">\n\n- **STEP 4: So you can see one as the output in the right side which depicts that the cancer is benign**\n<br><img src = \"https://github.com/vs74/Breast-Cancer-Detection-using-Artificial-Neural-Networks/blob/master/static_files/step_4.png\" hieght=\"900\" align=\"center\" width =\"900\">\n\n\n## Citation\n\nIf you use this work in your research, please cite it as follows:\n\nSharma, V., Rajasekaran, R. K. & Badhrinarayanan, S. (2019). Visualization of Data Mining Techniques for the Prediction of Breast Cancer with High Accuracy Rates. Journal of Computer Science, 15(1), 118-130. https://doi.org/10.3844/jcssp.2019.118.130\n\n Copyright: \u00a9 2019 Vasudev Sharma, Raj Kumar Rajasekaran and Shreya Badhrinarayanan. \n\n"
    },
    {
        "repo": "/yuhaomo/HoVerTrans",
        "language": "Python",
        "readme_contents": "# Hover-Trans: Anatomy-aware HoVer-Transformer for ROI-free Breast Cancer Diagnosis in Ultrasound Images\n![network](https://github.com/yuhaomo/HoVerTrans/blob/main/network.png)\n## Introduction\nThe implementation of: <br>\n[**Hover-Trans: Anatomy-aware HoVer-Transformer for ROI-free Breast Cancer Diagnosis in Ultrasound Images**](https://ieeexplore.ieee.org/document/10015121)\n## Requirements\n- python 3.9\n- Pytorch 1.10.1\n- torchvision 0.11.2\n- opencv-python\n- pandas\n- scipy\n## Setup\n### Installation\nClone the repo and install required packages:\n```\ngit clone https://github.com/yuhaomo/HoVerTrans.git\ncd HoVerTrans\npip install -r requirements.txt\n```\n### Dataset\n-  You can download our dataset ([GDPH&SYSUCC](https://1drv.ms/u/s!AgOtqK2ZncKlgoxsmt-UYbEwMyZY2g?e=INNhyK)) and unpack them into the ./data folder.\n```\n./data\n\u2514\u2500GDPH&SYSUCC\n      \u251c\u2500label.csv\n      \u2514\u2500img\n          \u251c\u2500benign(0).png\n          \u251c\u2500benign(1).png\n          \u251c\u2500benign(2).png\n          \u251c\u2500malignant(0).png\n          \u251c\u2500malignant(1).png\n          ...\n```\n- The format of the label.csv is as follows:\n```\n+------------------+-------+\n| name             | label |\n+------------------+-------+\n| benign(0).png    |   0   |\n| benign(1).png    |   0   |\n| benign(2).png    |   0   |\n| malignant(0).png |   1   |\n| malignant(1).png |   1   |\n...\n```\n### Training\n```\npython train.py --data_path ./data/GDPH&SYSUCC/img --csv_path ./data/GDPH&SYSUCC/label.csv --batch_size 32 --class_num 2 --epochs 250 --lr 0.0001 \n```\n## Citation\nIf you find this repository useful or use our dataset, please consider citing our work:\n```\n@ARTICLE{10015121,\n  author={Mo, Yuhao and Han, Chu and Liu, Yu and Liu, Min and Shi, Zhenwei and Lin, Jiatai and Zhao, Bingchao and Huang, Chunwang and Qiu, Bingjiang and Cui, Yanfen and Wu, Lei and Pan, Xipeng and Xu, Zeyan and Huang, Xiaomei and Li, Zhenhui and Liu, Zaiyi and Wang, Ying and Liang, Changhong},\n  journal={IEEE Transactions on Medical Imaging}, \n  title={HoVer-Trans: Anatomy-aware HoVer-Transformer for ROI-free Breast Cancer Diagnosis in Ultrasound Images}, \n  year={2023},\n  volume={},\n  number={},\n  pages={1-1},\n  doi={10.1109/TMI.2023.3236011}}\n```\n"
    },
    {
        "repo": "/MainakRepositor/Breast-Cancer-Detector",
        "language": "Python",
        "readme_contents": "# Breast Cancer Detection\n\n![home](https://user-images.githubusercontent.com/64016811/218371012-5f116303-8625-4a07-9719-cf04b9fdcbd7.png)\n\n### Problem : \n\nSubstantial support for breast cancer awareness and research funding has helped create advances in the diagnosis and treatment of breast cancer. Breast cancer survival rates have increased, and the number of deaths associated with this disease is steadily declining, largely due to factors such as earlier detection, a new personalized approach to treatment and a better understanding of the disease.\n\n### Solution:\n\nThis Web app will help you to detect whether a person has Breast Cancer by analysing the values of several features using the Decision Tree Classifier.\n\n### Idea: \nBuilding an application that can predict the occurrence of a cardiac arrest or the possible causes of it by indicating the highly relevant factors. \n\n### Layout\n\n```\n\u251c\u2500\u2500\u2500images\n\u251c\u2500\u2500\u2500Tabs\n\u2502   \u2514\u2500\u2500\u2500__pycache__\n|   \u2514\u2500\u2500\u2500 home.py\n|   \u2514\u2500\u2500\u2500 data.py\n|   \u2514\u2500\u2500\u2500 predict.py\n|   \u2514\u2500\u2500\u2500 visualize.py\n|   \u2514\u2500\u2500\u2500 about.py\n\u2514\u2500\u2500\u2500__pycache__\n\u2514\u2500\u2500\u2500 main.py\n\u2514\u2500\u2500\u2500 web_functions.py\n\u2514\u2500\u2500\u2500 requirements.txt\n```\n\n\n"
    },
    {
        "repo": "/Zero-We/PMIL",
        "language": "Python",
        "readme_contents": "# Prototypical multiple instance learning for predicting lymph node metastasis of breast cancer from whole-slide pathological images\n\n<img src=\"https://github.com/Zero-We/PMIL/blob/main/docs/pmil-overview.png\">\n\n\n## Introduction\nComputerized identification of lymph node metastasis (LNM) from whole-slide pathological images (WSIs) can largely benefit the therapy decision and prognosis of breast cancer. Besides the general challenges of computational pathology, including extra high resolution, very expensive fine-grained annotation and significant inter-tumoral heterogeneity, one particular difficulty with this task lies in identifying metastasized tumors with tiny foci (called micro-metastasis). In this study, we introduce a weakly supervised method, called Prototypical Multiple Instance Learning (PMIL), to learn to predict lymph node metastasis of breast cancer from whole slide pathological images with only slide-level class labels. Firstly, PMIL discovers a collection of so-called prototypes from the training data by unsupervised clustering. Secondly, the prototypes are matched against the constitutive patches in the WSI, and the resultant similarity scores are aggregated into a soft-assignment histogram describing the statistical distribution of the prototypes in the WSI, which is taken as the slide features. Finally, WSI classification is accomplished by using the slide features.\n<br/>\n\n## Model\nThe trained model weights and precomputed patch feature vectors are provided here \uff08[[Google Drive]](https://drive.google.com/drive/folders/1kfib8H-4jhNzwj-_LDmUGVtjCv3Lg6zT?usp=sharing) | [[Baidu Cloud]](https://pan.baidu.com/s/1OQJM8Tp7y1RlRIPUKdjqIA) (fzts)\uff09. You can download these files and drag `pmil_model.pth` and `pmil_model_simclr.pth` to  the `model` directory, drag `mil-feat` and `simclr-feat` to the `feat` directory.  \n<br/>\n\n## Dataset\n* **Camelyon16**  \nCamelyon16 is a public challenge dataset of sentinel lymph\nnode biopsy of early-stage breast cancer, which includes 270 H&E-stained WSIs for training and 129 for testing (48 LNM-positive and 81 LNM-negative), collected from two medical centers.   \nDownload from [here](https://camelyon17.grand-challenge.org/Data/).\n\n* **Zbraln**  \nThe Zhujiang Breast Cancer Lymph Node (Zbraln) was created by ourselves. Specifically, we collected 635 H&E-stained glass slides of dissected ALNs.  \nWe only provide a few whole slide images data here due to the privacy policy. [[Google Drive]](https://drive.google.com/drive/folders/1kfib8H-4jhNzwj-_LDmUGVtjCv3Lg6zT?usp=sharing) | [[Baidu Cloud]](https://pan.baidu.com/s/1OQJM8Tp7y1RlRIPUKdjqIA) (fzts)  \n<br/>\n\n## Training  \nThe patch-level feature encoder will be initialized by training the standard instance-space MIL with max-pooling. Part of our code refer to: (Campanella et al., 2019), you can refer to [here](https://github.com/MSKCC-Computational-Pathology/MIL-nature-medicine-2019). And the input data should be stored in dictionary with `torch.save()` in `.ckpt` file format including following keys:  \n* `'slides'`: a list of paths to WSIs.  \n* `'grid'`: a list of patch coordinates tuple (x,y). Size of the list equal to number of slides, and the size of each sublist is equal to the numbers of patches in each slide.  \n* `'target'`: a list of slide-level target.  \n* `'mult'`: scale factor for achieving resolutions different than the ones saved in WSI pyramid file.\n* `'level'`: WSI level to tile the patches.  \n\nYou can run following command to train the standard MAX-MIL model and extract the feature vectors of each patch simultaneously:  \n~~~\npython max-mil.py --save_model --save_index --save_feat\n~~~  \n<br/>\n  \n\nAffinity propagation clustering algorithm is used to capture the typical pathological patterns, which we call prototypes. To obtain the prototypes on Camelyon16 dataset, you can run following command:  \n~~~\npython cluster.py\n~~~  \n<br/>\n  \n\nTrain the PMIL framework that encodes WSI by its compositions in terms of the frequencies of occurence of prototypes found inside. Here, we use patch features match against prototypes to get soft-assignment histogram, and histograms of each patch in WSI will be aggregated by selective pooling module:  \n~~~\npython pmil.py --save_model\n~~~  \n<br/>\n  \n  \n\n## Inference  \nYou can evaluate the performance of PMIL at 40x magnification on Camelyon16 dataset by following command: \n~~~\npython pmil.py --load_model --is_test\n~~~  \n<br/>\n\n## Visualization\nWe illustare the prototype discovery on Camelyon16 dataset here. The above row of images show the discovered prototypes, and the colors of bounding boxes are matched with the colors of each cluster in the below row. The below shows intra-slide patch clustering results on two WSIs, the left is LNM-positive and the right is LNM-negative.  \n<div align=center><img src=\"https://github.com/Zero-We/PMIL/blob/main/docs/prototype-discovery.png\" width=\"800px\"></div>\n<br/>\n\nInterpretablity is important to deep learning based algorithms for medical applications, fow which MIL methods often utilize a so-called heatmap to visualize the contribution of each location in a WSI to the classification decision. And we also illustrate the attention maps obtained by PMIL in the `vis` directory. We can observe that, the attention map can completely highlight the tumor regions, which are consistent with the ground truth annotations.  \n<div align=center><img src=\"https://github.com/Zero-We/PMIL/blob/main/docs/attention-map.png\" width=\"800px\"></div>\n<br/>\n\n## License  \nThis code is made available under the GPLv3 License and is available for non-commercial academic purposes.\n<br/>\n\n## Citation  \nIf you find our work useful in your research or if you use parts of this code please consider citing our paper.  \n~~~\n@article{yu2023prototypical,\n  title={Prototypical multiple instance learning for predicting lymph node metastasis of breast cancer from whole-slide pathological images},\n  author={Yu, Jin-Gang and Wu, Zihao and Ming, Yu and Deng, Shule and Li, Yuanqing and Ou, Caifeng and He, Chunjiang and Wang, Baiye and Zhang, Pusheng and Wang, Yu},\n  journal={Medical Image Analysis},\n  pages={102748},\n  year={2023},\n  publisher={Elsevier}\n}\n~~~  \n"
    },
    {
        "repo": "/akashxg/Mammogram-Image-Classifier",
        "language": "Python",
        "readme_contents": "# TensorFlow Mammogram Image Classifier\nBy using a convolutional neural network (CNN) this program classifies microcalcifications and masses in a mammogram as either *benign* or *malignant*. If there are no masses present in the breast tissue then the mammogram will be classified as *normal*. The biggest challenge in this project was that the amount of mammogram images without any segmentation or overlay available online (publicly) was scarce and not feasible to train an entire CNN on, thus why I trained the last layer of Google's Inception v3 network on pre-segmented image data. The .gitignore contains files that are installed via the retrain.py script that is pulled from TensorFlow.\n![Left](/results/A_1105_1.LEFT_MLO.LJPEG.1_highpass.png) ![Right](/results/A_1363_1.RIGHT_MLO.LJPEG.1_highpass.png)\n\n## Tools & Resources\n- [TensorFlow for Poets](https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#0)\n- [Docker](https://www.docker.com/)\n- [Digital Database for Screening Mammography (DDSM)](http://marathon.csee.usf.edu/Mammography/Database.html)\n\n## Results\n#### Normal image test\n![Normal Test](/results/normal.png)\n\n#### Benign image test\n![Benign Test](/results/benign.png)\n\n#### Malignant image test\n![Malevolent Test](/results/malevolent.png)\n\n## Overall Accuracy\n![Final Test](/results/final.png)\n"
    },
    {
        "repo": "/lucko515/breast-cancer-classification",
        "language": "Jupyter Notebook",
        "readme_contents": "# Breast cancer prediction with Machine Learning\n\nThis is a small project to test custom algorithms on the dataset for breast cancer. In this repository you will find necessary information to get you going with \nthese 3 classifcation algorithms (KNN, Logistic Regression and Naive Bayes)\n\n## Dataset\n\nThis is Wisconsin Dataset for breast cancer but you will find it inside the root folder of this project.\n\n## Install\n\n### &nbsp;&nbsp;&nbsp; Supported Python version\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- Python version used in this project: 3.5+\n\n### &nbsp;&nbsp;&nbsp; Libraries used\n\n> *  [Pandas](http://pandas.pydata.org) 0.18.0\n> *  [Numpy](http://www.numpy.org) 1.10.4\n> *  [Matplotlib](https://matplotlib.org) 1.5.1\n> *  [Scikit-learn](http://scikit-learn.org/stable/) 0.17.1\n\n## Code\n\nEach algorithm tested (or version of it) has its own separate .ipynb file. Each file has its name to tell you what algorithm is used.\n\n## Run\n\nTo run this project you will need some software, like Anaconda, which provides support for running .ipynb files (Jupyter Notebook).\n\nAfter making sure you have that:\n\nFor example if yu want to test vectorized version of KNN you should execute one of these 2 lines in your terminal:\n\n`ipython notebook KNN - Vectorized.ipynb`\n\nor\n\n`jupyter notebook KNN - Vectorized.ipynb`\n\n\n\n## License\n\nMIT License\n\nCopyright (c) 2017 Luka Anicin\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
    },
    {
        "repo": "/forderation/breast-cancer-retrieval",
        "language": "Jupyter Notebook",
        "readme_contents": "## CNN Based Autoencoder Application in Breast Cancer Image Retrieval\n#### IEEE 2021 International Seminar on Intelligent Technology and Its Applications (ISITIA)\n[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/cnn-based-autoencoder-application-in-breast/medical-image-retrieval-on-breakhis)](https://paperswithcode.com/sota/medical-image-retrieval-on-breakhis?p=cnn-based-autoencoder-application-in-breast)\n\n![alt text](./assets/app.png \"RETRIEVAL\")\n\n### Abstract\nContent Based Medical Image Retrieval (CBMIR) is considered as a common technique to retrieve relevant images by comparing the features contained in the query image with the features contained in the image located in the database. Currently, the study related to CBMIR on breast cancer image however remains challenging due to inadequate research in such area. Previous study has a low performance and misinformation emphasizing the feature extraction process. Therefore, this study aims to utilize the CNN based Autoencoder method to minimize misinformation in the feature extraction process and to improve the performance result. The dataset used in this study is the BreakHis dataset. Overall, the results of image retrieval in breast cancer applying the CNN based Autoencoder method achieved higher performance compared to the method used in the previous study with an average precision of 0.9237 in the mainclass dataset category and 0.6825 in the subclass dataset category.\n\n### Usage\n#### this is sample of flow running app based on our paper explanation\n- Make sure you already installed python version at least 3.7\n- Download and place dataset in same project folder\n- If you have any further question don't hesitate to email at kharisma.muzaki@gmail.com\n```\npip install requirements.txt\npython split_image_binary.py\npython split_image_multi_class.py\npython training_binary_sample_400.py\npython training_subclass_sample_400.py\npython retrieval_sample.py\npython graph_sample.py\n```\n### Still cumbersome\n##### Don't worry we have usage with notebook example with already dataset splitted\n##### - [Subclass 40X Magnification Example](notebook/Example_Usage_of_Subclass.ipynb)\n##### - [Binary 40X Magnification Example](notebook/Example_Usage_of_Binary.ipynb)\n\n### Project Structure\nThis is project structured are contained in this git repository\n```\n\ud83d\udce6 projects\n \u2523 \ud83d\udcc2 notebook [collection notebook example usage]\n \u2523 \ud83d\udcc2 utils\n \u2503 \u2523 \ud83d\udcdc conv_auto_encoder.py [core of main model for deep learning feature extraction, you can change our model up to you]\n \u2503 \u2517 \ud83d\udcdc retrieval.py [retrival supporting code]\n \u2523 \ud83d\udcdc core_split.py   [core support for splitting images]\n \u2523 \ud83d\udcdc graph_sample.py     [draw chart from comparison of training]\n \u2523 \ud83d\udcdc retrieval_sample.py     [retrieval sample on 400x Magnification]\n \u2523 \ud83d\udcdc split_image_binary.py   [splitting images based on binary scenario]\n \u2523 \ud83d\udcdc split_image_multi_class.py      [splitting images based on subclass from all available class]\n \u2523 \ud83d\udcdc training_binary_sample_400.py   [training binary scenario example]\n \u2517 \ud83d\udcdc training_subclass_sample_400.py     [training subclass scenario example]\n```\n\n### Assets\n- [Full Paper Here](https://ieeexplore.ieee.org/document/9502205)\n- [Dataset Source](https://web.inf.ufpr.br/vri/databases/breast-cancer-histopathological-database-breakhis/)\n- [Project Raw Version on Google Drive](https://drive.google.com/drive/folders/1oQXs24X8BIA4CqllPUQTk4PWDdTbJREf?usp=sharing)\n\n### Citation\n##### If you are found any useful information from us please support by making citation based on our paper, gracias \u270c\nA. E. Minarno, K. M. Ghufron, T. S. Sabrila, L. Husniah and F. D. S. Sumadi, \"CNN Based Autoencoder Application in Breast Cancer Image Retrieval,\" 2021 International Seminar on Intelligent Technology and Its Applications (ISITIA), 2021, pp. 29-34, doi: 10.1109/ISITIA52817.2021.9502205.\n\n### TODO\n- ~~Usage with python native script~~\n- ~~Usage subclass with notebook~~\n- ~~Usage binary scenario with notebook~~\n- Deployed version as a web service\n\n### Datapath structure used\n```\nbinary_scenario\n\u251c\u2500\u2500 test\n\u2502   \u251c\u2500\u2500 100X\n\u2502   \u2502   \u251c\u2500\u2500 benign\n\u2502   \u2502   \u2514\u2500\u2500 malignant\n\u2502   \u251c\u2500\u2500 200X\n\u2502   \u2502   \u251c\u2500\u2500 benign\n\u2502   \u2502   \u2514\u2500\u2500 malignant\n\u2502   \u251c\u2500\u2500 400X\n\u2502   \u2502   \u251c\u2500\u2500 benign\n\u2502   \u2502   \u2514\u2500\u2500 malignant\n\u2502   \u2514\u2500\u2500 40X\n\u2502       \u251c\u2500\u2500 benign\n\u2502       \u2514\u2500\u2500 malignant\n\u251c\u2500\u2500 train\n\u2502   \u251c\u2500\u2500 100X\n\u2502   \u2502   \u251c\u2500\u2500 benign\n\u2502   \u2502   \u2514\u2500\u2500 malignant\n\u2502   \u251c\u2500\u2500 200X\n\u2502   \u2502   \u251c\u2500\u2500 benign\n\u2502   \u2502   \u2514\u2500\u2500 malignant\n\u2502   \u251c\u2500\u2500 400X\n\u2502   \u2502   \u251c\u2500\u2500 benign\n\u2502   \u2502   \u2514\u2500\u2500 malignant\n\u2502   \u2514\u2500\u2500 40X\n\u2502       \u251c\u2500\u2500 benign\n\u2502       \u2514\u2500\u2500 malignant\n\u2514\u2500\u2500 val\n    \u251c\u2500\u2500 100X\n    \u2502   \u251c\u2500\u2500 benign\n    \u2502   \u2514\u2500\u2500 malignant\n    \u251c\u2500\u2500 200X\n    \u2502   \u251c\u2500\u2500 benign\n    \u2502   \u2514\u2500\u2500 malignant\n    \u251c\u2500\u2500 400X\n    \u2502   \u251c\u2500\u2500 benign\n    \u2502   \u2514\u2500\u2500 malignant\n    \u2514\u2500\u2500 40X\n        \u251c\u2500\u2500 benign\n        \u2514\u2500\u2500 malignant\n```\n```\nsubclass_scenario\n\u251c\u2500\u2500 test\n\u2502   \u251c\u2500\u2500 100X\n\u2502   \u2502   \u251c\u2500\u2500 adenosis\n\u2502   \u2502   \u251c\u2500\u2500 ductal_carcinoma\n\u2502   \u2502   \u251c\u2500\u2500 fibroadenoma\n\u2502   \u2502   \u251c\u2500\u2500 lobular_carcinoma\n\u2502   \u2502   \u251c\u2500\u2500 mucinous_carcinoma\n\u2502   \u2502   \u251c\u2500\u2500 papillary_carcinoma\n\u2502   \u2502   \u251c\u2500\u2500 phyllodes_tumor\n\u2502   \u2502   \u2514\u2500\u2500 tubular_adenoma\n\u2502   \u251c\u2500\u2500 200X\n\u2502   \u2502   \u251c\u2500\u2500 adenosis\n\u2502   \u2502   \u251c\u2500\u2500 ductal_carcinoma\n\u2502   \u2502   \u251c\u2500\u2500 fibroadenoma\n\u2502   \u2502   \u251c\u2500\u2500 lobular_carcinoma\n\u2502   \u2502   \u251c\u2500\u2500 mucinous_carcinoma\n\u2502   \u2502   \u251c\u2500\u2500 papillary_carcinoma\n\u2502   \u2502   \u251c\u2500\u2500 phyllodes_tumor\n\u2502   \u2502   \u2514\u2500\u2500 tubular_adenoma\n\u2502   \u251c\u2500\u2500 400X\n\u2502   \u2502   \u251c\u2500\u2500 adenosis\n\u2502   \u2502   \u251c\u2500\u2500 ductal_carcinoma\n\u2502   \u2502   \u251c\u2500\u2500 fibroadenoma\n\u2502   \u2502   \u251c\u2500\u2500 lobular_carcinoma\n\u2502   \u2502   \u251c\u2500\u2500 mucinous_carcinoma\n\u2502   \u2502   \u251c\u2500\u2500 papillary_carcinoma\n\u2502   \u2502   \u251c\u2500\u2500 phyllodes_tumor\n\u2502   \u2502   \u2514\u2500\u2500 tubular_adenoma\n\u2502   \u2514\u2500\u2500 40X\n\u2502       \u251c\u2500\u2500 adenosis\n\u2502       \u251c\u2500\u2500 ductal_carcinoma\n\u2502       \u251c\u2500\u2500 fibroadenoma\n\u2502       \u251c\u2500\u2500 lobular_carcinoma\n\u2502       \u251c\u2500\u2500 mucinous_carcinoma\n\u2502       \u251c\u2500\u2500 papillary_carcinoma\n\u2502       \u251c\u2500\u2500 phyllodes_tumor\n\u2502       \u2514\u2500\u2500 tubular_adenoma\n\u251c\u2500\u2500 train\n\u2502   \u251c\u2500\u2500 100X\n\u2502   \u2502   \u251c\u2500\u2500 adenosis\n\u2502   \u2502   \u251c\u2500\u2500 ductal_carcinoma\n\u2502   \u2502   \u251c\u2500\u2500 fibroadenoma\n\u2502   \u2502   \u251c\u2500\u2500 lobular_carcinoma\n\u2502   \u2502   \u251c\u2500\u2500 mucinous_carcinoma\n\u2502   \u2502   \u251c\u2500\u2500 papillary_carcinoma\n\u2502   \u2502   \u251c\u2500\u2500 phyllodes_tumor\n\u2502   \u2502   \u2514\u2500\u2500 tubular_adenoma\n\u2502   \u251c\u2500\u2500 200X\n\u2502   \u2502   \u251c\u2500\u2500 adenosis\n\u2502   \u2502   \u251c\u2500\u2500 ductal_carcinoma\n\u2502   \u2502   \u251c\u2500\u2500 fibroadenoma\n\u2502   \u2502   \u251c\u2500\u2500 lobular_carcinoma\n\u2502   \u2502   \u251c\u2500\u2500 mucinous_carcinoma\n\u2502   \u2502   \u251c\u2500\u2500 papillary_carcinoma\n\u2502   \u2502   \u251c\u2500\u2500 phyllodes_tumor\n\u2502   \u2502   \u2514\u2500\u2500 tubular_adenoma\n\u2502   \u251c\u2500\u2500 400X\n\u2502   \u2502   \u251c\u2500\u2500 adenosis\n\u2502   \u2502   \u251c\u2500\u2500 ductal_carcinoma\n\u2502   \u2502   \u251c\u2500\u2500 fibroadenoma\n\u2502   \u2502   \u251c\u2500\u2500 lobular_carcinoma\n\u2502   \u2502   \u251c\u2500\u2500 mucinous_carcinoma\n\u2502   \u2502   \u251c\u2500\u2500 papillary_carcinoma\n\u2502   \u2502   \u251c\u2500\u2500 phyllodes_tumor\n\u2502   \u2502   \u2514\u2500\u2500 tubular_adenoma\n\u2502   \u2514\u2500\u2500 40X\n\u2502       \u251c\u2500\u2500 adenosis\n\u2502       \u251c\u2500\u2500 ductal_carcinoma\n\u2502       \u251c\u2500\u2500 fibroadenoma\n\u2502       \u251c\u2500\u2500 lobular_carcinoma\n\u2502       \u251c\u2500\u2500 mucinous_carcinoma\n\u2502       \u251c\u2500\u2500 papillary_carcinoma\n\u2502       \u251c\u2500\u2500 phyllodes_tumor\n\u2502       \u2514\u2500\u2500 tubular_adenoma\n\u2514\u2500\u2500 val\n    \u251c\u2500\u2500 100X\n    \u2502   \u251c\u2500\u2500 adenosis\n    \u2502   \u251c\u2500\u2500 ductal_carcinoma\n    \u2502   \u251c\u2500\u2500 fibroadenoma\n    \u2502   \u251c\u2500\u2500 lobular_carcinoma\n    \u2502   \u251c\u2500\u2500 mucinous_carcinoma\n    \u2502   \u251c\u2500\u2500 papillary_carcinoma\n    \u2502   \u251c\u2500\u2500 phyllodes_tumor\n    \u2502   \u2514\u2500\u2500 tubular_adenoma\n    \u251c\u2500\u2500 200X\n    \u2502   \u251c\u2500\u2500 adenosis\n    \u2502   \u251c\u2500\u2500 ductal_carcinoma\n    \u2502   \u251c\u2500\u2500 fibroadenoma\n    \u2502   \u251c\u2500\u2500 lobular_carcinoma\n    \u2502   \u251c\u2500\u2500 mucinous_carcinoma\n    \u2502   \u251c\u2500\u2500 papillary_carcinoma\n    \u2502   \u251c\u2500\u2500 phyllodes_tumor\n    \u2502   \u2514\u2500\u2500 tubular_adenoma\n    \u251c\u2500\u2500 400X\n    \u2502   \u251c\u2500\u2500 adenosis\n    \u2502   \u251c\u2500\u2500 ductal_carcinoma\n    \u2502   \u251c\u2500\u2500 fibroadenoma\n    \u2502   \u251c\u2500\u2500 lobular_carcinoma\n    \u2502   \u251c\u2500\u2500 mucinous_carcinoma\n    \u2502   \u251c\u2500\u2500 papillary_carcinoma\n    \u2502   \u251c\u2500\u2500 phyllodes_tumor\n    \u2502   \u2514\u2500\u2500 tubular_adenoma\n    \u2514\u2500\u2500 40X\n        \u251c\u2500\u2500 adenosis\n        \u251c\u2500\u2500 ductal_carcinoma\n        \u251c\u2500\u2500 fibroadenoma\n        \u251c\u2500\u2500 lobular_carcinoma\n        \u251c\u2500\u2500 mucinous_carcinoma\n        \u251c\u2500\u2500 papillary_carcinoma\n        \u251c\u2500\u2500 phyllodes_tumor\n        \u2514\u2500\u2500 tubular_adenoma\n```\n"
    },
    {
        "repo": "/DaemonFG/BreastCancerWisconsin",
        "language": "Python",
        "readme_contents": "# BreastCancerWisconsin\n\u903b\u8f91\u56de\u5f52\u505a\u4e8c\u5206\u7c7b\u8fdb\u884c\u764c\u75c7\u9884\u6d4b(\u57fa\u4e8e\u7ec6\u80de\u7684\u5c5e\u6027\u7279\u5f81)\n\n- \u6570\u636e\u6765\u6e90\uff1ahttp://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data\n\n- \u6570\u636e\u63cf\u8ff0\n1. 699\u6761\u6837\u672c\uff0c\u517111\u5217\u6570\u636e\uff0c\u7b2c\u4e00\u5217\u7528\u8bed\u68c0\u7d22\u7684id\uff0c\u540e9\u5217\u5206\u522b\u662f\u4e0e\u80bf\u7624\u76f8\u5173\u7684\u533b\u5b66\u7279\u5f81\uff0c\u6700\u540e\u4e00\u5217\u8868\u793a\u80bf\u7624\u7c7b\u578b\u7684\u6570\u503c\u3002\n2. \u5305\u542b16\u4e2a\u7f3a\u5931\u503c\uff0c\u7528\u201d?\u201d\u6807\u51fa\u3002\n\n- \u601d\u8def\uff1a\n1. \u83b7\u53d6\u6570\u636e\uff0c\u6307\u5b9a\u5217\u540d\n2. \u7f3a\u5931\u503c\u5904\u7406\n3. \u6570\u636e\u5206\u5272\n4. \u6807\u51c6\u5316\u5904\u7406\n5. \u8bad\u7ec3\u6570\u636e\n6. \u505a\u51fa\u9884\u6d4b\n"
    },
    {
        "repo": "/mrtungleung/breast_cancer",
        "language": "Python",
        "readme_contents": "# breast_cancer\n\n## \u4e73\u817a\u764c\u68c0\u6d4b:\n\u91c7\u7528SVM\u65b9\u6cd5\uff0c\u5bf9\u7f8e\u56fd\u5a01\u65af\u5eb7\u661f\u5dde\u7684\u4e73\u817a\u764c\u8bca\u65ad\u6570\u636e\u96c6\u8fdb\u884c\u5206\u7c7b\uff0c\u6700\u7ec8\u5b9e\u73b0\u4e00\u4e2a\u9488\u5bf9\u4e73\u817a\u764c\u68c0\u6d4b\u7684\u5206\u7c7b\u5668 \n\n\u6570\u636e\u96c6\u6765\u81ea\u7f8e\u56fd\u5a01\u65af\u5eb7\u661f\u5dde\u7684\u4e73\u817a\u764c\u8bca\u65ad\u6570\u636e\u96c6\n\u533b\u7597\u4eba\u5458\u91c7\u96c6\u4e86\u60a3\u8005\u4e73\u817a\u80bf\u5757\u7ecf\u8fc7\u7ec6\u9488\u7a7f\u523a (FNA) \u540e\u7684\u6570\u5b57\u5316\u56fe\u50cf\uff0c\u5e76\u4e14\u5bf9\u8fd9\u4e9b\u6570\u5b57\u56fe\u50cf\u8fdb\u884c\u4e86\u7279\u5f81\u63d0\u53d6\uff0c\u8fd9\u4e9b\u7279\u5f81\u53ef\u4ee5\u63cf\u8ff0\u56fe\u50cf\u4e2d\u7684\u7ec6\u80de\u6838\u5448\u73b0\u3002\u80bf\u7624\u53ef\u4ee5\u5206\u6210\u826f\u6027\u548c\u6076\u6027\u3002\u90e8\u5206\u6570\u636e\u622a\u5c4f\u5982\u4e0b\u6240\u793a\uff1a\n\n![image](https://github.com/mrtungleung/breast_cancer/blob/master/images/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-09-10%20%E4%B8%8A%E5%8D%8810.18.44.png)\n\n\u6570\u636e\u8868\u4e00\u5171\u5305\u62ec\u4e86 32 \u4e2a\u5b57\u6bb5\uff0c\u4ee3\u8868\u7684\u542b\u4e49\u5982\u4e0b\uff1a\n\n![image](https://github.com/mrtungleung/breast_cancer/blob/master/images/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-09-10%20%E4%B8%8A%E5%8D%8810.35.03.png)\n\n\u4e0a\u9762\u7684\u8868\u683c\u4e2d\uff0cmean \u4ee3\u8868\u5e73\u5747\u503c\uff0cse \u4ee3\u8868\u6807\u51c6\u5dee\uff0cworst \u4ee3\u8868\u6700\u5927\u503c\uff083 \u4e2a\u6700\u5927\u503c\u7684\u5e73\u5747\u503c\uff09\u3002\u6bcf\u5f20\u56fe\u50cf\u90fd\u8ba1\u7b97\u4e86\u76f8\u5e94\u7684\u7279\u5f81\uff0c\u5f97\u51fa\u4e86\u8fd9 30 \u4e2a\u7279\u5f81\u503c\uff08\u4e0d\u5305\u62ec ID \u5b57\u6bb5\u548c\u5206\u7c7b\u6807\u8bc6\u7ed3\u679c\u5b57\u6bb5 diagnosis\uff09\uff0c\u5b9e\u9645\u4e0a\u662f 10 \u4e2a\u7279\u5f81\u503c\uff08radius\u3001texture\u3001perimeter\u3001area\u3001smoothness\u3001compactness\u3001concavity\u3001concave points\u3001symmetry \u548cfractal_dimension_mean\uff09\u7684 3 \u4e2a\u7ef4\u5ea6\uff0c\u5e73\u5747\u3001\u6807\u51c6\u5dee\u548c\u6700\u5927\u503c\u3002\u8fd9\u4e9b\u7279\u5f81\u503c\u90fd\u4fdd\u7559\u4e864 \u4f4d\u6570\u5b57\u3002\u5b57\u6bb5\u4e2d\u6ca1\u6709\u7f3a\u5931\u7684\u503c\u3002\u5728 569 \u4e2a\u60a3\u8005\u4e2d\uff0c\u4e00\u5171\u6709 357 \u4e2a\u662f\u826f\u6027\uff0c212 \u4e2a\u662f\u6076\u6027\u3002\n\n\u597d\u4e86\uff0c\u6211\u4eec\u7684\u76ee\u6807\u662f\u751f\u6210\u4e00\u4e2a\u4e73\u817a\u764c\u8bca\u65ad\u7684 SVM \u5206\u7c7b\u5668\uff0c\u5e76\u8ba1\u7b97\u8fd9\u4e2a\u5206\u7c7b\u5668\u7684\u51c6\u786e\u7387\u3002\n\u9996\u5148\u8bbe\u5b9a\u9879\u76ee\u7684\u6267\u884c\u6d41\u7a0b\uff1a\n\n![image](https://github.com/mrtungleung/breast_cancer/blob/master/images/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-09-10%20%E4%B8%8A%E5%8D%8810.44.37.png)\n\n1. \u9996\u5148\u6211\u4eec\u9700\u8981\u52a0\u8f7d\u6570\u636e\u6e90\uff1b2. \u5728\u51c6\u5907\u9636\u6bb5\uff0c\u9700\u8981\u5bf9\u52a0\u8f7d\u7684\u6570\u636e\u6e90\u8fdb\u884c\u63a2\u7d22\uff0c\u67e5\u770b\u6837\u672c\u7279\u5f81\u548c\u7279\u5f81\u503c\uff0c\u8fd9\u4e2a\u8fc7\u7a0b\u4f60\u4e5f\u53ef\u4ee5\u4f7f\u7528\u6570\u636e\u53ef\u89c6\u5316\uff0c\u5b83\u53ef\u4ee5\u65b9\u4fbf\u6211\u4eec\u5bf9\u6570\u636e\u53ca\u6570\u636e\u4e4b\u95f4\u7684\u5173\u7cfb\u8fdb\u4e00\u6b65\u52a0\u6df1\u4e86\u89e3\u3002\u7136\u540e\u6309\u7167\u201c\u5b8c\u5168\u5408\u4e00\u201d\u7684\u51c6\u5219\u6765\u8bc4\u4f30\u6570\u636e\u7684\u8d28\u91cf\uff0c\u5982\u679c\u6570\u636e\u8d28\u91cf\u4e0d\u9ad8\u5c31\u9700\u8981\u505a\u6570\u636e\u6e05\u6d17\u3002\u6570\u636e\u6e05\u6d17\u4e4b\u540e\uff0c\u4f60\u53ef\u4ee5\u505a\u7279\u5f81\u9009\u62e9\uff0c\u65b9\u4fbf\u540e\u7eed\u7684\u6a21\u578b\u8bad\u7ec3\uff1b3. \u5728\u5206\u7c7b\u9636\u6bb5\uff0c\u9009\u62e9\u6838\u51fd\u6570\u8fdb\u884c\u8bad\u7ec3\uff0c\u5982\u679c\u4e0d\u77e5\u9053\u6570\u636e\u662f\u5426\u4e3a\u7ebf\u6027\uff0c\u53ef\u4ee5\u8003\u8651\u4f7f\u7528SVC(kernel=\u2018rbf\u2019) \uff0c\u4e5f\u5c31\u662f\u9ad8\u65af\u6838\u51fd\u6570\u7684 SVM \u5206\u7c7b\u5668\u3002\u7136\u540e\u5bf9\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u7528\u6d4b\u8bd5\u96c6\u8fdb\u884c\u8bc4\u4f30\u3002\n\u6309\u7167\u4e0a\u9762\u7684\u6d41\u7a0b\uff0c\u6211\u4eec\u6765\u7f16\u5199\u4e0b\u4ee3\u7801\uff0c\u52a0\u8f7d\u6570\u636e\u5e76\u5bf9\u6570\u636e\u505a\u90e8\u5206\u7684\u63a2\u7d22\uff1a\n\n![image](https://github.com/mrtungleung/breast_cancer/blob/master/images/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-09-10%20%E4%B8%8A%E5%8D%8810.45.55.png)\n\n\u8fd9\u662f\u90e8\u5206\u7684\u8fd0\u884c\u7ed3\u679c\u3002\n\n![image](https://github.com/mrtungleung/breast_cancer/blob/master/images/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-09-10%20%E4%B8%8A%E5%8D%8810.46.44.png)\n\n\u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u5c31\u8981\u5bf9\u6570\u636e\u8fdb\u884c\u6e05\u6d17\u4e86\u3002\n\u8fd0\u884c\u7ed3\u679c\u4e2d\uff0c\u4f60\u80fd\u770b\u5230 32 \u4e2a\u5b57\u6bb5\u91cc\uff0cid \u662f\u6ca1\u6709\u5b9e\u9645\u542b\u4e49\u7684\uff0c\u53ef\u4ee5\u53bb\u6389\u3002diagnosis \u5b57\u6bb5\u7684\u53d6\u503c\u4e3a B \u6216\u8005 M\uff0c\u6211\u4eec\u53ef\u4ee5\u7528 0 \u548c 1 \u6765\u66ff\u4ee3\u3002\u53e6\u5916\u5176\u4f59\u7684 30 \u4e2a\u5b57\u6bb5\uff0c\u5176\u5b9e\u53ef\u4ee5\u5206\u6210\u4e09\u7ec4\u5b57\u6bb5\uff0c\u4e0b\u5212\u7ebf\u540e\u9762\u7684 mean\u3001se \u548c worst \u4ee3\u8868\u4e86\u6bcf\u7ec4\u5b57\u6bb5\u4e0d\u540c\u7684\u5ea6\u91cf\u65b9\u5f0f\uff0c\u5206\u522b\u662f\u5e73\u5747\u503c\u3001\u6807\u51c6\u5dee\u548c\u6700\u5927\u503c\u3002\n\n![image](https://github.com/mrtungleung/breast_cancer/blob/master/images/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-09-10%20%E4%B8%8A%E5%8D%8810.47.15.png)\n\n\u7136\u540e\u6211\u4eec\u8981\u505a\u7279\u5f81\u5b57\u6bb5\u7684\u7b5b\u9009\uff0c\u9996\u5148\u9700\u8981\u89c2\u5bdf\u4e0b features_mean \u5404\u53d8\u91cf\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u8fd9\u91cc\u6211\u4eec\u53ef\u4ee5\u7528 DataFrame \u7684 corr() \u51fd\u6570\uff0c\u7136\u540e\u7528\u70ed\u529b\u56fe\u5e2e\u6211\u4eec\u53ef\u89c6\u5316\u5448\u73b0\u3002\u540c\u6837\uff0c\u6211\u4eec\u4e5f\u4f1a\u770b\u6574\u4f53\u826f\u6027\u3001\u6076\u6027\u80bf\u7624\u7684\u8bca\u65ad\u60c5\u51b5\u3002\n\n![image](https://github.com/mrtungleung/breast_cancer/blob/master/images/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-09-10%20%E4%B8%8A%E5%8D%8810.47.56.png)\n\n\u8fd9\u662f\u8fd0\u884c\u7684\u7ed3\u679c\uff1a\n\n![image](https://github.com/mrtungleung/breast_cancer/blob/master/images/1.png)\n![image](https://github.com/mrtungleung/breast_cancer/blob/master/images/201600150008.png)\n\n\u70ed\u529b\u56fe\u4e2d\u5bf9\u89d2\u7ebf\u4e0a\u7684\u4e3a\u5355\u53d8\u91cf\u81ea\u8eab\u7684\u76f8\u5173\u7cfb\u6570\u662f 1\u3002\u989c\u8272\u8d8a\u6d45\u4ee3\u8868\u76f8\u5173\u6027\u8d8a\u5927\u3002\u6240\u4ee5\u4f60\u80fd\u770b\u51fa\u6765 radius_mean\u3001perimeter_mean \u548c area_mean \u76f8\u5173\u6027\u975e\u5e38\u5927\uff0ccompactness_mean\u3001concavity_mean\u3001concave_points_mean \u8fd9\u4e09\u4e2a\u5b57\u6bb5\u4e5f\u662f\u76f8\u5173\u7684\uff0c\u56e0\u6b64\u6211\u4eec\u53ef\u4ee5\u53d6\u5176\u4e2d\u7684\u4e00\u4e2a\u4f5c\u4e3a\u4ee3\u8868\u3002\n\u90a3\u4e48\u5982\u4f55\u8fdb\u884c\u7279\u5f81\u9009\u62e9\u5462\uff1f\u7279\u5f81\u9009\u62e9\u7684\u76ee\u7684\u662f\u964d\u7ef4\uff0c\u7528\u5c11\u91cf\u7684\u7279\u5f81\u4ee3\u8868\u6570\u636e\u7684\u7279\u6027\uff0c\u8fd9\u6837\u4e5f\u53ef\u4ee5\u589e\u5f3a\u5206\u7c7b\u5668\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u907f\u514d\u6570\u636e\u8fc7\u62df\u5408\u3002\n\u6211\u4eec\u80fd\u770b\u5230 mean\u3001se \u548c worst \u8fd9\u4e09\u7ec4\u7279\u5f81\u662f\u5bf9\u540c\u4e00\u7ec4\u5185\u5bb9\u7684\u4e0d\u540c\u5ea6\u91cf\u65b9\u5f0f\uff0c\u6211\u4eec\u53ef\u4ee5\u4fdd\u7559 mean \u8fd9\u7ec4\u7279\u5f81\uff0c\u5728\u7279\u5f81\u9009\u62e9\u4e2d\u5ffd\u7565\u6389 se \u548c worst\u3002\u540c\u65f6\u6211\u4eec\u80fd\u770b\u5230 mean \u8fd9\u7ec4\u7279\u5f81\u4e2d\uff0cradius_mean\u3001perimeter_mean\u3001area_mean \u8fd9\u4e09\u4e2a\u5c5e\u6027\u76f8\u5173\u6027\u5927\uff0ccompactness_mean\u3001daconcavity_mean\u3001concave points_mean \u8fd9\u4e09\u4e2a\u5c5e\u6027\u76f8\u5173\u6027\u5927\u3002\u6211\u4eec\u5206\u522b\u4ece\u8fd9 2 \u7c7b\u4e2d\u9009\u62e9 1 \u4e2a\u5c5e\u6027\u4f5c\u4e3a\u4ee3\u8868\uff0c\u6bd4\u5982 radius_mean \u548ccompactness_mean\u3002\n\u8fd9\u6837\u6211\u4eec\u5c31\u53ef\u4ee5\u628a\u539f\u6765\u7684 10 \u4e2a\u5c5e\u6027\u7f29\u51cf\u4e3a 6 \u4e2a\u5c5e\u6027\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a\n\n![image](https://github.com/mrtungleung/breast_cancer/blob/master/images/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-09-10%20%E4%B8%8A%E5%8D%8810.49.28.png)\n\n\u5bf9\u7279\u5f81\u8fdb\u884c\u9009\u62e9\u4e4b\u540e\uff0c\u6211\u4eec\u5c31\u53ef\u4ee5\u51c6\u5907\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\uff1a\n\n![image](https://github.com/mrtungleung/breast_cancer/blob/master/images/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-09-10%20%E4%B8%8A%E5%8D%8810.49.47.png)\n\n\u5728\u8bad\u7ec3\u4e4b\u524d\uff0c\u6211\u4eec\u9700\u8981\u5bf9\u6570\u636e\u8fdb\u884c\u89c4\u8303\u5316\uff0c\u8fd9\u6837\u8ba9\u6570\u636e\u540c\u5728\u540c\u4e00\u4e2a\u91cf\u7ea7\u4e0a\uff0c\u907f\u514d\u56e0\u4e3a\u7ef4\u5ea6\u95ee\u9898\u9020\u6210\u6570\u636e\u8bef\u5dee\uff1a\n\n![image](https://github.com/mrtungleung/breast_cancer/blob/master/images/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-09-10%20%E4%B8%8A%E5%8D%8810.50.20.png)\n\n\u6700\u540e\u6211\u4eec\u53ef\u4ee5\u8ba9 SVM \u505a\u8bad\u7ec3\u548c\u9884\u6d4b\u4e86\uff1a\n\n![image](https://github.com/mrtungleung/breast_cancer/blob/master/images/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-09-10%20%E4%B8%8A%E5%8D%8810.50.37.png)\n\n\u8fd0\u884c\u7ed3\u679c\uff1a\n\n![image](https://github.com/mrtungleung/breast_cancer/blob/master/images/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-09-10%20%E4%B8%8A%E5%8D%8810.50.57.png)\n\n\u51c6\u786e\u7387\u5927\u4e8e 90%\uff0c\u8bf4\u660e\u8bad\u7ec3\u7ed3\u679c\u8fd8\u4e0d\u9519\u3002\n"
    },
    {
        "repo": "/nalamidi/Breast-Cancer-Classification-with-Support-Vector-Machine",
        "language": "Jupyter Notebook",
        "readme_contents": "# Breast-Cancer-Classification-with-Support-Vector-Machine\nIn this study, my task is to classify tumors into malignant or benign using features obtained from several cell images.\n"
    },
    {
        "repo": "/LailaMahmoudi/Breast-Cancer-Predictions-With-SVM",
        "language": "Jupyter Notebook",
        "readme_contents": "# Breast-Cancer-Predictions-With-SVM : Project Overview\n\n\n![](https://mlfjqdsf5ptg.i.optimole.com/iQrIoNc-LQvF_N5U/w:800/h:400/q:69/https://nationaldaycalendar.com/wp-content/uploads/2014/10/Breast-Cancer-Awareness-Month-October-1.jpg)\n\n\n\n**Implementation of SVM Classifier To Perform Classification on the dataset of Breast Cancer Wisconin; to predict if the tumor is cancer or not**.\n\n* Building some plots and graphs to take an overview about what your data looks like.\n\n* Machine Learning Algorithms used in this Notebook: Logistic Regression, Gradient Boosting Classifier, Random Forest Classifier, Decision Tree Classifier, Kneighbours Classifier, XGB Classifier, Supportr vector Classifier\n\n* Evaluating the performance of SVM Classifier by Differents Metrics.\n\n\n# Code and Resources Used \n\n* Python Version: 3.8.3\n\n* Packages : Pandas, Numpy, Matplotlib, Seaborn, Sklearn.\n\n* [Understanding a Classification Report For Your Machine Learning Model](https://medium.com/@kohlishivam5522/understanding-a-classification-report-for-your-machine-learning-model-88815e2ce397).\n\n* [True Positive Rate](https://www.sciencedirect.com/topics/computer-science/true-positive-rate)\n\n* [How to plot an ROC curve in Python](https://www.sciencedirect.com/topics/computer-science/true-positive-rate)\n\n\n# Data\n \n [Breast Cancer Wisconsin (Diagnostic) Data Set](https://www.kaggle.com/uciml/breast-cancer-wisconsin-data)\n\n\n# Look at the dataset\n\n ![difference between Malignant and Benignt](https://gotalktogetherdotcom.files.wordpress.com/2016/05/cancerbenignmalig1.jpg?w=550)\n \n # EDA\n \n * Checking for the correlation \n \n \n  ![](https://www.kaggleusercontent.com/kf/44797707/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..q-QmVsJmOeECtdjQAtzdlQ.mDkpMK_zPBgsiF3lkc-qPYS9IT-vMT1f_Wc9bu42fUF6YaAaZHSvzfTI8b6CAvhekmx9xNH3UNU2ngGrwLREjyzqMutxGxIRcTjSZLwmxxqIsZj8VS1xX0-wXiJtqeM06NUFQ5UCO4Y0sZapgUhto6yn0JDk1mHnIgHDkuOmwA9V9JXUgEKrUGZXWUlicltHeazooTH_sJ3xH9EzkQjsstfQEkZ9vLP91vE8N9xEtVXmcuxXWkmcvm_VNCgkALxO2GVgF63BqjFt4155ULP_GqC6h7Mjtmb7ehhMAMmFGu20DUBbXp5-xe5wHj0ZRvfhFzjUS1XJi6mPpEJ69kkpiNh3_CckVhpi-__eXYQXnqWKi68gQAqWC1_os8dffLmDwVUqXJ62EHCJlyfUGaMKuj_25td5gmCvw8iH1N-df1wAl66eZulVGWx9Ye70zS45KYnoL7aRgEMRg1J6m2nHYI-vJWttXYJWipdOPzUpw-0XUGoEOaTz9cGOyKqFf-gwpy2r84VDU5Mx2Prh7CMpqKtDI_2bk4cgr14puIkU3bME-kgDhyqxmN_KZh5qMr8RM63NtD2RyWOx4AXL_XHXHHynNLM9Ioc74Waz8-0F11PAYCvZuqsBswHDR_ahXx-qgIK1dkQWzyNoBqdapcc6vlSgreMFQcZ-U1jY2JgVKd4.fgAXN29VHTT2EnnJoETb3w/__results___files/__results___26_0.png)\n \n  * plotting the highly correlated pairs\n  \n  ![](https://www.kaggleusercontent.com/kf/44797707/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..q-QmVsJmOeECtdjQAtzdlQ.mDkpMK_zPBgsiF3lkc-qPYS9IT-vMT1f_Wc9bu42fUF6YaAaZHSvzfTI8b6CAvhekmx9xNH3UNU2ngGrwLREjyzqMutxGxIRcTjSZLwmxxqIsZj8VS1xX0-wXiJtqeM06NUFQ5UCO4Y0sZapgUhto6yn0JDk1mHnIgHDkuOmwA9V9JXUgEKrUGZXWUlicltHeazooTH_sJ3xH9EzkQjsstfQEkZ9vLP91vE8N9xEtVXmcuxXWkmcvm_VNCgkALxO2GVgF63BqjFt4155ULP_GqC6h7Mjtmb7ehhMAMmFGu20DUBbXp5-xe5wHj0ZRvfhFzjUS1XJi6mPpEJ69kkpiNh3_CckVhpi-__eXYQXnqWKi68gQAqWC1_os8dffLmDwVUqXJ62EHCJlyfUGaMKuj_25td5gmCvw8iH1N-df1wAl66eZulVGWx9Ye70zS45KYnoL7aRgEMRg1J6m2nHYI-vJWttXYJWipdOPzUpw-0XUGoEOaTz9cGOyKqFf-gwpy2r84VDU5Mx2Prh7CMpqKtDI_2bk4cgr14puIkU3bME-kgDhyqxmN_KZh5qMr8RM63NtD2RyWOx4AXL_XHXHHynNLM9Ioc74Waz8-0F11PAYCvZuqsBswHDR_ahXx-qgIK1dkQWzyNoBqdapcc6vlSgreMFQcZ-U1jY2JgVKd4.fgAXN29VHTT2EnnJoETb3w/__results___files/__results___28_0.png)\n  \n  # Model Building\n  \n  * In this section,  I tried different models and evaluate them using the Accuracy_Score:\n  \n   + **Logistic Regression** \n   + **Gradient Boosting Classifier**\n   + **Random Forest Classifier**\n   + **Decision Tree Classifier**\n   + **Kneighbours Classifier**\n   + **XGB Classifier**\n   + **Supportr vector Classifier**\n\n\n![](https://www.kaggleusercontent.com/kf/44797707/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..q-QmVsJmOeECtdjQAtzdlQ.mDkpMK_zPBgsiF3lkc-qPYS9IT-vMT1f_Wc9bu42fUF6YaAaZHSvzfTI8b6CAvhekmx9xNH3UNU2ngGrwLREjyzqMutxGxIRcTjSZLwmxxqIsZj8VS1xX0-wXiJtqeM06NUFQ5UCO4Y0sZapgUhto6yn0JDk1mHnIgHDkuOmwA9V9JXUgEKrUGZXWUlicltHeazooTH_sJ3xH9EzkQjsstfQEkZ9vLP91vE8N9xEtVXmcuxXWkmcvm_VNCgkALxO2GVgF63BqjFt4155ULP_GqC6h7Mjtmb7ehhMAMmFGu20DUBbXp5-xe5wHj0ZRvfhFzjUS1XJi6mPpEJ69kkpiNh3_CckVhpi-__eXYQXnqWKi68gQAqWC1_os8dffLmDwVUqXJ62EHCJlyfUGaMKuj_25td5gmCvw8iH1N-df1wAl66eZulVGWx9Ye70zS45KYnoL7aRgEMRg1J6m2nHYI-vJWttXYJWipdOPzUpw-0XUGoEOaTz9cGOyKqFf-gwpy2r84VDU5Mx2Prh7CMpqKtDI_2bk4cgr14puIkU3bME-kgDhyqxmN_KZh5qMr8RM63NtD2RyWOx4AXL_XHXHHynNLM9Ioc74Waz8-0F11PAYCvZuqsBswHDR_ahXx-qgIK1dkQWzyNoBqdapcc6vlSgreMFQcZ-U1jY2JgVKd4.fgAXN29VHTT2EnnJoETb3w/__results___files/__results___65_1.png)\n\n# Model Performance\n\nIn this step, I evaluate the performance of the models using:\n\n* **Accuracy_Score**\n* **Recall**\n* **Precision**\n* **Classification Report**\n* **The ROC Curve**\n\n\n\n\n  \n"
    },
    {
        "repo": "/yala/Tempo",
        "language": "Python",
        "readme_contents": "# Tempo: Optimizing risk-based breast cancer screening policies with reinforcement learning  [![DOI](https://zenodo.org/badge/419388269.svg)](https://zenodo.org/badge/latestdoi/419388269)\n\n# Introduction\nThis repository was used to develop Tempo, as described in: [Optimizing risk-based breast cancer screening policies with reinforcement learning](https://www.nature.com/articles/s41591-021-01599-w)\n\nScreening programs must balance the benefits of early detection against the costs of over screening. Here, we introduce a novel reinforcement learning-based framework for personalized screening, Tempo, and demonstrate its efficacy in the context of breast cancer. We trained our risk-based screening policies on a large screening mammography dataset from Massachusetts General Hospital (MGH) USA and validated them on held-out patients from MGH, and on external datasets from Emory USA, Karolinska Sweden and Chang Gung Memorial Hospital (CGMH) Taiwan. Across all test sets, we found that a Tempo policy combined with an image-based AI risk model, Mirai [1] was significantly more efficient than current regimes used in clinical practice in terms of simulated early detection per screen frequency. Moreover, we showed that the same Tempo policy can be easily adapted to a wide range of possible screening preferences, allowing clinicians to select their desired early detection to screening cost trade-off without training new policies. Finally, we demonstrated Tempo policies based on AI-based risk models out performed Tempo policies based on less accurate clinical risk models. Altogether, our results show that pairing AI-based risk models with agile AI-designed screening policies has the potential to improve screening programs,  advancing early detection while reducing over-screening.\n\nThis code base is meant to provide exact implementation details for the development of Tempo.\n\n## Aside on Software Depedencies\nThis code assumes python3.6 and a Linux environment.\nThe package requirements can be install with pip:\n\n`pip install -r requirements.txt`\n\nTempo-Mirai assumes access to Mirai risk assessments. Resources for using Mirai are shown [here](https://github.com/yala/mirai).\n\n# Method\n![method](figure2.png)\n\nOur full framework, named Tempo, is depicted above.  As described above, we first train a risk progression neural network to predict future risk assessments given previous assessments. This model is then used to estimate patient risk at unobserved timepoints and it enables us to simulate risk-based screening policies. Next, we train our screening policy, which is implemented as a neural network, to maximize the reward (i.e combination of early detection and screening cost) on our retrospective training set. We train our screening policy to support all possible early detection vs screening cost trade-offs using envelope Q-learning [2], an RL algorithm designed to balance multiple objectives. The input of our screening policies is the patient's risk assessment, and desired weighting between rewards (i.e screening preference). The output of the policy is a recommendation for when to return for the next screen, ranging from six months to three years in the future, in multiples of six months. Our reward balances two contrasting aspects, one reflecting the imaging cost, i.e., the average mammograms a year recommended by the policy, and one modeling early detection benefit relative to the retrospective screening trajectory. Our early detection reward measures the time difference in months between each patient's recommended screening date, if it was after their last negative mammogram, and their actual diagnosis date. We evaluate screening policies by simulating their recommendations for heldout patients.\n\n## Training Risk progression models\nWe experimented with different learning rates, hidden sizes, numbers of layers and dropout, and chose the model that obtained the lowest validation KL divergence on the MGH validation set. Our final risk progression RNN had two layers, a hidden dimension size of 100, a dropout of 0.25, and was trained for 30 epochs with a learning rate of 1e-3 using the Adam optimizer.\n\nTo reproduce our grid search for our Mirai risk progression model, you can run:\n```\npython scripts/dispatcher.py --experiment_config_path configs/risk_progression/gru.json\n```\n\nGiven a trained risk progression model, we can now estimate unobserved risk assessments auto-regressively. At each time step, the model takes as input the previous risk assessment, the prior hidden state, using the previous predicted assessment if the real one is not available, and predicts the risk assessment at the next time step.\n\n## Training Tempo Personalized Screening Policies\n\nWe implemented our personalized screening policy as multiple layer perceptron, which took as input a risk assessment and weighting between rewards and predicted the Q-value for each action, i.e follow up recommendation, across the rewards. This network was trained using Envelope Q-Learning [2]. We experimented with different numbers of layers, hidden dimension sizes, learning rates, dropouts, exploration epsilons, target network reset rates and weight decay rates.\n\nTo reproduce our grid search for our Mirai risk progression model, you can run:\n```\npython scripts/dispatcher.py --experiment_config_path configs/screening/neural.json\n```\n\n\n### Data availability\nAll datasets were used under license to the respective hospital system for the current study and are not publicly available. To access the MGH dataset, investigators should reach out to C.L. to apply for an IRB approved research collaboration and obtain an appropriate Data Use Agreement. To access the Karolinska dataset, investigators should reach out to F.S. to apply for an approved research collaboration and sign a Data Use Agreement. To access the CGMH dataset, investigators should contact G.L. to apply for an IRB approved research collaboration. To access the Emory dataset, investigators should reach out to H.T to apply for an approved collaboration.\n\n\n### References\n[1] Yala, Adam, et al. \"Toward robust mammography-based models for breast cancer risk.\" Science Translational Medicine 13.578 (2021).\n\n\n[2] Yang, Runzhe, Xingyuan Sun, and Karthik Narasimhan. \"A generalized algorithm for multi-objective reinforcement learning and policy adaptation.\" arXiv preprint arXiv:1908.08342 (2019).\n\n## Citing Tempo\n```\n@article{yala2021optimizing,\n  title={Optimizing risk-based breast cancer screening policies with reinforcement learning},\n  author={Yala, Adam and Mikhael, Peter and Lehman, Constance and Lin, Gigin and Strand, Fredrik and Wang, Yung-Liang and Hughes, Kevin and Satuluru, Siddharth and Kim, Thomas and Banerjee, Imon and others},\n  year={2021}\n}\n```\n"
    },
    {
        "repo": "/scottykwok/bach2018",
        "language": null,
        "readme_contents": "# bach2018\nThis repo contains the materials related to \"BACH: Grand Challenge on Breast Cancer Histology Images\"\n\n1. The wrap-up paper prepared by organizers: https://arxiv.org/abs/1808.04277\n2. My paper: https://github.com/scottykwok/bach2018/blob/master/Multiclass%20Classification%20of%20Breast%20Cancer%20in%20Whole%20slide%20Images.pdf\n3. My PyCon HK 2018 talk:  https://github.com/scottykwok/bach2018/blob/master/PyCon2018_slides.pdf \n4. Opensource, tba.\n\n"
    },
    {
        "repo": "/gholste/breast_mri_fusion",
        "language": "Python",
        "readme_contents": "# breast_mri_fusion\n\nAuthor: Greg Holste, Adam Alessio<br/>\nLast Modified: 08/16/21\n\n--------------\n\n## Description\n\nCode for [CVAMD 2021](https://sites.google.com/view/CVAMD2021/) paper, \"End-to-End Learning of Fused Image and Non-Image Feature for Improved Breast Cancer Classification from MRI\" by Gregory Holste, Savannah Partridge, Habib Rahbar, Debosmita Biswas, Christoph Lee, and Adam Alessio.\n\n<!-- ![](figs/fusion_architectures_all_v11.png) -->\n<p align=center>\n    <img src=figs/fusion_architectures_all_v11.png height=600>\n</p>\n\nWhile clinicans integrate imaging findings with a variety of clinical data to make diagnostic decisions, deep-learned approaches to automatic diagnosis often only utilize imaging or only utilize clinical data. In this work, we explore methods to learn jointly from breast MRI imaging and associated non-image data in an end-to-end trainable manner. Our experiments evaluate fusion at various stages in the model (fusing intermediate learned features vs. probabilities) and with different fusion operations (concatenation vs. addition vs. multiplication). These approaches were validated on a dataset of over 15,000 dynamic contrast-enhanced MRI (DCE-MRI) iamges and associated non-image data (e.g., mammographic breast density and clinical indication) collected at the University of Washington to predict year-to-date breast cancer status. We found that all multimodal fusion approaches significantly outperformed unimodal baselines, and that fusion of intermediate learned features outperforms fusion of final probabilities.\n\n## Usage\n\nAssuming you have Anaconda installed, run `conda env create -f mri_fusion.yaml` to install all pre-requisites. The pipeline used for this work was simply to preprocess the data (`preprocess.py`) and train models (`train.py`); you can find the specific commands to conduct all experiments in `run_experiments.sh`. After training, we conducted a feature importance analysis (`feature_imp.py`) to understand which non-image features were most influential to breast cancer prediction. Lastly, scripts to produce figures and perform significance tests comparing models can be found in the `analysis/` directory. Results from the main experiments (the \"best run\" of the two unimodal baselines and the three main fusion models) can be found in the `results/` directory.\n\n**To use this repository on your own multimodal binary classification dataset,** you must prepare the data in the following file structure:\n```bash\n\u251c\u2500\u2500 <your_dataset>\n\u2502   \u251c\u2500\u2500 Train\n\u2502   \u2502   \u251c\u2500\u2500 1_x.npy\n\u2502   \u2502   \u251c\u2500\u2500 1_y.npy\n\u2502   \u2502   \u251c\u2500\u2500 1_meta.npy\n\u2502   \u2502   \u251c\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 Val\n\u2502   \u2502   \u251c\u2500\u2500 1_x.npy\n\u2502   \u2502   \u251c\u2500\u2500 1_y.npy\n\u2502   \u2502   \u251c\u2500\u2500 1_meta.npy\n\u2502   \u2502   \u251c\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 Test\n\u2502   \u2502   \u251c\u2500\u2500 1_x.npy\n\u2502   \u2502   \u251c\u2500\u2500 1_y.npy\n\u2502   \u2502   \u251c\u2500\u2500 1_meta.npy\n\u2502   \u2502   \u251c\u2500\u2500 ...\n```\nwhere `*_x.npy` contains a preprocessed image of shape `(h, w, c)`, `*_y.npy` contains the associated target of shape `(1,)`, and `*_meta.npy` contains the associated non-image data of shape `(n_features,)`. (This repository could easily be adapted to perform multi-class classification by tweaking the output layers and activations of the architectures in `models.py`.)\n\nOnce this is complete, you can simply run `train.py` with the arguments of your choice; for example, if you wanted to train the Learned Feature Fusion model on your dataset with the settings used in the paper, you might run\n```python\npython train.py --data_dir <path_to_your_dataset> \\\n                --out_dir <path_to_results> \\\n                --model learned-feature-fusion \\\n                --fusion_mode concat \\\n                --n_TTA 5 \\\n                --augment \\\n                --use_class_weights \\\n                --label_smoothing 0.1\n```\n\nAlternatively, you could use your own data loader and directory structure and simply import the model of your choosing from `models.py` in a custom training script.\n"
    },
    {
        "repo": "/ab93/Detection-of-Breast-Cancer-from-mammogram-images",
        "language": "Matlab",
        "readme_contents": "# Detection-of-Breast-Cancer-from-mammogram-images\nAttempt to detect potential breast cancer from mammograms (MIAS database)\n\nPlease refer and cite the MIAS database for future work. (http://peipa.essex.ac.uk/info/mias.html)\n\nSteps involved in this project are:\n- Generate labels of the mammogram images\n- Preprocess the images\n- Perform statistical feature extraction\n- Divide the images into test and training data\n- Classify the images using AdaBoost Classifier\n\nNote: \n- The images are not included in the repository. Please go to http://peipa.essex.ac.uk/info/mias.html to obtain the images.\n- Python and MATLAB are used in this project.\n"
    },
    {
        "repo": "/cyc1am3n/HeLP2019_Breast_Cancer_1st_solution",
        "language": "Python",
        "readme_contents": "# HeLP Challenge 2019 Breast Cancer 1st place solution\n\nThis repository is **1st place solution** to the **Breast Cancer Classification Task of HeLP Challenge 2019**.  \n![task_description](./assets/task_description.png)\n\n\n## Model\n![model_description](./assets/model_description.png)\n### Stage 1\n- Preprocessing: ROI extraction, Rescale, Vahadane Stain Normalization\n- Pixel-wise Segmentation: Feature Pyramid Network(FPN)\n### Stage 2\n- Feature extraction from probability heatmap\n- Prediction final probability and major axis based on features\n\nAnd also, please click [this link](./assets/slide.pdf) to see the detailed model description.\n\n## Dependencies\n- keras\n- segmentation_models\n- openslide\n- staintools\n- numpy\n- pandas\n- sklearn\n- skimage\n\n## Usage\n\n### Dataset\n\n```bash\ndata\n  \u2514\u2500\u2500 train\n     \u251c\u2500\u2500 level4\n     \u2502  \u251c\u2500\u2500 Image\n     \u2502  \u2502  \u251c\u2500\u2500 slide_001.png\n     \u2502  \u2502  \u251c\u2500\u2500 ...\n     \u2502  \u2502  \u2514\u2500\u2500 slide_#.png\n     \u2502  \u2514\u2500\u2500 Mask\n     \u2502     \u251c\u2500\u2500 mask_001.png\n     \u2502\t   \u251c\u2500\u2500 ...\n     \u2502\t   \u2514\u2500\u2500 mask_#.png\n     \u2514\u2500\u2500 label.csv\n            \n========= After training, the directories are created as below. =========\n\n  \u251c\u2500\u2500 volume\n  \u2502  \u251c\u2500\u2500 dataset\n  \u2502  \u2502  \u2514\u2500\u2500 level4 \n  \u2502  \u2502     \u251c\u2500\u2500 img\n  \u2502  \u2502\t   \u2502  \u251c\u2500\u2500 slide001_patch001.png\n  \u2502  \u2502 \t   \u2502  \u251c\u2500\u2500 ...\n  \u2502  \u2502     \u2502  \u2514\u2500\u2500 slide#_patch#.png\n  \u2502  \u2502\t   \u2514\u2500\u2500 mask\n  \u2502  \u2502\t      \u251c\u2500\u2500 mask001_patch001.png\n  \u2502  \u2502        \u251c\u2500\u2500 ...\n  \u2502  \u2502        \u2514\u2500\u2500 mask#_patch#.png\n  \u2502  \u2514\u2500\u2500 model\n  \u2502       \u2514\u2500\u2500 fpn_weights.h5\n  \u2514\u2500\u2500 heatmap\n      ...\n```\n\n\n\n### Train\nRun the `train.py`.  \n```bash\n$ python train.py\n```\n### Inference\nRun the `inference.sh`.\n```bash\n$ sh inference.sh\n```\n\n## Authors\n- Daeyoung Kim / [@cyc1am3n](https://github.com/cyc1am3n)  \n- Taewoo Kim / [@Taeu](https://github.com/Taeu)  \n- Jonghyun Choi / [@ExcelsiorCJH](https://github.com/ExcelsiorCJH)\n"
    },
    {
        "repo": "/lishen/dream2016_dm",
        "language": "Python",
        "readme_contents": "# Breast cancer diagnosis using deep residual nets and transfer learning\n\nLi Shen\n\nIcahn School of Medicine at Mount Sinai\n\nNew York, New York, USA\n\n## About\nThis repository is originally from my entry in the DREAM 2016 Digital Mammography challenge. In the challenge, I have developed a classifier based on residual net + probabilistic heatmap + gradient boosting trees for breast cancer diagnosis. After the challenge ended, I have switched to other methods for the same task and much improved the result. The repository represents the method I used for the challenge but does not reflect further changes.\n\nFor my entry in the challenge, see: https://www.synapse.org/LiShenDMChallenge. I ended up ranking 12 in sub-challenge 1 and 10 in sub-challenge 2 out of more than 100 teams.\n\n\n## Overview\nUnder the current folder are several Python modules that are used by both training and inference. The training and inference code are under specific sub-folders as described below.\n\nDevelopment Environment: CUDA 8 + Python 2.7 + Tensorflow + Keras 1.x\n\n## Training\n### Patch classifier training on CBIS-DDSM\nUnder [ddsm_train](./ddsm_train) folder:\n\n[sample_patches_combined.py](./ddsm_train/sample_patches_combined.py) is a Python program to generate image patches from CBIS-DDSM images. It takes in images and corresponding binary masks (i.e. annotations) as input and generates image patches to external folders. See [sample_combined_patches_im4096.sh](./ddsm_train/sample_combined_patches_im4096.sh) for an example to call this program.\n\n[patch_clf_train.py](./ddsm_train/patch_clf_train.py) is a Python program to train a deep learning model to classify image patches into background, benign and malignant. See [train_patch_clf_im4096_256_3Cls.sh](./ddsm_train/train_patch_clf_im4096_256_3Cls.sh) for an example to call this program.\n### Patch classifier finetuning on DM challenge data\nUnder [training](./training) folder:\n\n[dm_patchClf_finetune.py](./training/dm_patchClf_finetune.py) is a Python program used to finetune a patch classifier on the DM challenge data. See [finetune_patch_clf.sh](./training/finetune_patch_clf.sh) for a shell script to run this program in the cloud. See [finetune_patch_clf_local.sh](./training/finetune_patch_clf_local.sh) for a shell script to run this program locally.\n### Breast-level classifier training on probabilistic heatmaps\nUnder [training](./training) folder:\n\n[dm_heatmap_score.py](./training/dm_heatmap_score.py) is a Python program to generate probabilistic heatmaps from DM challenge images using a trained patch classifier. See [score_heatmap.sh](./training/score_heatmap.sh) for a shell script to run it in the cloud. See [score_heatmap_local.sh](./training/score_heatmap_local.sh) for a shell script to test it locally. \n\n[prob_heatmap_clf_dist.ipynb](./training/prob_heatmap_clf_dist.ipynb) is a Python notebook that contains code to develop a breast-level classifier based on the features extracted from the probabilistic heatmaps. \n### Sub-challenge 2 classifier training\nUnder [training](./training) folder:\n\n[dm_sc2_meta_clf.py](./training/dm_sc2_meta_clf.py) is a Python script that trains a random forest classifier using image scores and clinical information from the current and previous exams.\n\n## Inference\nUnder [inference](./inference) folder:\n\n[dm_sc1_phm_infer.py](./inference/dm_sc1_phm_infer.py) and [dm_sc2_phm_infer.py](./inference/dm_sc2_phm_infer.py) are the Python programs for SC1 and SC2 inference, respectively. [sc1_phm_infer.sh](./inference/sc1_phm_infer.sh) and [sc2_phm_infer.sh](./inference/sc2_phm_infer.sh) are the shell scripts to run them in the cloud. [sc1_phm_infer_local.sh](./inference/sc1_phm_infer_local.sh) and [sc2_phm_infer_local.sh](./inference/sc2_phm_infer_local.sh) are the shell scripts to test them locally.\n\n\n\n"
    },
    {
        "repo": "/zhenweishi/QMITH",
        "language": "Jupyter Notebook",
        "readme_contents": "# QMITH: MRI-based quantitative measure of  intra-tumoral heterogeneity in breast cancer\n\n<p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/17007301/219617294-a5f38b07-4599-4834-aa7c-96d01299a531.png\" width=\"600\" height=\"300\">\n</p>\n\n\nThis repository includes the analysis codes for the paper [MRI-based Quantification of Intratumoral Heterogeneity for Predicting Treatment Response to Neoadjuvant Chemotherapy in Breast Cancerr](https://pubs.rsna.org/doi/10.1148/radiol.222830) published in Radiology. We are working hard to make the processing workflow automatically. The stable version is comming soon. Please wait with more patience.\n\n### License\n\nThe QMITH package is freely available to browse, download, and use for scientific and educational purposes as outlined in the [Creative Commons Attribution 3.0 Unported License](https://creativecommons.org/licenses/by/3.0/).\n\n### Main Developers\n - [Dr. Zhenwei Shi](https://github.com/zhenweishi) <sup/>1, 2\n - MSc. Zhihe Zhao <sup/>2, 3\n - MD. Xiaomei Huang <sup/>2, 4\n - [Dr. Chu Han](https://chuhan89.com) <sup/>1, 2\n - MD. Changhong Liang <sup/>1, 2\n - MD. Zaiyi Liu <sup/>1, 2\n \n\n<sup>1</sup> Department of Radiology, Guangdong Provincial People's Hospital (Guangdong Academy of Medical Sciences), Southern Medical University, China <br/>\n<sup>2</sup> Guangdong Provincial Key Laboratory of Artificial Intelligence in Medical Image Analysis and Application, China <br/>\n<sup>3</sup> School of Medicine, South China University of Technology, China <br/>\n<sup>4</sup> Department of Medical Imaging, Nanfang Hospital, Southern Medical University, China \n\n### Contact\nWe are happy to help you with any questions. Please contact Zhenwei Shi.\nEmail: shizhenwei@gdph.org.cn\n\nWe welcome contributions to QMITH.\n"
    },
    {
        "repo": "/Ammar-Raneez/ONCO",
        "language": "Jupyter Notebook",
        "readme_contents": "# ONCO\n[![CodeQL](https://github.com/Ammar-Raneez/SDGP-ONCO/actions/workflows/codeql-analysis.yml/badge.svg)](https://github.com/Ammar-Raneez/SDGP-ONCO/actions/workflows/codeql-analysis.yml)\n[![CodeFactor](https://www.codefactor.io/repository/github/ammar-raneez/onco/badge)](https://www.codefactor.io/repository/github/ammar-raneez/onco)\n[![Codemagic build status](https://api.codemagic.io/apps/604df07a8ee52e4c314eef7d/604df07a8ee52e4c314eef7c/status_badge.svg)](https://codemagic.io/apps/604df07a8ee52e4c314eef7d/604df07a8ee52e4c314eef7c/latest_build)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\nONCO is an Open Source mobile platform that specializes in the diagnosis and prognosis of Skin, Lung, and Breast Cancer, utilizing deep convolutional neural networks for diagnosis, and machine learning and risk models for prognosis. The main aim is to bridge the gap between many mediocre and unreliable diagnosis and prognosis applications, while at the same time making it more enhanced and effective.\n\n# Tech-Stack Used\n* Flutter\n* Python\n* Azure\n* TensorFlow | Keras | Sci-kit Learn | NLTK\n* Flask\n* OpenCV\n* Heroku\n\n# Getting Started\nTo get started, have a look at the [Learn.md](./Learn.md) file, which describes about the implementation of lung cancer prognosis briefly.\nFlutter and Python are required to run this project and test it out.\n\n# Contributors\n* [Ammar Raneez](https://github.com/Ammar-Raneez) - Owner & Maintainer\n* [Nazhim Kalam](https://github.com/nazhimkalam) - Contributor\n* [Luqman Rumaiz](https://github.com/luqmanrumaiz) - Contributor\n* [Hammadh Arquil](https://github.com/hammvdh) - Contributor\n* [Maneesha Walisundara](https://github.com/maneesha14w) - Contributor\n* [Sabiq Sabry](https://github.com/sabiqsabry48) - Contributor\n\nAnyone is free to contribute \ud83d\ude01, please feel free to request a review from me and ensure that you follow the [Contributing Guideleines](./CONTRIBUTING.md) and adhere to the [COC](./CODE_OF_CONDUCT.md)\n"
    },
    {
        "repo": "/mistersharmaa/BreastCancerPrediction",
        "language": "Jupyter Notebook",
        "readme_contents": "# BreastCancerPrediction\nBreast cancer has the second highest mortality rate in women next to lung cancer. As per clinical statistics, 1 in every 8 women is diagnosed with breast cancer in their lifetime. However, periodic clinical check-ups and self-tests help in early detection and thereby significantly increase the chances of survival. Invasive detection techniques cause rupture of the tumor, accelerating the spread of cancer to adjoining areas. Hence, there arises the need for a more robust, fast, accurate, and efficient non-invasive cancer detection system. Early detection can give patients more treatment options. In order to detect signs of cancer, breast tissue from biopsies is stained to enhance the nuclei and cytoplasm for microscopic examination. Then, pathologists evaluate the extent of any abnormal structural variation to determine whether there are tumors. Architectural Distortion (AD) is a very subtle contraction of the breast tissue and may represent the earliest sign of cancer. Since it is very likely to be unnoticed by radiologists, several approaches have been proposed over the years but none using deep learning techniques. AI will become a transformational force in healthcare and soon, computer vision models will be able to get a higher accuracy when researchers have the access to more medical imaging datasets. The application of machine learning models for prediction and prognosis of disease development has become an irrevocable part of cancer studies aimed at improving the subsequent therapy and management of patients. The application of machine learning models for accurate prediction of survival time in breast cancer on the basis of clinical data is the main objective.  We have developed a computer vision model to detect breast cancer in histopathological images. Two classes will be used in this project: Benign and Malignant\n"
    },
    {
        "repo": "/alejandro-ao/streamlit-cancer-predict",
        "language": "Python",
        "readme_contents": "# Breast cancer diagnosis predictor\n\n## Overview\n\nThe Breast Cancer Diagnosis app is a machine learning-powered tool designed to assist medical professionals in diagnosing breast cancer. Using a set of measurements, the app predicts whether a breast mass is benign or malignant. It provides a visual representation of the input data using a radar chart and displays the predicted diagnosis and probability of being benign or malignant. The app can be used by manually inputting the measurements or by connecting it to a cytology lab to obtain the data directly from a machine. The connection to the laboratory machine is not a part of the app itself.\n\nThe app was developed as a machine learning exercice from the public dataset [Breast Cancer Wisconsin (Diagnostic) Data Set](https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data). Note that this dataset may not be reliable as this project was developed for educational purposes in the field of machine learning only and not for professional use.\n\nA live version of the application can be found on [Streamlit Community Cloud](https://alejandro-ao-streamlit-cancer-predict-appmain-uitjy1.streamlit.app/). \n\n## Installation\n\nTo run the Cell Image Analyzer locally, you will need to have Python 3.6 or higher installed. Then, you can install the required packages by running:\n\n```bash\npip install -r requirements.txt\n```\n\nThis will install all the necessary dependencies, including Streamlit, OpenCV, and scikit-image.\n\n## Usage\nTo start the app, simply run the following command:\n\n```bash\nstreamlit run app.py\n```\n\nThis will launch the app in your default web browser. You can then upload an image of cells to analyze and adjust the various settings to customize the analysis. Once you are satisfied with the results, you can export the measurements to a CSV file for further analysis.\n"
    },
    {
        "repo": "/arpit512512/Mammogram-Classification-using-GLCM-Features",
        "language": "Python",
        "readme_contents": "# Mammogram-Classification-using-GLCM-Features\nDetection and Classification of breast cancer in mammogram using textual and statistical features of image\n\nProject involves extracting textual features of mammogram image using Grey-Level Cocurrence Matrix and classification of mammograms into Abnormal and Normal class using Random Forest classifier.\nGaussian Filtering is incorporated for image enhancement and smoothing which reduces noise from image. Best GLCM features were selected by analyzing features scores obtained from AdaBoost classifier.\nAim is to improve upon existing research work by trying different algorithms and extracting more powerful features.Overall accuracy achieved is 93.90%, which is comparable with most of the past research.\nDataset used is MIAS.\n"
    },
    {
        "repo": "/AICAN-Research/H2G-Net",
        "language": "TeX",
        "readme_contents": "# H2G-Net\n\n[![License](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)\n[![Paper](https://zenodo.org/badge/DOI/10.3389/fneur.2022.932219.svg)](https://doi.org/10.3389/fmed.2022.971873)\n[![Poster](https://img.shields.io/badge/Poster-PDF-f39f37)](https://github.com/andreped/H2G-Net/blob/main/poster/poster.pdf)\n\nThis repository contains the code relevant for the proposed design H2G-Net, which was introduced in the manuscript [*\"H2G-Net: A multi-resolution refinement approach for segmentation of breast cancer region in gigapixel histopathological images\"*](https://www.frontiersin.org/articles/10.3389/fmed.2022.971873/full), published in Frontiers in Medicine.\n\nThe work was also presented at a region conference (HMN RHF 2022), where it won best poster award!\n\n## Brief summary of the paper and contributions\n\nWe propose a cascaded convolutional neural network for semantic segmentation of breast cancer tumours from whole slide images (WSIs). It is a two-stage design. In the first stage (detection stage), we apply a patch-wise classifier across the image which produces a tumour probability heatmap. In the second stage (refinement stage), we merge the resultant heatmap with a low-resolution version of the original WSI, before we send it to a new convolutional autoencoder that produces a final segmentation of the tumour ROI.\n\n- The paper proposed a hierarchically-balanced sampling scheme to adjust for the many data imbalance problems:\n\n<p align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/29090665/190863187-a239afc5-7a98-48df-9b5e-f0bd899b2d76.jpg\" width=\"80%\">\n</p>\n\n- Second, a two-stage cascaded convolutional neural network design, H2G-Net, was proposed that utilizes a refinement network to refine generated patch-wise predictions to improve low-resolution segmentation of breast cancer region.\n\n<p align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/29090665/190863086-ced55fbb-b4ed-4b4e-be56-3b9c4b6d474d.jpg\" width=\"80%\">\n</p>\n\n- The final model has been integrated into the open software [FastPathology](https://github.com/AICAN-Research/FAST-Pathology) and only takes ~1 minute to use on a full whole slide image using the CPU.\n\n- Developed annotated dataset of 624 whole slide images of breast cancer.\n\n## Test the model on your own data\nYou can easily test the H2G-Net model using [FAST](https://fast.eriksmistad.no).\nFirst make sure you have [all requirements for FAST installed](https://fast.eriksmistad.no/install.html).\nThen install FAST using pip, and run the breast tumour segmentation pipeline from your terminal.\nThis will download the model, run it on the WSI you specify and visualize the results.\n```bash\npip install pyfast\nrunPipeline --datahub breast-tumour-segmentation --file path/to/your/whole-slide-image.vsi\n```\n\nOr you can test the model in the graphical user interface [FastPathology](https://github.com/AICAN-Research/FAST-Pathology) which allows you to run the model on multiple images, change the visualization of the segmentation and export the segmentation to disk.\n\n## Code info\nOther useful scripts and tips for importing/exporting predictions/annotations to/from QuPath <-> FastPathology can be found in the [NoCodeSeg](https://github.com/andreped/NoCodeSeg) repository.\n\n**Disclaimer:** The source code is provided as is, only to demonstrate how to define the architecture and design used in the paper. The code itself requires modifications to run on a new dataset, as it contains various hard-coded solutions, but all components are provided, as well as the code for training and evaluating the refinement model.\n\n\n## How to cite\nPlease, cite our paper if you find the work useful:\n<pre>\n@article{10.3389/fmed.2022.971873,\n  author={Pedersen, Andr\u00e9 and Smistad, Erik and Rise, Tor V. and Dale, Vibeke G. and Pettersen, Henrik S. and Nordmo, Tor-Arne S. and Bouget, David and Reinertsen, Ingerid and Valla, Marit},\n  title={H2G-Net: A multi-resolution refinement approach for segmentation of breast cancer region in gigapixel histopathological images},\n  journal={Frontiers in Medicine},\n  volume={9},\n  year={2022},\n  url={https://www.frontiersin.org/articles/10.3389/fmed.2022.971873},\n  doi={10.3389/fmed.2022.971873},\n  issn={2296-858X}\n}\n</pre>\n\n## Contact\nPlease, contact andrped94@gmail.com for any further questions.\n\n## Acknowledgements\nCode for the AGU-Net and DAGU-Net architectures were based on the publication:\n<pre>\n@misc{bouget2021meningioma,\n  title={Meningioma segmentation in T1-weighted MRI leveraging global context and attention mechanisms},\n  author={David Bouget and Andr\u00e9 Pedersen and Sayied Abdol Mohieb Hosainey and Ole Solheim and Ingerid Reinertsen},\n  year={2021},\n  eprint={2101.07715},\n  archivePrefix={arXiv},\n  primaryClass={eess.IV}\n}\n</pre>\n\nCode for the DoubleU-Net architectures were based on the official GitHub [repository](https://github.com/DebeshJha/2020-CBMS-DoubleU-Net), based on this publication:\n<pre>\n@INPROCEEDINGS{9183321,\n  author={D. {Jha} and M. A. {Riegler} and D. {Johansen} and P. {Halvorsen} and H. D. {Johansen}},\n  booktitle={2020 IEEE 33rd International Symposium on Computer-Based Medical Systems (CBMS)}, \n  title={DoubleU-Net: A Deep Convolutional Neural Network for Medical Image Segmentation}, \n  year={2020},\n  pages={558-564}\n}\n</pre>\n"
    },
    {
        "repo": "/BishalDali/Breast_Cancer_Prediction",
        "language": "Jupyter Notebook",
        "readme_contents": "# Breast_Cancer_Prediction"
    },
    {
        "repo": "/IndianAIProduction-Channel/Breast-Cancer-Detection-App",
        "language": "HTML",
        "readme_contents": "# Breast-Cancer-Detection-App\n Breast Cancer Detection App Using Machine Learning XGBoost Classifier\n"
    },
    {
        "repo": "/gmineo/Breast-Cancer-Prediction-Project",
        "language": "R",
        "readme_contents": "# Breast-Cancer-Prediction\n\nChoose-your-own project - HarvardX: PH125.9x Capstone Course for the Data Science Professional Certificate\n\nThe best algorithm to predict whether a breast cancer cell is Benign or Malignant.\n\nIn this project we have developed a machine learning algorithm that predicts whether a breast cancer cell is benign or malignant based on the Breast Cancer Wisconsin (Diagnostic) DataSet. \n\nThis repository contains:\n\nReport.Rmd -> Project report in .rmd format\n\nReport.pdf -> Project report in .pdf format\n\nRScript.R -> R-Script that generates the data analysis\n\nThis project was created for the assignment of the EdX Capstone, Choose-your-own project, course from the following program: Professional Certificate in Data Science.\n"
    },
    {
        "repo": "/iharnoor/BreastCancer-Kmeans",
        "language": "Python",
        "readme_contents": "# BreastCancer-Kmeans\nClustering the data into benign or malignant using kMeans Algorithm\n\n## Also tested the data with different algorithms \n* kNN\n* Logistic Regression\n* Hierarchical Clustering\n\n## Results\n* Supervised Learning: Logistic regression with 25:75 data split Accuracy = 97%\n*  Supervised Learning: K-Nearest Neighbours with 25:75 data split Accuracy = 95%\n*  UnSupervised Learning: Hierarchical Clustering Accuracy = 77%\"\"\"\n*  UnSupervised Learning: kMeans Clustering Accuracy = 85%\n*  Can be even higher with Neural network.\n"
    },
    {
        "repo": "/sagnikghoshcr7/Breast-Cancer-Prediction",
        "language": "Jupyter Notebook",
        "readme_contents": "# Breast-Cancer-Prediction\n\nDiagnosing Malignant versus Benign Breast Tumours via Machine Learning Techniques in High Dimensions\n\nFiles:\nClassification.py : Logistc Regression, SVM and Decision Tree methods to classify data\n"
    }
]
=======
[]
>>>>>>> 22dd6f6c12869c348d766d8112faeb316f38a5b6
