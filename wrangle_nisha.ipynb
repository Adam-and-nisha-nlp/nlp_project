{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f44de9b0-9e2a-4e74-b83e-f723bc1ab027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrangle the data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# see the data\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# play with words\n",
    "import unicodedata\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re\n",
    "from pprint import pprint\n",
    "\n",
    "# split and model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# import of stats testing\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# import warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b64231ce-6d28-4b47-a420-fb0bc2098e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "### functions to get the data###\n",
    "### functions to prep the data ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01bc9093-bfe7-46d0-b779-5dd9d4eb7453",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(original):\n",
    "    '''\n",
    "    Input: original text or .apply(basic_clean) to entire data frame\n",
    "    Actions: \n",
    "    lowercase everything,\n",
    "    normalizes everything,\n",
    "    removes anything that's not a letter, number, whitespace, or single quote\n",
    "    Output: Cleaned text\n",
    "    '''\n",
    "    # lowercase everything\n",
    "    basic_cleaned = original.lower()\n",
    "    # normalize unicode characters\n",
    "    basic_cleaned = unicodedata.normalize('NFKD', basic_cleaned)\\\n",
    "    .encode('ascii', 'ignore')\\\n",
    "    .decode('utf-8', 'ignore')\n",
    "    # Replace anything that is not a letter, number, whitespace or a single quote.\n",
    "    basic_cleaned = re.sub(r'[^a-z0-9\\'\\s]', '', basic_cleaned)\n",
    "    \n",
    "    return basic_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50a0e488-1b53-4e60-ba79-bab1cbadcbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(basic_cleaned):\n",
    "    '''\n",
    "    Input: basic_cleaned text string or .apply(tokenize) to entire data frame\n",
    "    Actions:\n",
    "    creates the tokenizer\n",
    "    uses the tokenizer\n",
    "    Output: clean_tokenize text string\n",
    "    '''\n",
    "    #create the tokenizer\n",
    "    tokenize = nltk.tokenize.ToktokTokenizer()\n",
    "    #use the tokenizer\n",
    "    clean_tokenize = tokenize.tokenize(basic_cleaned, return_str=True)\n",
    "    \n",
    "    return clean_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0ea37ab-d743-4d53-b25a-288d4b3c114e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(lemma_or_stem, extra_words=[], exclude_words=[]):\n",
    "    '''\n",
    "    Input:text string or .apply(remove_stopwords) to entire data frame\n",
    "    Action: removes standard stop words\n",
    "    Output: parsed_article\n",
    "    '''\n",
    "    # save stopwords\n",
    "    stopwords_ls = stopwords.words('english')\n",
    "    # removing any stopwords in exclude list\n",
    "    stopwords_ls = set(stopwords_ls) - set(exclude_words)\n",
    "    # adding any stopwords in extra list\n",
    "    stopwords_ls = stopwords_ls.union(set(extra_words))\n",
    "    \n",
    "    # split words in article\n",
    "    words = lemma_or_stem.split()\n",
    "    # remove stopwords from list of words\n",
    "    filtered = [word for word in words if word not in stopwords_ls]\n",
    "    # join words back together\n",
    "    parsed_article = ' '.join(filtered)\n",
    "    \n",
    "    return parsed_article\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95ab0766-efa2-4c4f-a2e5-53341d624815",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(clean_tokenize):\n",
    "    '''\n",
    "    Inputs: clean_tokenize\n",
    "    Actions: creates lemmatizer and applies to each word\n",
    "    Outputs: clean_tokenize_lemma\n",
    "    '''\n",
    "    #create the lemmatizer\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    #use lemmatize - apply to each word in our string\n",
    "    lemmas = [wnl.lemmatize(word) for word in clean_tokenize.split()]\n",
    "    #join words back together\n",
    "    clean_tokenize_lemma = ' '.join(lemmas)\n",
    "    \n",
    "    return clean_tokenize_lemma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e2f21fa-024f-4549-8502-d01ecfc74ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    '''\n",
    "    A simple function to cleanup text data.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to be cleaned.\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of lemmatized words after cleaning.\n",
    "    '''\n",
    "    \n",
    "    # basic_clean() function from last lesson:\n",
    "    # Normalize text by removing diacritics, encoding to ASCII, decoding to UTF-8, and converting to lowercase\n",
    "    text = (unicodedata.normalize('NFKD', text)\n",
    "             .encode('ascii', 'ignore')\n",
    "             .decode('utf-8', 'ignore')\n",
    "             .lower())\n",
    "    \n",
    "    # Remove punctuation, split text into words\n",
    "    words = re.sub(r'[^\\w\\s]', '', text).split()\n",
    "    \n",
    "    \n",
    "    # lemmatize() function from last lesson:\n",
    "    # Initialize WordNet lemmatizer\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    \n",
    "    # Combine standard English stopwords with additional stopwords\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    \n",
    "    # Lemmatize words and remove stopwords\n",
    "    cleaned_words = [wnl.lemmatize(word) for word in words if word not in stopwords]\n",
    "    \n",
    "    return cleaned_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e51bccf-71ed-47af-888a-7287ef75e0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaned(df):\n",
    "    '''\n",
    "    This function will clean the df\n",
    "    drop nulls, replace special characters\n",
    "    '''\n",
    "    # drop nulls\n",
    "    df = df.dropna()\n",
    "    # replace special characters with space\n",
    "    df.readme_contents = df.readme_contents.str.replace('[/,_,-,:,\"]', ' ', regex=True)\n",
    "    # replace heavy, check, and mark with nothing\n",
    "    df.readme_contents = df.readme_contents.str.replace('heavy', '').str.replace('check', '').str.replace('mark', '')\n",
    "    # create column with clean text. Tokenized, normalized, lemmatized, stop words removed\n",
    "    df['clean_norm_token'] = df.readme_contents.apply(tokenize).apply(basic_clean).apply(remove_stopwords).apply(lemmatize)\n",
    "    # replace 124 with nothing. 124 was created by the program removing '|'\n",
    "    df.clean_norm_token = df.clean_norm_token.str.replace('124', '')\n",
    "    #in language column replace language with other if it is not in the top 5 languages\n",
    "    top_5 = df.language.value_counts().head(5).index.tolist()\n",
    "    df.language = df.language.apply(lambda x: x if x in top_5 else 'other')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "338a502d-ffb5-44f1-a3b8-63e019010b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### functions to split the data##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b89e375-0094-485f-b588-82a20c471799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_function_cat_target(df_name, target_varible_column_name):\n",
    "    train, test = train_test_split(df_name,\n",
    "                                   random_state=123, #can be whatever you want\n",
    "                                   test_size=.20,\n",
    "                                   stratify= df_name[target_varible_column_name])\n",
    "    \n",
    "    train, validate = train_test_split(train,\n",
    "                                   random_state=123,\n",
    "                                   test_size=.25,\n",
    "                                   stratify= train[target_varible_column_name])\n",
    "    return train, validate, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "223e3560-ffc8-44c9-91c5-39cdf383f868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the test will contain 20% of the data,\n",
    "# the validation contain 25%('test_size') of previous train which is 20% of original dataset the same as the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "806c02bc-2525-4430-b772-485dcef89dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to explore the data##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6bc08a4a-fb44-4917-b449-eac33729b384",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore(df,train):\n",
    "    #create a list of all the words in each language\n",
    "    python_words = clean(' '.join(train[train.language=='Python']['clean_norm_token']))\n",
    "    jupyter_notebook_words = clean(' '.join(train[train.language=='Jupyter Notebook']['clean_norm_token']))\n",
    "    html_words = clean(' '.join(train[train.language=='HTML']['clean_norm_token']))\n",
    "    r_words = clean(' '.join(train[train.language=='R']['clean_norm_token']))\n",
    "    other_words = clean(' '.join(train[train.language=='other']['clean_norm_token']))\n",
    "\n",
    "    all_words = clean(' '.join(df['clean_norm_token']))\n",
    "    #create a series of the frequency of each word in each language\n",
    "    python_freq = pd.Series(python_words).value_counts()\n",
    "    java_script_freq = pd.Series(java_script_words).value_counts()\n",
    "    jupyter_notebook_freq = pd.Series(jupyter_notebook_words).value_counts()\n",
    "    html_freq = pd.Series(html_words).value_counts()\n",
    "    r_freq = pd.Series(r_words).value_counts()\n",
    "    other_freq = pd.Series(other_words).value_counts()\n",
    "\n",
    "    all_freq = pd.Series(all_words).value_counts()\n",
    "\n",
    "    return python_freq, jupyter_notebook_freq, html_freq, r_freq, other_freq, all_freq, python_words,\\\n",
    "          java_script_words, jupyter_notebook_words, html_words, r_words, other_words, all_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b578f7d-47d3-4aa2-b4ca-4970e4295053",
   "metadata": {},
   "source": [
    " The explore function analyzes text data for different programming languages, computes word frequencies for each language, and returns these frequencies and word lists. It helps identify common words associated with each language in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94b751ae-1662-4cf6-9b12-79a14ee9cae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#makes ngrams depending on the number you put in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "923ed660-8bc5-4bbf-9259-2085aea2b174",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ngrams(words, n):\n",
    "    return pd.Series(nltk.ngrams(words, n)).value_counts().head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d1cfb1b-315a-4b74-a5be-f55f763864f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plots the ngrams and single words via wordcloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc3ba367-408c-460a-ba0d-eb5abb721b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bigrams(language,words):\n",
    "    word_data = {k[0] + ' ' + k[1]: v for k, v in words.to_dict().items()}\n",
    "    \n",
    "    word_img = WordCloud(background_color='white', width=800, height=400).generate_from_frequencies(word_data)\n",
    "    \n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.imshow(word_img)\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Top Words for {language}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "adec7a43-8f4c-4072-9bd6-9f6a724c9c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_counts_df(python_freq, jupyter_notebook_freq,\n",
    "                        html_freq, r_freq, all_freq):\n",
    "    word_counts = pd.concat([python_freq, jupyter_notebook_freq,\n",
    "                            html_freq, r_freq, all_freq], axis=1).fillna(0).astype(int)\n",
    "\n",
    "    # rename the col names\n",
    "    word_counts.columns = ['python', 'jupyter_notebook', 'html', 'r', 'all']    \n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "934d2c03-eec1-4c45-943f-70cec4580688",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_unique_words(unique_python_words, unique_r_words):\n",
    "    python_unique = [word for word in unique_python_words if word not in unique_r_words]\n",
    "    R_unique = [word for word in unique_java_script_words if word not in unique_python_words]\n",
    "    python_unique = pd.DataFrame(python_unique, columns=['Python Words'])\n",
    "    R_unique = pd.DataFrame(r_unique, columns=['r Words'])\n",
    "\n",
    "    return print(f\"{python_unique.head(10)} \\n -------------------------------------------- \\n{java_script_unique.head(10)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc858536-3f4a-4acd-a065-7d71a6febd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def python_wordcloud(python_freq):\n",
    "    '''\n",
    "    this funtion will plot a wordcloud for top 40 python words\n",
    "    '''\n",
    "    blog_img = WordCloud(background_color='white').generate_from_frequencies(python_freq.head(40))\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.imshow(blog_img)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7b31c0be-3cb6-419d-9322-28656183776c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def R_wordcloud(r_freq):\n",
    "    '''\n",
    "    this function will plot a wordcloud for top 40 java script words\n",
    "    '''\n",
    "    blog_img = WordCloud(background_color='white').generate_from_frequencies(r_freq.head(40))\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.imshow(blog_img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8b183054-1144-4db4-bf86-3b49257ef6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique word count for python and R\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6372f826-cbdc-450c-88d9-47a694fe3949",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_words_for_language(python_words, R_words):\n",
    "    '''\n",
    "    This fucntion will find the number of unique words in python and java script repos\n",
    "    '''\n",
    "    \n",
    "    unique_python_words = list(set(python_words))\n",
    "    unique_R_words = list(set(r_words))\n",
    "    #compare the words in python_words and R_words and return unique words from each\n",
    "    python_unique = [word for word in unique_python_words if word not in unique_r_words]\n",
    "    java_script_unique = [word for word in unique_r_words if word not in unique_python_words]\n",
    "    \n",
    "    print(f'     Number of unique Python words: {len(python_unique)}')\n",
    "    print(f'Number of unique R words: {len(r_unique)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "44861ca6-c633-4ea0-ad6c-ff25d2d0714f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"---------------------------------------------------------------------functions to set the X, y sets ----------------------------------------------------------------------\"\"\"\n",
    "#sets the X and y variables for train, validate, and test\n",
    "def X_y_variables(train, validate, test):\n",
    "    X_train = train.clean_norm_token\n",
    "    y_train = train.language\n",
    "    X_validate = validate.clean_norm_token\n",
    "    y_validate = validate.language\n",
    "    X_test = test.clean_norm_token\n",
    "    y_test = test.language\n",
    "\n",
    "    return X_train, y_train, X_validate, y_validate, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "283d64ad-fa23-41c8-b1a6-5c80c28b8f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sets the X and y variables for train, validate, and test for the bag of words TF\n",
    "#make my bag of words Term Frequency \n",
    "def X_y_variables_bow(X_train, X_validate, X_test):\n",
    "    cv = CountVectorizer()\n",
    "    X_bow = cv.fit_transform(X_train) \n",
    "    X_validate_bow = cv.transform(X_validate)\n",
    "    X_test_bow = cv.transform(X_test)\n",
    "\n",
    "    return X_bow, X_validate_bow, X_test_bow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c5d30429-aa78-42f0-9b1d-ac4ca26a4ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sets the X and y variables for train, validate, and test for the bag of words TFIDF\n",
    "#make my bag of words TF-IDF\n",
    "def X_y_variables_tfidf(X_train, X_validate, X_test):\n",
    "    tfidf = TfidfVectorizer()\n",
    "    X_bow = tfidf.fit_transform(X_train) \n",
    "    X_validate_bow = tfidf.transform(X_validate)\n",
    "    X_test_bow = tfidf.transform(X_test)\n",
    "\n",
    "    return X_bow, X_validate_bow, X_test_bow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "55cd286e-8a61-48c0-8be1-7946197946dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sets the X and y variables for train, validate, and test for the bag of ngrams TFIDF\n",
    "def X_y_variables_ngrams_tfidf(X_train, X_validate, X_test):\n",
    "    tfidf = TfidfVectorizer(ngram_range=(1, 3))\n",
    "    X_bow = tfidf.fit_transform(X_train) \n",
    "    X_validate_bow = tfidf.transform(X_validate)\n",
    "    X_test_bow = tfidf.transform(X_test)\n",
    "\n",
    "    return X_bow, X_validate_bow, X_test_bow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e4ec1bfd-617e-49f1-aef3-57d9e9606440",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a statistical test to see if there is a difference in the word frequency between python and R\n",
    "def ttest_ind(python_freq, r_freq):\n",
    "    alpha = .05\n",
    "    t, p = stats.ttest_ind(python_freq, r_freq)\n",
    "    if (p < alpha):\n",
    "        print(f\"{p} is less than {alpha}.\\nWe reject the null hypothesis, there is a difference between the two groups.\")\n",
    "    else:\n",
    "        print(\"We fail to reject the null hypothesis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc33805-9718-4c55-a272-2483bfe22152",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
